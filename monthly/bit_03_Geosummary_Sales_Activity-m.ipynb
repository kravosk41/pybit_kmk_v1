{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f34425-570f-426b-8445-419e260ba4a5",
   "metadata": {},
   "source": [
    "### GS Sales Activity v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a546890e-c348-4507-ba0c-133a303e3471",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import datetime, timedelta,date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b7f9655-5d51-4e38-ba27-8232ab28dad2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "curr_date = datetime.strptime(js['curr_date'], '%Y-%m-%d').date()\n",
    "quarter_start = datetime.strptime(js['quarter_start'], '%Y-%m-%d').date()\n",
    "quarter_end = datetime.strptime(js['quarter_end'], '%Y-%m-%d').date()\n",
    "qtr_data = js['qtr_data']\n",
    "num_weeks_calls = js['num_weeks_calls']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "num_of_months = js['num_of_months']\n",
    "monthly_data_date = js['monthly_data_date']\n",
    "data_date = js['data_date']\n",
    "YTD = js['YTD']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "geo = f's3://{bucket}/PYADM/quaterly/{qtr_data}/geography/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'\n",
    "mxpn = f's3://{bucket}/PYADM/monthly/archive/{monthly_data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07053fe5-ca6d-41f0-a4c9-be46264e7d10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')\n",
    "\n",
    "def intck(interval, start_date, end_date):\n",
    "    if interval == 'DAY':\n",
    "        return (end_date - start_date).days\n",
    "    elif interval == 'MONTH':\n",
    "        rd = relativedelta(end_date, start_date)\n",
    "        return rd.years * 12 + rd.months\n",
    "    elif interval == 'WEEK':\n",
    "        return (end_date - start_date).days // 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6e749e-8e2a-444c-9fce-c67f8a95e4c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "load('temp_calls')\n",
    "load('temp_samples')\n",
    "load('temp_abbv')\n",
    "load('mp_spec_seg_dec')\n",
    "load('hierarchy',geo)\n",
    "load('wd_raw')\n",
    "load('lirwd_call_plan')\n",
    "load('MASTER_UNI')\n",
    "load('roster')\n",
    "\n",
    "geo_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/GeographyMapping.txt',separator='|')\n",
    "geo_mapping = geo_mapping.with_columns(\n",
    "    Code = pl.when(pl.col('Code')!= 'NATION').then(pl.lit('1111-')+pl.col('Code')).otherwise(pl.col('Code'))\n",
    ")\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "\n",
    "geo_id_full = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "\n",
    "\n",
    "#fixes for vortex import -> Probably caused by Polars Upgrades\n",
    "temp_calls = temp_calls.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_samples = temp_samples.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_abbv = temp_abbv.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "wd_raw = wd_raw.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "#laxdn_geoid_sum = laxdn_geoid_sum.with_columns(pl.col('geography_id').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9aa10c-656f-4c5f-8fea-26031bc69cdc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Processing  1. temp calls  2. temp samples 3. temp abbv datasets\n",
    "# - doubt is physiian terr id same as salesrepterrid for every record?\n",
    "temp_calls_mp_spec = (\n",
    "    temp_calls\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")\n",
    "# NOTE -\n",
    "# Combining MP and dropping null geo\n",
    "# Adding Area and Region Code\n",
    "# Adding Working Day\n",
    "# Adding call_freq_quarter\n",
    "\n",
    "# For Supproting Calc ->\n",
    "#geo_code_mapper = temp_calls_mp_spec[['geography_id','region_geography_id','area_geography_id','nation_geography_id']].unique()\n",
    "geo_code_mapper = geo_id_full\n",
    "geo_code_mapper.to_pandas().to_parquet(dflib+'geo_code_mapper.parquet') #exporting for other code use\n",
    "\n",
    "###\n",
    "temp_samples_mp_spec = (\n",
    "    temp_samples\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")\n",
    "###\n",
    "temp_abbv_mp_spec = (\n",
    "    temp_abbv\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda2c37e-54c9-4482-b200-55860efaec2c",
   "metadata": {},
   "source": [
    "RX Util Functions ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77182396-fe4b-43af-85ca-ad99dc5321cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal - \n",
    "def get_lin_voucher():\n",
    "    vch = pl.read_parquet(f'{mxpn}LIN_VOUCHER.parquet') \n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod in ['LIN1','LIN2','LIN3']: # LINV\n",
    "        vch_prod = (\n",
    "            vch.select(\n",
    "                pl.col('IID'),\n",
    "                pl.col(f'{prod}TUF1').alias(f'vTUF_1c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,4)]).alias(f'vTUF_3c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,7)]).alias(f'vTUF_6c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,13)]).alias(f'vTUF_12c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(4,7)]).alias(f'vTUF_pqtrc'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,YTD+1)]).alias(f'vTUF_ytdc'),\n",
    "                pl.col(f'{prod}TUF2').alias(f'vTUF_1p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(4,7)]).alias(f'vTUF_3p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(7,13)]).alias(f'vTUF_6p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(13,25)]).alias(f'vTUF_12p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(7,10)]).alias(f'vTUF_pqtrp'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(13,13+YTD)]).alias(f'vTUF_ytdp')\n",
    "            )\n",
    "            .with_columns(pl.lit(f'LI{prod[-1]}').alias('PROD_CD'))\n",
    "        )\n",
    "        if prod[-1] == '1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "\n",
    "    # voucher_mapping = {'LI1': 4, 'LI2': 5, 'LI3': 3, 'LIV': 2}\n",
    "    vch1 = vch1.fill_null(0)\n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28d95903-1f57-456e-b6b4-466c7e3e83db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_metric_period(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,25)]\n",
    "    df = pl.read_parquet(mxpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,4)]).alias(metric+'_3c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,7)]).alias(metric+'_6c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,13)]).alias(metric+'_12c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(4,7)]).alias(metric+'_pqtrc'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,YTD+1)]).alias(metric+'_ytdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(4,7)]).alias(metric+'_3p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(7,13)]).alias(metric+'_6p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(13,25)]).alias(metric+'_12p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(7,10)]).alias(metric+'_pqtrp'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(13,13+YTD)]).alias(metric+'_ytdp')\n",
    "    )\n",
    "\n",
    "    # For Voucher Removal - \n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_3c') -  pl.col(f'v{metric}_3c').alias(f'{metric}_3c'),\n",
    "            pl.col(f'{metric}_6c') -  pl.col(f'v{metric}_6c').alias(f'{metric}_6c'),\n",
    "            pl.col(f'{metric}_12c') -  pl.col(f'v{metric}_12c').alias(f'{metric}_12c'),\n",
    "            pl.col(f'{metric}_pqtrc') -  pl.col(f'v{metric}_pqtrc').alias(f'{metric}_pqtrc'),\n",
    "            pl.col(f'{metric}_ytdc') -  pl.col(f'v{metric}_ytdc').alias(f'{metric}_ytdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_3p') -  pl.col(f'v{metric}_3p').alias(f'{metric}_3p'),\n",
    "            pl.col(f'{metric}_6p') -  pl.col(f'v{metric}_6p').alias(f'{metric}_6p'),\n",
    "            pl.col(f'{metric}_12p') -  pl.col(f'v{metric}_12p').alias(f'{metric}_12p'),\n",
    "            pl.col(f'{metric}_pqtrp') -  pl.col(f'v{metric}_pqtrp').alias(f'{metric}_pqtrp'),\n",
    "            pl.col(f'{metric}_ytdp') -  pl.col(f'v{metric}_ytdp').alias(f'{metric}_ytdp')\n",
    "        ).drop(cols_to_remove)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    metrics_to_calc = {}\n",
    "    for f in ['c','p']:\n",
    "        for p in [1,3,6,12,'pqtr','ytd']:\n",
    "            column = f'{metric}_{p}{f}'\n",
    "            metrics_to_calc[column] = pl.col(column).sum()\n",
    "    \n",
    "    df_terr = df.group_by(['geography_id','specialty_group','segment','decile','PROD_CD']).agg(**metrics_to_calc)\n",
    "\n",
    "    df_reg = df.join(geo_code_mapper[['geography_id','region_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['region_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**metrics_to_calc)\n",
    "\n",
    "    df_area = df.join(geo_code_mapper[['geography_id','area_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['area_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**metrics_to_calc)\n",
    "\n",
    "    df_nation = df.join(geo_code_mapper[['geography_id','nation_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['nation_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**metrics_to_calc)\n",
    "\n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a943dcc5-6c47-43f8-93b0-aa9feaf28edc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows(all_prod_df):\n",
    "    # converting tuple to list , because i cant assign the processed df back to it\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    for i in range(4): \n",
    "        df = all_prod_df[i]\n",
    "        agg_dict = {}\n",
    "        for col in df.columns[5:]:\n",
    "            agg_dict[col] = pl.col(col).sum()\n",
    "        \n",
    "        join_cols = df.columns[0:4]\n",
    "\n",
    "        df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "        df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "        df_2_35 = df_2_35.group_by(join_cols + ['parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "        df_1 = df.group_by(join_cols).agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "        # stack 1, 2_35 with df and return\n",
    "        df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "        vstack_helper = df.columns\n",
    "        df = df.vstack(\n",
    "            df_2_35.select(vstack_helper)\n",
    "        ).vstack(\n",
    "            df_1.select(vstack_helper)\n",
    "        )\n",
    "\n",
    "        all_prod_df[i] = df\n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2080ed5b-baa3-4e34-8e99-9c4cb23ed690",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_full_rollups(all_prod_df):\n",
    "    # converting the tuple of dfs into a list for processing\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    # for trivializing formulas - \n",
    "    p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    \n",
    "    #Looping over 4 levels (terr,reg,area,nation)\n",
    "    for i in range(4):\n",
    "        df = all_prod_df[i]\n",
    "        g = df.columns[0] #should contain geo level\n",
    "        metric_cols = df.columns[4:-1] #should contain the tuf / nuf columns\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols) #used for vstack later\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        #### Processing Done ####\n",
    "        df = (\n",
    "            df.select(main_seq)\n",
    "            .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "            .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "            .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        # Store Data Back :\n",
    "        all_prod_df[i] = df\n",
    "    \n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59bc595a-f75f-4084-b139-86ae63fee6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8_657, 17)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>geography_id</th><th>specialty_group</th><th>segment</th><th>decile</th><th>PROD_CD</th><th>TUF_1c</th><th>TUF_3c</th><th>TUF_6c</th><th>TUF_12c</th><th>TUF_pqtrc</th><th>TUF_ytdc</th><th>TUF_1p</th><th>TUF_3p</th><th>TUF_6p</th><th>TUF_12p</th><th>TUF_pqtrp</th><th>TUF_ytdp</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>98</td><td>&quot;A/O&quot;</td><td>&quot;Target&quot;</td><td>&quot;5-7&quot;</td><td>&quot;LI1&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>62</td><td>&quot;GE&quot;</td><td>&quot;Target&quot;</td><td>&quot;8-10&quot;</td><td>&quot;LI3&quot;</td><td>62.486</td><td>133.145</td><td>329.908</td><td>737.561</td><td>196.763</td><td>397.581</td><td>60.59</td><td>196.763</td><td>407.653</td><td>847.983</td><td>206.25</td><td>488.44</td></tr><tr><td>41</td><td>&quot;A/O&quot;</td><td>&quot;Non-Target&quot;</td><td>&quot;0-2&quot;</td><td>&quot;LI3&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.081</td><td>0.0</td><td>0.0</td></tr><tr><td>108</td><td>&quot;PED&quot;</td><td>&quot;ALG-ONLY-TARGET&quot;</td><td>&quot;0-2&quot;</td><td>&quot;LI3&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>102</td><td>&quot;A/O&quot;</td><td>&quot;Non-Target&quot;</td><td>&quot;8-10&quot;</td><td>&quot;LI2&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>96</td><td>&quot;GE&quot;</td><td>&quot;Target&quot;</td><td>&quot;8-10&quot;</td><td>&quot;LI3&quot;</td><td>14.381</td><td>111.597</td><td>191.475</td><td>562.635</td><td>79.878</td><td>297.601</td><td>60.167</td><td>79.878</td><td>371.16</td><td>739.856</td><td>184.634</td><td>422.638</td></tr><tr><td>97</td><td>&quot;GE&quot;</td><td>&quot;Non-Target&quot;</td><td>&quot;3-4&quot;</td><td>&quot;LI2&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>94</td><td>&quot;PED&quot;</td><td>&quot;Non-Target&quot;</td><td>&quot;3-4&quot;</td><td>&quot;LI3&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>75</td><td>&quot;GE&quot;</td><td>&quot;Target&quot;</td><td>&quot;8-10&quot;</td><td>&quot;LI3&quot;</td><td>0.0</td><td>81.028</td><td>167.767</td><td>284.308</td><td>86.739</td><td>193.466</td><td>38.374</td><td>86.739</td><td>116.541</td><td>189.603</td><td>56.757</td><td>112.691</td></tr><tr><td>43</td><td>&quot;PCP&quot;</td><td>&quot;ALG-ONLY-TARGET&quot;</td><td>&quot;3-4&quot;</td><td>&quot;LI3&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8_657, 17)\n",
       "┌──────────────┬─────────────┬─────────────┬────────┬───┬─────────┬─────────┬───────────┬──────────┐\n",
       "│ geography_id ┆ specialty_g ┆ segment     ┆ decile ┆ … ┆ TUF_6p  ┆ TUF_12p ┆ TUF_pqtrp ┆ TUF_ytdp │\n",
       "│ ---          ┆ roup        ┆ ---         ┆ ---    ┆   ┆ ---     ┆ ---     ┆ ---       ┆ ---      │\n",
       "│ i64          ┆ ---         ┆ str         ┆ str    ┆   ┆ f64     ┆ f64     ┆ f64       ┆ f64      │\n",
       "│              ┆ str         ┆             ┆        ┆   ┆         ┆         ┆           ┆          │\n",
       "╞══════════════╪═════════════╪═════════════╪════════╪═══╪═════════╪═════════╪═══════════╪══════════╡\n",
       "│ 98           ┆ A/O         ┆ Target      ┆ 5-7    ┆ … ┆ 0.0     ┆ 0.0     ┆ 0.0       ┆ 0.0      │\n",
       "│ 62           ┆ GE          ┆ Target      ┆ 8-10   ┆ … ┆ 407.653 ┆ 847.983 ┆ 206.25    ┆ 488.44   │\n",
       "│ 41           ┆ A/O         ┆ Non-Target  ┆ 0-2    ┆ … ┆ 0.0     ┆ 6.081   ┆ 0.0       ┆ 0.0      │\n",
       "│ 108          ┆ PED         ┆ ALG-ONLY-TA ┆ 0-2    ┆ … ┆ 0.0     ┆ 0.0     ┆ 0.0       ┆ 0.0      │\n",
       "│              ┆             ┆ RGET        ┆        ┆   ┆         ┆         ┆           ┆          │\n",
       "│ 102          ┆ A/O         ┆ Non-Target  ┆ 8-10   ┆ … ┆ 0.0     ┆ 0.0     ┆ 0.0       ┆ 0.0      │\n",
       "│ …            ┆ …           ┆ …           ┆ …      ┆ … ┆ …       ┆ …       ┆ …         ┆ …        │\n",
       "│ 96           ┆ GE          ┆ Target      ┆ 8-10   ┆ … ┆ 371.16  ┆ 739.856 ┆ 184.634   ┆ 422.638  │\n",
       "│ 97           ┆ GE          ┆ Non-Target  ┆ 3-4    ┆ … ┆ 0.0     ┆ 0.0     ┆ 0.0       ┆ 0.0      │\n",
       "│ 94           ┆ PED         ┆ Non-Target  ┆ 3-4    ┆ … ┆ 0.0     ┆ 0.0     ┆ 0.0       ┆ 0.0      │\n",
       "│ 75           ┆ GE          ┆ Target      ┆ 8-10   ┆ … ┆ 116.541 ┆ 189.603 ┆ 56.757    ┆ 112.691  │\n",
       "│ 43           ┆ PCP         ┆ ALG-ONLY-TA ┆ 3-4    ┆ … ┆ 0.0     ┆ 0.0     ┆ 0.0       ┆ 0.0      │\n",
       "│              ┆             ┆ RGET        ┆        ┆   ┆         ┆         ┆           ┆          │\n",
       "└──────────────┴─────────────┴─────────────┴────────┴───┴─────────┴─────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rx_util():\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    all_products_tuf = get_summed_metric_period('TUF',['LI1','LI2','LI3'])\n",
    "    all_products_tuf = add_parent_product_rows(all_products_tuf)\n",
    "    all_products_tuf = add_full_rollups(all_products_tuf)\n",
    "    all_products_tuf = list(all_products_tuf)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(4):\n",
    "        all_products_tuf[i] = all_products_tuf[i].filter(pl.col('product_id').is_in({2,3,4,5})).select([levels[i],'segment','specialty_group','decile','product_id','TUF_qtdc'])\n",
    "    \n",
    "        df = all_products_tuf[i].clone()\n",
    "        \n",
    "        df_2 = df.filter(product_id = 2).rename({'TUF_qtdc':'wk_qtd_LIN'}).drop('product_id')\n",
    "        df_3 = df.filter(product_id = 3).rename({'TUF_qtdc':'wk_qtd_LI3'}).drop('product_id')\n",
    "        df_4 = df.filter(product_id = 4).rename({'TUF_qtdc':'wk_qtd_LI1'}).drop('product_id')\n",
    "        df_5 = df.filter(product_id = 5).rename({'TUF_qtdc':'wk_qtd_LI2'}).drop('product_id')\n",
    "        \n",
    "        df_final = (\n",
    "            df_2\n",
    "            .join(df_4,on = [levels[i],'segment','specialty_group','decile'], how = 'outer_coalesce')\n",
    "            .join(df_5,on = [levels[i],'segment','specialty_group','decile'], how = 'outer_coalesce')\n",
    "            .join(df_3,on = [levels[i],'segment','specialty_group','decile'], how = 'outer_coalesce')\n",
    "        )\n",
    "        res.append(df_final)\n",
    "    \n",
    "    return(res)\n",
    "laxdn_geoid_sum = rx_util()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
