{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecd01e8-5c85-4b8c-8c0e-1a79f32271a3",
   "metadata": {},
   "source": [
    "# GS Trend Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6214ecc6-3bfb-48f6-b125-a2aec4a59b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:47.309307Z",
     "iopub.status.busy": "2024-10-01T08:16:47.308488Z",
     "iopub.status.idle": "2024-10-01T08:16:47.911785Z",
     "shell.execute_reply": "2024-10-01T08:16:47.910713Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime, timedelta,date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b8545c-eb1a-45eb-80f1-0b20b09a4dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:47.917953Z",
     "iopub.status.busy": "2024-10-01T08:16:47.917153Z",
     "iopub.status.idle": "2024-10-01T08:16:47.926257Z",
     "shell.execute_reply": "2024-10-01T08:16:47.925194Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "qtr_data = js['qtr_data']\n",
    "bucket = js['bucket']\n",
    "YTD = js['YTD']\n",
    "monthly_data_date = js['monthly_data_date']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "geo = f's3://{bucket}/PYADM/quaterly/{qtr_data}/geography/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'\n",
    "mxpn = f's3://{bucket}/PYADM/monthly/archive/{monthly_data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f6d1e6-e997-4811-b888-732e36c6a0a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:47.933319Z",
     "iopub.status.busy": "2024-10-01T08:16:47.930212Z",
     "iopub.status.idle": "2024-10-01T08:16:47.940328Z",
     "shell.execute_reply": "2024-10-01T08:16:47.939155Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8141f51-f2a1-4441-acd7-64bd55a6e08b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:47.945439Z",
     "iopub.status.busy": "2024-10-01T08:16:47.943992Z",
     "iopub.status.idle": "2024-10-01T08:16:49.260668Z",
     "shell.execute_reply": "2024-10-01T08:16:49.259695Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL'] # only these products are to be read from lax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa159f66-d10e-4f82-942f-e1a07ddf9dcc",
   "metadata": {},
   "source": [
    "Generator Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1893c66-f226-4ce3-b14f-4c46207372d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:49.268577Z",
     "iopub.status.busy": "2024-10-01T08:16:49.267826Z",
     "iopub.status.idle": "2024-10-01T08:16:49.276342Z",
     "shell.execute_reply": "2024-10-01T08:16:49.275587Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal -\n",
    "def get_lin_voucher_13vols():    \n",
    "    vch = pl.read_parquet(f'{mxpn}LIN_VOUCHER.parquet') \n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod,prod2 in zip(['LI1','LI2','LI3'],['LIN1','LIN2','LIN3']):\n",
    "        rename_dict = dict(zip([f'{prod2}TUF{i}' for i in range(1,14)],[f'vVol{i}_TUF' for i in range(1,14)]))\n",
    "        vch_prod = (\n",
    "            vch.select(['IID'] + [f'{prod2}TUF{i}' for i in range(1,14)])\n",
    "            .rename(rename_dict)\n",
    "            .with_columns(pl.lit(prod).alias('PROD_CD'))\n",
    "        )\n",
    "        if prod == 'LI1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "    vch1 = vch1.fill_null(0)\n",
    "    \n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315ebd42-9883-4388-a873-4e64cdd853e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:49.280373Z",
     "iopub.status.busy": "2024-10-01T08:16:49.279653Z",
     "iopub.status.idle": "2024-10-01T08:16:49.294304Z",
     "shell.execute_reply": "2024-10-01T08:16:49.293163Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_volumes_geos(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,14)]\n",
    "    df = pl.read_parquet(mxpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "    rename_dict = dict(zip(columns[2:],['Vol'+str(i)+'_'+metric for i in range(1,14)]))\n",
    "    df = df.rename(rename_dict)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null()\n",
    "    )\n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher_13vols()\n",
    "        df = (\n",
    "            df\n",
    "            .join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "            .with_columns([(pl.col(f'Vol{i}_TUF') - pl.col(f'vVol{i}_TUF')).alias(f'Vol{i}_TUF') for i in range(1,14)])\n",
    "            .drop(dfv.columns[1:-1])\n",
    "        )\n",
    "\n",
    "    volume_cols = [f'Vol{i}_{metric}' for i in range(1,14)]\n",
    "    agg_dict = {col: pl.col(col).sum() for col in volume_cols}\n",
    "\n",
    "    df_terr = df.group_by(['geography_id','specialty_group','segment','decile','PROD_CD']).agg(**agg_dict)\n",
    "\n",
    "    df_reg = df.join(geo_code_mapper[['geography_id','region_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['region_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**agg_dict)\n",
    "\n",
    "    df_area = df.join(geo_code_mapper[['geography_id','area_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['area_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**agg_dict)\n",
    "\n",
    "    df_nation = df.join(geo_code_mapper[['geography_id','nation_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['nation_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**agg_dict)\n",
    "\n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc6f407-c1b8-4a09-aa9d-fdd4d5877955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:49.307114Z",
     "iopub.status.busy": "2024-10-01T08:16:49.304999Z",
     "iopub.status.idle": "2024-10-01T08:16:49.316707Z",
     "shell.execute_reply": "2024-10-01T08:16:49.315523Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows(all_prod_df):\n",
    "    # converting tuple to list , because i cant assign the processed df back to it\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    for i in range(4): \n",
    "        df = all_prod_df[i]\n",
    "        agg_dict = {}\n",
    "        for col in df.columns[5:]:\n",
    "            agg_dict[col] = pl.col(col).sum()\n",
    "        \n",
    "        join_cols = df.columns[0:4]\n",
    "\n",
    "        df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "        df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "        df_2_35 = df_2_35.group_by(join_cols + ['parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "        df_1 = df.group_by(join_cols).agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "        # stack 1, 2_35 with df and return\n",
    "        df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "        vstack_helper = df.columns\n",
    "        df = df.vstack(\n",
    "            df_2_35.select(vstack_helper)\n",
    "        ).vstack(\n",
    "            df_1.select(vstack_helper)\n",
    "        )\n",
    "\n",
    "        all_prod_df[i] = df\n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10fd2a6-1fb1-4c0b-a85c-3343cf0ef665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:49.322212Z",
     "iopub.status.busy": "2024-10-01T08:16:49.321086Z",
     "iopub.status.idle": "2024-10-01T08:16:49.339117Z",
     "shell.execute_reply": "2024-10-01T08:16:49.338227Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_full_rollups(all_prod_df):\n",
    "    # converting the tuple of dfs into a list for processing\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    # for trivializing formulas - \n",
    "    p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    \n",
    "    #Looping over 4 levels (terr,reg,area,nation)\n",
    "    for i in range(4):\n",
    "        df = all_prod_df[i]\n",
    "        g = df.columns[0] #should contain geo level\n",
    "        metric_cols = df.columns[4:-1] #should contain the tuf / nuf columns\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols) #used for vstack later\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        #### Processing Done ####\n",
    "        df = (\n",
    "            df.select(main_seq)\n",
    "            .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "            .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "            .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        # Store Data Back :\n",
    "        all_prod_df[i] = df\n",
    "    \n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b7d603-8604-4856-aaca-9de4bbcc9b3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:49.344322Z",
     "iopub.status.busy": "2024-10-01T08:16:49.343926Z",
     "iopub.status.idle": "2024-10-01T08:16:54.058197Z",
     "shell.execute_reply": "2024-10-01T08:16:54.057126Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data Prep - ETA 5 Seconds\n",
    "all_products_tuf = get_volumes_geos('TUF',fetch_products)\n",
    "all_products_tuf = add_parent_product_rows(all_products_tuf)\n",
    "all_products_tuf = add_full_rollups(all_products_tuf)\n",
    "\n",
    "all_products_nuf = get_volumes_geos('NUF',fetch_products)\n",
    "all_products_nuf = add_parent_product_rows(all_products_nuf)\n",
    "all_products_nuf = add_full_rollups(all_products_nuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fcf33c-ecac-41c6-b466-adcf8f2d5a8c",
   "metadata": {},
   "source": [
    "Functions -\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e3bdb5-f4c9-4ec7-97d6-eae858196bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:54.062862Z",
     "iopub.status.busy": "2024-10-01T08:16:54.062070Z",
     "iopub.status.idle": "2024-10-01T08:16:54.069198Z",
     "shell.execute_reply": "2024-10-01T08:16:54.068284Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_1(df):\n",
    "    for i in range(4):\n",
    "        g = levels[i]\n",
    "        gb_helper = [g,spc,sg,d,p]\n",
    "        f = (\n",
    "            all_products_tuf[i]\n",
    "            .join(all_products_nuf[i],on = gb_helper,how = 'left')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e717d06f-3853-4dee-8d81-58d1d0d45b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:54.077135Z",
     "iopub.status.busy": "2024-10-01T08:16:54.076593Z",
     "iopub.status.idle": "2024-10-01T08:16:54.084407Z",
     "shell.execute_reply": "2024-10-01T08:16:54.083300Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_2(df):\n",
    "    for j in range(4): #changing this from i to j because of a local var conflict\n",
    "        f = df[j]\n",
    "\n",
    "        rename_dict = {}\n",
    "        expn_dict = {}\n",
    "        for i in range(1,14):\n",
    "            for metric in ['TUF','NUF']:\n",
    "                rename_dict[f'Vol{i}_{metric}'] = f'lax_Vol{i}_{metric}'\n",
    "                expn_dict[f'Shr{i}_{metric}'] = pl.col(f'Vol{i}_{metric}')/pl.col(f'lax_Vol{i}_{metric}')\n",
    "\n",
    "        f_1 = f.filter(pl.col(p)==1).rename(rename_dict).drop(p)\n",
    "        f = (\n",
    "            f\n",
    "            .join(f_1,on=[levels[j],spc,sg,d],how='left')\n",
    "            .with_columns(**expn_dict)\n",
    "            .drop(list(rename_dict.values()))\n",
    "        )\n",
    "\n",
    "        df[j] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f6ba67-f7dd-4f91-ab16-45127233d85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:54.092715Z",
     "iopub.status.busy": "2024-10-01T08:16:54.091513Z",
     "iopub.status.idle": "2024-10-01T08:16:54.125352Z",
     "shell.execute_reply": "2024-10-01T08:16:54.123079Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Converting To feed ready Data\n",
    "def get_feed(temp1):\n",
    "    temp1[0] = temp1[0].rename({'geography_id': 'Geography_id'})\n",
    "    temp1[1] = temp1[1].rename({'region_geography_id': 'Geography_id'})\n",
    "    temp1[2] = temp1[2].rename({'area_geography_id': 'Geography_id'})\n",
    "    temp1[3] = temp1[3].rename({'nation_geography_id': 'Geography_id'})\n",
    "    final_feed = temp1[0].vstack(temp1[1]).vstack(temp1[2]).vstack(temp1[3])\n",
    "    #function to diving dataframe in two levels('Trx','Nrx')\n",
    "    def select_columns_by_condition(df,metric):\n",
    "        # Get the column names to be excluded based on the condition\n",
    "        excluded_columns = [col for col in df.columns if not col.endswith(metric)]\n",
    "        \n",
    "        # Select all columns except the excluded ones\n",
    "        selected_df = df.select(excluded_columns)\n",
    "        return selected_df\n",
    "    final_feed_nrx = select_columns_by_condition(final_feed,'TUF')# dataframe not including trx columns\n",
    "    final_feed_nrx = final_feed_nrx.with_columns(pl.lit('NRX').alias(\"Metric\"))\n",
    "    final_feed_trx = select_columns_by_condition(final_feed,'NUF')\n",
    "    final_feed_trx = final_feed_trx.with_columns(pl.lit('TRX').alias(\"Metric\"))\n",
    "    #function to remove _trx or _nrx from final_feed_nrx and final_feed_trx\n",
    "    def rename_columns_by_condition(df,metric):\n",
    "        renamed_columns = {col: col[:-4] if col.endswith(metric) else col for col in df.columns}\n",
    "        renamed_df = df.rename(renamed_columns)\n",
    "        return renamed_df\n",
    "    # making trx feed columns and nrx feed columns similar so that we can vstack them\n",
    "    final_feed_nrx = rename_columns_by_condition(final_feed_nrx,'NUF')\n",
    "    final_feed_trx = rename_columns_by_condition(final_feed_trx,'TUF')\n",
    "    final_feed = final_feed_trx.vstack(final_feed_nrx)\n",
    "    #required new columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addna = [\n",
    "        \"Pres1\", \"Pres2\", \"Pres3\", \"Pres4\", \"Pres5\", \"Pres6\", \"Pres7\", \"Pres8\", \"Pres9\", \"Pres10\", \"Pres11\", \"Pres12\", \"Pres13\", \n",
    "        \"DS1_Vol1\", \"DS1_Vol2\", \"DS1_Vol3\", \"DS1_Vol4\", \"DS1_Vol5\", \"DS1_Vol6\", \"DS1_Vol7\", \"DS1_Vol8\", \"DS1_Vol9\", \"DS1_Vol10\", \n",
    "        \"DS1_Vol11\", \"DS1_Vol12\", \"DS1_Vol13\", \"DS2_Vol1\", \"DS2_Vol2\", \"DS2_Vol3\", \"DS2_Vol4\", \"DS2_Vol5\", \"DS2_Vol6\", \"DS2_Vol7\",\n",
    "          \"DS2_Vol8\", \"DS2_Vol9\", \"DS2_Vol10\", \"DS2_Vol11\", \"DS2_Vol12\", \"DS2_Vol13\"\n",
    "    \n",
    "    ]\n",
    "    for my_col in col_to_addna:\n",
    "            final_feed = final_feed.with_columns(pl.lit('\\\\N').alias(my_col))\n",
    "      \n",
    "    final_feed = final_feed.with_columns(pl.lit('MONTHLY').alias('ReportType'))\n",
    "    #Renaming columns\n",
    "    new_col_mapping = {\n",
    "        'product_id': 'Product_id',\n",
    "        'segment': 'Segment',\n",
    "        'specialty_group': 'Specialty',\n",
    "        'decile': 'Decile'\n",
    "    }\n",
    "    final_feed = final_feed.rename(new_col_mapping)\n",
    "\n",
    "    # changing value of column to match with sas - 06/21\n",
    "    final_feed = final_feed.with_columns(\n",
    "        pl.when(pl.col('Segment')=='ALG-ONLY-TARGET')\n",
    "        .then(pl.lit('AGNT'))\n",
    "        .when(pl.col('Segment')=='Target')\n",
    "        .then(pl.lit('T'))\n",
    "        .when(pl.col('Segment')=='Non-Target')\n",
    "        .then(pl.lit('NT'))\n",
    "        .otherwise(pl.col('Segment'))\n",
    "        .alias('Segment'))\n",
    "\n",
    "    # rearranging columns accoring to feed.\n",
    "    req_cols = [\n",
    "        'Geography_id', 'Product_id', 'Segment', 'Specialty', 'Metric', 'ReportType', 'Decile', 'Vol1', 'Vol2', 'Vol3', 'Vol4', 'Vol5', \n",
    "        'Vol6', 'Vol7', 'Vol8', 'Vol9', 'Vol10', 'Vol11', 'Vol12', 'Vol13', 'Shr1', 'Shr2', 'Shr3', 'Shr4', 'Shr5', 'Shr6', 'Shr7', \n",
    "        'Shr8', 'Shr9', 'Shr10', 'Shr11', 'Shr12', 'Shr13', 'Pres1', 'Pres2', 'Pres3', 'Pres4', 'Pres5', 'Pres6', 'Pres7', 'Pres8', \n",
    "        'Pres9', 'Pres10', 'Pres11', 'Pres12', 'Pres13', 'DS1_Vol1', 'DS1_Vol2', 'DS1_Vol3', 'DS1_Vol4', 'DS1_Vol5', 'DS1_Vol6', 'DS1_Vol7', \n",
    "        'DS1_Vol8', 'DS1_Vol9', 'DS1_Vol10', 'DS1_Vol11', 'DS1_Vol12', 'DS1_Vol13', 'DS2_Vol1', 'DS2_Vol2', 'DS2_Vol3', 'DS2_Vol4', 'DS2_Vol5', \n",
    "        'DS2_Vol6', 'DS2_Vol7', 'DS2_Vol8', 'DS2_Vol9', 'DS2_Vol10', 'DS2_Vol11', 'DS2_Vol12', 'DS2_Vol13'\n",
    "    ]\n",
    "    final_feed = final_feed.select(req_cols)\n",
    "    \n",
    "    return(final_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ad145-57ae-48d1-a896-70d59ef3f2b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fbb5363-5922-4ac5-a90f-44be4b09b0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:54.134343Z",
     "iopub.status.busy": "2024-10-01T08:16:54.133455Z",
     "iopub.status.idle": "2024-10-01T08:16:54.142855Z",
     "shell.execute_reply": "2024-10-01T08:16:54.141448Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for trvializing formula : \n",
    "p,sg,spc,d = 'product_id','segment','specialty_group','decile'\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/GeoSummary/Monthly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf59abf6-2807-46c9-bc55-25bfd7cb52df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:16:54.149523Z",
     "iopub.status.busy": "2024-10-01T08:16:54.148973Z",
     "iopub.status.idle": "2024-10-01T08:17:15.011508Z",
     "shell.execute_reply": "2024-10-01T08:17:15.005277Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Trend Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "# Calling Functions and Exporting Feeds-\n",
    "temp1 = [pl.DataFrame() for _ in range(4)] # creating an empty dataframe holder list obj\n",
    "temp1 = process_1(temp1)\n",
    "temp1 = process_2(temp1)\n",
    "feed_dataset = get_feed(temp1)\n",
    "#===================================================\n",
    "feed_dataset = feed_dataset.to_pandas()\n",
    "# Select columns of type 'object' (string)\n",
    "string_columns = feed_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "feed_dataset[string_columns] = feed_dataset[string_columns].fillna('\\\\N')\n",
    "feed_dataset = feed_dataset.replace('NaN', '\\\\N')\n",
    "feed_dataset = feed_dataset.replace([np.nan, np.inf, -np.inf], '\\\\N')\n",
    "feed_dataset.to_csv(f'{OUT}Monthly_GeoSummary_Trend_Feed.txt', sep='|', lineterminator='\\r\\n',index=False)\n",
    "print('GS Trend Feed Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3b815-b6da-43e9-bc2c-90aa50646e13",
   "metadata": {},
   "source": [
    "Geosummary X Feed -\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b6a7ed-450e-40ad-a821-df13ae2913a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T08:17:15.028220Z",
     "iopub.status.busy": "2024-10-01T08:17:15.025514Z",
     "iopub.status.idle": "2024-10-01T08:17:15.118339Z",
     "shell.execute_reply": "2024-10-01T08:17:15.116957Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presc X Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "rx_date = datetime.strptime(monthly_data_date,'%Y%m')\n",
    "list_of_dates = [rx_date]\n",
    "serial_no = [i for i in range(1,14)]\n",
    "for i in range(1,13):\n",
    "    date_val = rx_date - timedelta(days = 30*i)\n",
    "    list_of_dates.append(date_val)\n",
    "\n",
    "date_df = pl.DataFrame(\n",
    "    {\n",
    "        'X':serial_no,\n",
    "        'Name':list_of_dates\n",
    "    }\n",
    ")\n",
    "\n",
    "date_df = date_df.with_columns(\n",
    "   date_df['Name'].dt.strftime('%b-%y')\n",
    ")\n",
    "\n",
    "date_df.to_pandas().to_csv(f'{OUT}Monthly_GeoSummary_X_Feed.txt', sep='|',lineterminator='\\r\\n',index=False)\n",
    "print('Presc X Feed Exported !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
