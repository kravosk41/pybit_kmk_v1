{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d584753b-f10b-47e0-ad02-7d67016bfd6a",
   "metadata": {},
   "source": [
    "### Presc Trend Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85dafab-2960-4b6d-9b1b-69a953e9f77b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime, timedelta,date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "021126a1-a18c-4209-aafd-67acd60edbae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "qtr_data = js['qtr_data']\n",
    "bucket = js['bucket']\n",
    "YTD = js['YTD']\n",
    "monthly_data_date = js['monthly_data_date']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "geo = f's3://{bucket}/PYADM/quaterly/{qtr_data}/geography/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'\n",
    "mxpn = f's3://{bucket}/PYADM/monthly/archive/{monthly_data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9cc23ee-95cd-438b-bba4-aadb0ed9df08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfdcdba2-02f1-4554-baee-d3e0a6cd761d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "load('MASTER_UNI')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729c77da-9ad7-4696-9edd-6aa7b373a05f",
   "metadata": {},
   "source": [
    "Generator Functions -\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ebbf02-d12c-4e97-8d82-4a4cafcf3b27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal -\n",
    "def get_lin_voucher_13vols():    \n",
    "    vch = pl.read_parquet(f'{mxpn}LIN_VOUCHER.parquet') \n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod,prod2 in zip(['LI1','LI2','LI3'],['LIN1','LIN2','LIN3']):\n",
    "        rename_dict = dict(zip([f'{prod2}TUF{i}' for i in range(1,14)],[f'vVol{i}_TUF' for i in range(1,14)]))\n",
    "        vch_prod = (\n",
    "            vch.select(['IID'] + [f'{prod2}TUF{i}' for i in range(1,14)])\n",
    "            .rename(rename_dict)\n",
    "            .with_columns(pl.lit(prod).alias('PROD_CD'))\n",
    "        )\n",
    "        if prod == 'LI1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "    vch1 = vch1.fill_null(0)\n",
    "    \n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "142cda53-788c-462c-b1a8-2e118d598ba1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_volumes(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,14)]\n",
    "    df = pl.read_parquet(mxpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "    rename_dict = dict(zip(columns[2:],['Vol'+str(i)+'_'+metric for i in range(1,14)]))\n",
    "    df = df.rename(rename_dict)\n",
    "\n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher_13vols()\n",
    "        df = (\n",
    "            df\n",
    "            .join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "            .with_columns([(pl.col(f'Vol{i}_TUF') - pl.col(f'vVol{i}_TUF')).alias(f'Vol{i}_TUF') for i in range(1,14)])\n",
    "            .drop(dfv.columns[1:-1])\n",
    "        )\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null()\n",
    "    ).drop(['specialty_group','segment','decile','geography_id'])\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d634b2-eb08-4ae8-8f58-75f965599e13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##### adding parent product rows\n",
    "def add_parent_product_rows(df): #pass dataframe with all products and 13 month volumes for both metrics here\n",
    "\n",
    "    agg_dict  = {}\n",
    "\n",
    "    for i in range(1,14):\n",
    "        for metric in ['TUF','NUF']:\n",
    "            col_name = f'Vol{i}_{metric}'\n",
    "            agg_dict[col_name] = pl.col(col_name).sum()\n",
    "\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    df_1 = df.group_by(['IID']).agg(**agg_dict).with_columns(product_id = pl.lit(1)\n",
    "    ).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba9c545-63ad-4ac4-a409-4d75820ecfe6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data Prep \n",
    "all_products_volume_tuf = get_volumes('TUF',fetch_products)\n",
    "all_products_volume_nuf = get_volumes('NUF',fetch_products)\n",
    "all_products_volume = all_products_volume_tuf.join(all_products_volume_nuf,on = ['IID','PROD_CD'],how='left')\n",
    "\n",
    "#for sub level groups -\n",
    "prod_mapping1 = prod_mapping[['product_id','parent_product_id','code']]#.filter(pl.col('parent_product_id')!=1)\n",
    "\n",
    "all_products_volume = all_products_volume.join(prod_mapping1,left_on='PROD_CD',right_on='code',how='left')\n",
    "all_products_volume = add_parent_product_rows(all_products_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee3d7a-4358-404e-b2c7-d11452dd78fa",
   "metadata": {},
   "source": [
    "Functions -\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277c6bbe-27fa-4125-9744-911bf2299d77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Volume\n",
    "#this will increase nobs on temp1 as its one IID to many prod rows\n",
    "def add_volume_cols(df): \n",
    "    return(df.join(all_products_volume,on='IID',how='left').filter(pl.col('product_id').is_not_null()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ee7017-50fc-4519-b44f-50153a42dc71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Share\n",
    "def process_share(df):\n",
    "    rename_dict = {}\n",
    "    expn_dict = {}\n",
    "    for i in range(1,14):\n",
    "        for metric in ['TUF','NUF']:\n",
    "            rename_dict[f'Vol{i}_{metric}'] = f'lax_Vol{i}_{metric}'\n",
    "            expn_dict[f'Shr{i}_{metric}'] = pl.col(f'Vol{i}_{metric}')/pl.col(f'lax_Vol{i}_{metric}')\n",
    "\n",
    "    df_1 = df.filter(pl.col('product_id')==1).rename(rename_dict).drop(['product_id','geography_id']) # this will contain LAX volumes for each IID\n",
    "    df = df.join(df_1,on='IID',how='left'\n",
    "    ).with_columns(**expn_dict).drop(list(rename_dict.values()))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3537a1a4-f5b2-470e-8c68-b338e3dadf6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Trend\n",
    "def process_trend(df,metric):\n",
    "    THRE_13 = 1/26\n",
    "    THRE_4 = 1/10\n",
    "    #THRE = 1/4 # not used\n",
    "\n",
    "    df2 = df.select(['IID','geography_id','product_id']+[f'Vol{i}_{metric}' for i in range(1,14)])\n",
    "    #AVG_TUF\n",
    "    df2 = df2.with_columns(\n",
    "        AVG_TUF = pl.mean_horizontal([f'Vol{i}_{metric}' for i in range(1,14)])\n",
    "    )\n",
    "\n",
    "    #SLOPE_13\n",
    "    AVG_TUF = pl.col('AVG_TUF') #just to make formatting easier (polars.expr.expr.Expr obj)\n",
    "    df2 = df2.with_columns(\n",
    "        SLOPE_13 = (\n",
    "        -5.5 * (pl.col(f'Vol13_{metric}') - AVG_TUF) \n",
    "        -4.5 * (pl.col(f'Vol12_{metric}') - AVG_TUF) \n",
    "        -3.5 * (pl.col(f'Vol11_{metric}') - AVG_TUF) \n",
    "        -2.5 * (pl.col(f'Vol10_{metric}') - AVG_TUF) \n",
    "        -1.5 * (pl.col(f'Vol9_{metric}') - AVG_TUF) \n",
    "        -0.5 * (pl.col(f'Vol8_{metric}') - AVG_TUF) \n",
    "        +0.5 * (pl.col(f'Vol6_{metric}') - AVG_TUF) \n",
    "        +1.5 * (pl.col(f'Vol5_{metric}') - AVG_TUF) \n",
    "        +2.5 * (pl.col(f'Vol4_{metric}') - AVG_TUF) \n",
    "        +3.5 * (pl.col(f'Vol3_{metric}') - AVG_TUF) \n",
    "        +4.5 * (pl.col(f'Vol2_{metric}') - AVG_TUF) \n",
    "        +5.5 * (pl.col(f'Vol1_{metric}') - AVG_TUF)\n",
    "        ) / 143\n",
    "    )\n",
    "\n",
    "    #AVG_TUF_4\n",
    "    df2 = df2.with_columns(\n",
    "        AVG_TUF_4 = pl.mean_horizontal([f'Vol{i}_{metric}' for i in range(5,9)])\n",
    "    )\n",
    "\n",
    "    #SLOPE_4\n",
    "    AVG_TUF_4 = pl.col('AVG_TUF_4') # just for formatting\n",
    "    df2 = df2.with_columns(\n",
    "        SLOPE_4 = (\n",
    "        -1.5 * (pl.col(f'Vol4_{metric}') - AVG_TUF_4) \n",
    "        -0.5 * (pl.col(f'Vol3_{metric}') - AVG_TUF_4) \n",
    "        +0.5 * (pl.col(f'Vol2_{metric}') - AVG_TUF_4) \n",
    "        +1.5 * (pl.col(f'Vol1_{metric}') - AVG_TUF_4)\n",
    "        ) / 15\n",
    "    )\n",
    "\n",
    "    #INDICATOR_SLOPE13\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('SLOPE_13')>THRE_13).then(pl.lit(1))\n",
    "        .when(pl.col('SLOPE_13')<-1*THRE_13).then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_SLOPE13')\n",
    "    )\n",
    "\n",
    "    #INDICATOR_SLOPE4\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('SLOPE_4')>THRE_4).then(pl.lit(1))\n",
    "        .when(pl.col('SLOPE_4')<-1*THRE_4).then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_SLOPE4')\n",
    "    )\n",
    "\n",
    "    #PEAK_DETECTOR\n",
    "    cols_1_13 = [f'Vol{i}_{metric}' for i in range(1,14)]\n",
    "    df2 = df2.with_columns(\n",
    "        PEAK_DETECTOR = pl.max_horizontal(cols_1_13)/pl.sum_horizontal(cols_1_13)\n",
    "    )\n",
    "\n",
    "    #INDICATOR_PEAK\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('PEAK_DETECTOR')>= 0.5).then(pl.lit(1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_PEAK')\n",
    "    )\n",
    "\n",
    "    #FINAL_SLOPE\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when((pl.col('INDICATOR_SLOPE13') == 1) & (pl.col('INDICATOR_SLOPE4') >= 0))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 0) & (pl.col('INDICATOR_SLOPE4') == 1))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == -1) & (pl.col('INDICATOR_SLOPE4') <= 0))\n",
    "            .then(pl.lit(-1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 0) & (pl.col('INDICATOR_SLOPE4') == -1))\n",
    "            .then(pl.lit(-1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == -1) & (pl.col('INDICATOR_SLOPE4') == 1) & (pl.col('SLOPE_4') >= 1))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 1) & (pl.col('INDICATOR_SLOPE4') == -1) & (pl.col('SLOPE_4') <= -1))\n",
    "            .then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0))\n",
    "        .alias('FINAL_SLOPE')\n",
    "    )\n",
    "\n",
    "    #GROWDECL\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('FINAL_SLOPE') == 1)\n",
    "            .then(pl.lit(\"P\"))\n",
    "        .when(pl.col('FINAL_SLOPE') == -1)\n",
    "            .then(pl.lit(\"Q\"))\n",
    "        .otherwise(pl.lit(\"S\"))\n",
    "        .alias(f'Trend_{metric}')\n",
    "    )\n",
    "\n",
    "    df2 = df2.select(['IID','product_id',f'Trend_{metric}'])\n",
    "    df = df.join(df2,on = ['IID','product_id'],how = 'left')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf599cc-1503-47c5-afc3-77a8bcfa2668",
   "metadata": {},
   "source": [
    "Processing \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8ea8131-2cf9-4704-b59e-b85f5e0232fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/Prescriber/Monthly/'\n",
    "temp1 = mp_spec_seg_dec.select(['IID','geography_id'])\n",
    "temp1 = add_volume_cols(temp1)\n",
    "temp1 = process_share(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87160005-d22f-4484-947e-467da80b644a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For removal of extra rows -\n",
    "\n",
    "#joining with 52wk flag dataset -> SKIPPED THIS STEP | NEED MORE INFO\n",
    "\n",
    "temp1 = temp1.with_columns(\n",
    "    flag_sum_TUF = pl.sum_horizontal([f'Vol{i}_TUF' for i in range(1,14)]),\n",
    "    flag_sum_NUF = pl.sum_horizontal([f'Vol{i}_NUF' for i in range(1,14)])\n",
    ")\n",
    "\n",
    "temp1= temp1.join(mp_spec_seg_dec[['IID','specialty_group']],on='IID',how='left')\n",
    "\n",
    "temp1 = temp1.filter(\n",
    "    (((pl.col('flag_sum_TUF')!=0) | (pl.col('flag_sum_NUF')!=0))) | \n",
    "    (pl.col('specialty_group')=='PED')\n",
    ").drop('flag_sum_TUF','flag_sum_NUF','specialty_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "724ca9e4-26aa-451d-80e5-537017248deb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp1 = process_trend(temp1,'TUF')\n",
    "temp1 = process_trend(temp1,'NUF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45e17f74-f937-4291-a95a-28d6f6f47b91",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Converting to Feed ready data\n",
    "temp1 = temp1.with_columns(ReportType = pl.lit('MONTHLY'))\n",
    "\n",
    "trx_cols = ['IID','geography_id','product_id','ReportType'] + [col for col in temp1.columns if '_TUF' in col ]\n",
    "nrx_cols = ['IID','geography_id','product_id','ReportType'] + [col for col in temp1.columns if '_NUF' in col ]\n",
    "\n",
    "temp1_TUF = temp1.select(trx_cols).with_columns(Metric = pl.lit('TRX'))\n",
    "temp1_NUF = temp1.select(nrx_cols).with_columns(Metric = pl.lit('NRX'))\n",
    "\n",
    "for df_name in ['temp1_TUF','temp1_NUF']: #renaming for vstack\n",
    "    for col in globals()[df_name].columns:\n",
    "        if col.endswith('_TUF'):\n",
    "            globals()[df_name] = globals()[df_name].rename({col: col.replace('_TUF', '')})\n",
    "        elif col.endswith('_NUF'):\n",
    "            globals()[df_name] = globals()[df_name].rename({col: col.replace('_NUF', '')})\n",
    "\n",
    "# Setting up Sequence\n",
    "final_sequence = ['IID','geography_id','product_id','Metric','ReportType','Trend'] + [f'Vol{i}' for i in range(1,14)] + [f'Shr{i}' for i in range(1,14)]\n",
    "\n",
    "temp1_TUF = temp1_TUF.select(final_sequence).rename({'IID':'Physician_ID','geography_id':'Geography_id','product_id':'Product_id'})\n",
    "temp1_NUF = temp1_NUF.select(final_sequence).rename({'IID':'Physician_ID','geography_id':'Geography_id','product_id':'Product_id'})\n",
    "\n",
    "temp2 = temp1_TUF.vstack(temp1_NUF) # final dataframe\n",
    "\n",
    "for new_col in ['DS1_Vol','DS2_Vol']:\n",
    "    for i in range(1,14):\n",
    "        col_name = f'{new_col}{i}'\n",
    "        temp2 = temp2.with_columns(pl.lit('\\\\N').alias(col_name)) #change this to /N later ? # null --> \\N (harsh)\n",
    "\n",
    "conversion_columns = temp2.columns[5:]\n",
    "temp2 = temp2.join(\n",
    "    MASTER_UNI[['IID','PDRPOptOutFlag']],left_on= 'Physician_ID',right_on='IID',how='left'\n",
    ")\n",
    "\n",
    "for c in [f'Shr{i}' for i in range(1,14)] : # Convert NaN to 0.0\n",
    "    temp2 = temp2.with_columns(pl.col(c).replace(np.nan,0.0))\n",
    "\n",
    "for col in conversion_columns:\n",
    "    temp2 = temp2.with_columns(\n",
    "        pl.when(pl.col(\"PDRPOptOutFlag\") == \"Y\").then(pl.lit('\\\\N')).otherwise(pl.col(col)).alias(col)\n",
    "    )\n",
    "temp2 = temp2.drop('PDRPOptOutFlag')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93b0209b-5215-4307-b255-26e20041b5f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presc Trend Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "# Export\n",
    "temp2.to_pandas().to_csv(f'{OUT}Monthly_Prescriber_Trend_Feed.txt', sep='|',lineterminator='\\r\\n',index=False)\n",
    "print('Presc Trend Feed Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32874e-a78b-4c5f-9f04-6068579af02b",
   "metadata": {},
   "source": [
    "### Prescriber Trend Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86c6c477-5ddb-4654-a089-cebe99109d3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presc X Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "rx_date = datetime.strptime(monthly_data_date,'%Y%m')\n",
    "list_of_dates = [rx_date]\n",
    "serial_no = [i for i in range(1,14)]\n",
    "for i in range(1,13):\n",
    "    date_val = rx_date - timedelta(days = 30*i)\n",
    "    list_of_dates.append(date_val)\n",
    "\n",
    "date_df = pl.DataFrame(\n",
    "    {\n",
    "        'X':serial_no,\n",
    "        'Name':list_of_dates\n",
    "    }\n",
    ")\n",
    "\n",
    "date_df = date_df.with_columns(\n",
    "   date_df['Name'].dt.strftime('%b-%y')\n",
    ")\n",
    "\n",
    "date_df.to_pandas().to_csv(f'{OUT}Monthly_Prescriber_X_Feed.txt', sep='|',lineterminator='\\r\\n',index=False)\n",
    "print('Presc X Feed Exported !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
