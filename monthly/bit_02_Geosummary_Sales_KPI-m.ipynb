{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e9c80f-f82b-4daf-8b9a-ed0aed68da62",
   "metadata": {},
   "source": [
    "# GS Sales KPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a43a9a-9d92-488a-aad4-8dfb36611c88",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import datetime, timedelta,date\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d21d56-2341-445a-8258-45a3c464edc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "qtr_data = js['qtr_data']\n",
    "bucket = js['bucket']\n",
    "YTD = js['YTD']\n",
    "monthly_data_date = js['monthly_data_date']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "geo = f's3://{bucket}/PYADM/quaterly/{qtr_data}/geography/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'\n",
    "mxpn = f's3://{bucket}/PYADM/monthly/archive/{monthly_data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4c7f88-d8aa-487a-9191-ec5bf52f092f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdfab9c-8aa1-4be4-a7f9-40229ba256fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "load('MASTER_UNI')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL'] # only these products are to be read from lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1af7850-e53b-4254-a399-89dfd5163920",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# loading  weekly data cut from library -\n",
    "all_products_tuf_wk = []\n",
    "for i in range(4):\n",
    "    all_products_tuf_wk.append(pl.read_parquet(f'{dflib}all_products_tuf_{i}.parquet'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877a956-d990-4fdb-933f-1e87e8985575",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19af18-fee5-47d8-a069-0a72547891e1",
   "metadata": {},
   "source": [
    "### Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4330fe-3782-4479-8ec9-90dbafaea5a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal - \n",
    "def get_lin_voucher():\n",
    "    vch = pl.read_parquet(f'{mxpn}LIN_VOUCHER.parquet') \n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod in ['LIN1','LIN2','LIN3']: # LINV\n",
    "        vch_prod = (\n",
    "            vch.select(\n",
    "                pl.col('IID'),\n",
    "                pl.col(f'{prod}TUF1').alias(f'vTUF_1c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,4)]).alias(f'vTUF_3c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,7)]).alias(f'vTUF_6c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,13)]).alias(f'vTUF_12c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(4,7)]).alias(f'vTUF_pqtrc'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,YTD+1)]).alias(f'vTUF_ytdc'),\n",
    "                pl.col(f'{prod}TUF2').alias(f'vTUF_1p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(4,7)]).alias(f'vTUF_3p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(7,13)]).alias(f'vTUF_6p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(13,25)]).alias(f'vTUF_12p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(7,10)]).alias(f'vTUF_pqtrp'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(13,13+YTD)]).alias(f'vTUF_ytdp'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,25)]).alias(f'vTUF_all') #added 105 week datacut\n",
    "            )\n",
    "            .with_columns(pl.lit(f'LI{prod[-1]}').alias('PROD_CD'))\n",
    "        )\n",
    "        if prod[-1] == '1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "\n",
    "    # voucher_mapping = {'LI1': 4, 'LI2': 5, 'LI3': 3, 'LIV': 2}\n",
    "    vch1 = vch1.fill_null(0)\n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3acbacaf-619d-4e11-b3af-014d35f32de0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data cuts - voucher + geo rollups - \n",
    "def get_summed_metric_period(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,25)]\n",
    "    df = pl.read_parquet(mxpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,3,6,12,pqtd,ytd for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,4)]).alias(metric+'_3c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,7)]).alias(metric+'_6c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,13)]).alias(metric+'_12c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(4,7)]).alias(metric+'_pqtrc'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,YTD+1)]).alias(metric+'_ytdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(4,7)]).alias(metric+'_3p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(7,13)]).alias(metric+'_6p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(13,25)]).alias(metric+'_12p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(7,10)]).alias(metric+'_pqtrp'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(13,13+YTD)]).alias(metric+'_ytdp')\n",
    "    )\n",
    "\n",
    "    # For Voucher Removal - \n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_3c') -  pl.col(f'v{metric}_3c').alias(f'{metric}_3c'),\n",
    "            pl.col(f'{metric}_6c') -  pl.col(f'v{metric}_6c').alias(f'{metric}_6c'),\n",
    "            pl.col(f'{metric}_12c') -  pl.col(f'v{metric}_12c').alias(f'{metric}_12c'),\n",
    "            pl.col(f'{metric}_pqtrc') -  pl.col(f'v{metric}_pqtrc').alias(f'{metric}_pqtrc'),\n",
    "            pl.col(f'{metric}_ytdc') -  pl.col(f'v{metric}_ytdc').alias(f'{metric}_ytdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_3p') -  pl.col(f'v{metric}_3p').alias(f'{metric}_3p'),\n",
    "            pl.col(f'{metric}_6p') -  pl.col(f'v{metric}_6p').alias(f'{metric}_6p'),\n",
    "            pl.col(f'{metric}_12p') -  pl.col(f'v{metric}_12p').alias(f'{metric}_12p'),\n",
    "            pl.col(f'{metric}_pqtrp') -  pl.col(f'v{metric}_pqtrp').alias(f'{metric}_pqtrp'),\n",
    "            pl.col(f'{metric}_ytdp') -  pl.col(f'v{metric}_ytdp').alias(f'{metric}_ytdp')\n",
    "        ).drop(cols_to_remove)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    metrics_to_calc = {}\n",
    "    for f in ['c','p']:\n",
    "        for p in [1,3,6,12,'pqtr','ytd']:\n",
    "            column = f'{metric}_{p}{f}'\n",
    "            metrics_to_calc[column] = pl.col(column).sum()\n",
    "    \n",
    "    df_terr = df.group_by(['geography_id','specialty_group','segment','decile','PROD_CD']).agg(**metrics_to_calc)\n",
    "\n",
    "    df_reg = df.join(geo_code_mapper[['geography_id','region_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['region_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**metrics_to_calc)\n",
    "\n",
    "    df_area = df.join(geo_code_mapper[['geography_id','area_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['area_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**metrics_to_calc)\n",
    "\n",
    "    df_nation = df.join(geo_code_mapper[['geography_id','nation_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['nation_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**metrics_to_calc)\n",
    "\n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4aaf67b-1385-4b80-9d16-cd4ee5272f38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# To Add Parent Product Rows -\n",
    "def add_parent_product_rows(all_prod_df):\n",
    "    # converting tuple to list , because i cant assign the processed df back to it\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    for i in range(4): \n",
    "        df = all_prod_df[i]\n",
    "        agg_dict = {}\n",
    "        for col in df.columns[5:]:\n",
    "            agg_dict[col] = pl.col(col).sum()\n",
    "        \n",
    "        join_cols = df.columns[0:4]\n",
    "\n",
    "        df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "        df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "        df_2_35 = df_2_35.group_by(join_cols + ['parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "        df_1 = df.group_by(join_cols).agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "        # stack 1, 2_35 with df and return\n",
    "        df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "        vstack_helper = df.columns\n",
    "        df = df.vstack(\n",
    "            df_2_35.select(vstack_helper)\n",
    "        ).vstack(\n",
    "            df_1.select(vstack_helper)\n",
    "        )\n",
    "\n",
    "        all_prod_df[i] = df\n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3215d6c-6400-47d7-bf93-d8328e822ed9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows_iid(df):\n",
    "    agg_dict = {}\n",
    "    for col in df.columns[2:]:\n",
    "        agg_dict[col] = pl.col(col).sum()\n",
    "    \n",
    "    #join_cols = ['geography_id','plan_type','PlanID','IID']\n",
    "\n",
    "    df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    \n",
    "    df_1 = df.group_by('IID').agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3092bb-8bd7-49d2-8f19-b19164e1f0f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# to add segment ,decile , spec etc full roll ups - \n",
    "def add_full_rollups(all_prod_df):\n",
    "    # converting the tuple of dfs into a list for processing\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    # for trivializing formulas - \n",
    "    p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    \n",
    "    #Looping over 4 levels (terr,reg,area,nation)\n",
    "    for i in range(4):\n",
    "        df = all_prod_df[i]\n",
    "        g = df.columns[0] #should contain geo level\n",
    "        metric_cols = df.columns[4:-1] #should contain the tuf / nuf columns\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols) #used for vstack later\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        #### Processing Done ####\n",
    "        df = (\n",
    "            df.select(main_seq)\n",
    "            .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "            .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "            .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        # Store Data Back :\n",
    "        all_prod_df[i] = df\n",
    "    \n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d43450eb-5694-4459-9d94-07412b1b80cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_period_prec_count_metric(metric,prod_cd):\n",
    "    # Variable Aliases to Trvialize : \n",
    "    p,sg,spc,d = 'product_id','segment','specialty_group','decile'\n",
    "    par = 'parent_product_id'\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    time_periods = [f'{metric}_1c',f'{metric}_3c',f'{metric}_6c',f'{metric}_12c',f'{metric}_pqtrc',f'{metric}_ytdc'\n",
    "                    ,f'{metric}_1p',f'{metric}_3p',f'{metric}_6p',f'{metric}_12p',f'{metric}_pqtrp',f'{metric}_ytdp'] #time periods\n",
    "    levels = ['geography_id','region_geography_id', 'area_geography_id', 'nation_geography_id'] #group levels\n",
    "    \n",
    "    # READ DATA FROM LAX AND GET DATA CUTS FOR EACH PERIOD CUR AND PRIOR AT IID AND PROD_CD LEVEL\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,25)]\n",
    "    df = pl.read_parquet(mxpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "    # 1,3,6,12,pqtd,ytd for current and prior period for a given Metri\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,4)]).alias(metric+'_3c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,7)]).alias(metric+'_6c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,13)]).alias(metric+'_12c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(4,7)]).alias(metric+'_pqtrc'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,YTD+1)]).alias(metric+'_ytdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(4,7)]).alias(metric+'_3p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(7,13)]).alias(metric+'_6p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(13,25)]).alias(metric+'_12p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(7,10)]).alias(metric+'_pqtrp'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(13,13+YTD)]).alias(metric+'_ytdp')\n",
    "    )\n",
    "    \n",
    "    # For Voucher Removal - \n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_3c') -  pl.col(f'v{metric}_3c').alias(f'{metric}_3c'),\n",
    "            pl.col(f'{metric}_6c') -  pl.col(f'v{metric}_6c').alias(f'{metric}_6c'),\n",
    "            pl.col(f'{metric}_12c') -  pl.col(f'v{metric}_12c').alias(f'{metric}_12c'),\n",
    "            pl.col(f'{metric}_pqtrc') -  pl.col(f'v{metric}_pqtrc').alias(f'{metric}_pqtrc'),\n",
    "            pl.col(f'{metric}_ytdc') -  pl.col(f'v{metric}_ytdc').alias(f'{metric}_ytdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_3p') -  pl.col(f'v{metric}_3p').alias(f'{metric}_3p'),\n",
    "            pl.col(f'{metric}_6p') -  pl.col(f'v{metric}_6p').alias(f'{metric}_6p'),\n",
    "            pl.col(f'{metric}_12p') -  pl.col(f'v{metric}_12p').alias(f'{metric}_12p'),\n",
    "            pl.col(f'{metric}_pqtrp') -  pl.col(f'v{metric}_pqtrp').alias(f'{metric}_pqtrp'),\n",
    "            pl.col(f'{metric}_ytdp') -  pl.col(f'v{metric}_ytdp').alias(f'{metric}_ytdp')\n",
    "        ).drop(cols_to_remove)\n",
    "    \n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "    # Adding Geo Hier\n",
    "    df = df.join(geo_code_mapper,on='geography_id')\n",
    "    # joining with prod_mapping - \n",
    "    df = df.join(prod_mapping[['product_id','parent_product_id','code']],left_on='PROD_CD',right_on='code').drop('PROD_CD')\n",
    "    \n",
    "    def util1(df,g,period):\n",
    "        main_seq = [g,p,sg,d,spc,f'cp_{period}']\n",
    "        main_seq2 = [g,par,sg,d,spc,f'cp_{period}']\n",
    "        main_seq3 = [g,sg,d,spc,f'cp_{period}']\n",
    "        # Removing Rows Where Volume < 0\n",
    "        df_filtered = df.filter(pl.col(period) > 0)\n",
    "        \n",
    "        # For Child Products Only :\n",
    "        # First Round :\n",
    "        sg_df = (df_filtered.group_by([g,p,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df_filtered.group_by([g,p,sg,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df_filtered.group_by([g,p,d,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df_filtered.group_by([g,p,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df_filtered.group_by([g,p,d]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df_filtered.group_by([g,p,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc),d_roll_up.alias(d)).select(main_seq))\n",
    "        #Third Round -\n",
    "        sg_d_spc_df = (df_filtered.group_by([g,p]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        whole_df = (df_filtered.group_by([g,p,sg,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).select(main_seq))\n",
    "        df_filtered_child_all_rollups = (whole_df.vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "    \n",
    "        # For Parent Products 2_35:\n",
    "        df_filtered_2_35 = df_filtered.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "        # First Round :\n",
    "        sg_df = (df_filtered_2_35.group_by([g,par,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg)).select(main_seq2))\n",
    "        d_df = (df_filtered_2_35.group_by([g,par,sg,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(d_roll_up.alias(d)).select(main_seq2))\n",
    "        spc_df = (df_filtered_2_35.group_by([g,par,d,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc)).select(main_seq2))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df_filtered_2_35.group_by([g,par,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq2))\n",
    "        sg_spc_df = (df_filtered_2_35.group_by([g,par,d]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq2))\n",
    "        d_spc_df = (df_filtered_2_35.group_by([g,par,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc),d_roll_up.alias(d)).select(main_seq2))\n",
    "        #Third Round -\n",
    "        sg_d_spc_df = (df_filtered_2_35.group_by([g,par]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq2))\n",
    "        whole_df = (df_filtered_2_35.group_by([g,par,sg,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).select(main_seq2))\n",
    "        df_filtered_2_35_all_rollups = (whole_df.vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df)).rename({par:p})\n",
    "    \n",
    "        # For All Products - \n",
    "        # First Round :\n",
    "        sg_df = (df_filtered.group_by([g,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg)).select(main_seq3))\n",
    "        d_df = (df_filtered.group_by([g,sg,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(d_roll_up.alias(d)).select(main_seq3))\n",
    "        spc_df = (df_filtered.group_by([g,d,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc)).select(main_seq3))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df_filtered.group_by([g,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq3))\n",
    "        sg_spc_df = (df_filtered.group_by([g,d]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq3))\n",
    "        d_spc_df = (df_filtered.group_by([g,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc),d_roll_up.alias(d)).select(main_seq3))\n",
    "        #Third Round -\n",
    "        sg_d_spc_df = (df_filtered.group_by([g]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq3))\n",
    "        whole_df = (df_filtered.group_by([g,sg,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).select(main_seq3))\n",
    "        df_filtered_lax_all_rollups = (\n",
    "            whole_df.vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df)\n",
    "            .with_columns(pl.lit(1).alias(p)).select(main_seq).with_columns(pl.col(p).cast(pl.Int64))\n",
    "        )\n",
    "    \n",
    "        # stack 1, 2_35 ,child\n",
    "        df_filtered_all_prod_all_roll = df_filtered_child_all_rollups.vstack(df_filtered_2_35_all_rollups).vstack(df_filtered_lax_all_rollups)\n",
    "    \n",
    "        return(df_filtered_all_prod_all_roll)\n",
    "    \n",
    "    df_terr = None\n",
    "    df_reg = None\n",
    "    df_area = None\n",
    "    df_nation = None #initialize the output dfs\n",
    "    \n",
    "    for period in time_periods:\n",
    "        for g in levels:\n",
    "            new_col_df = util1(df,g,period)\n",
    "            # If the dataframes are not initialized, assign df_period to them\n",
    "            if g == 'geography_id' and df_terr is None:\n",
    "                df_terr = new_col_df\n",
    "            elif g == 'region_geography_id' and df_reg is None:\n",
    "                df_reg = new_col_df\n",
    "            elif g == 'area_geography_id' and df_area is None:\n",
    "                df_area = new_col_df\n",
    "            elif g == 'nation_geography_id' and df_nation is None:\n",
    "                df_nation = new_col_df\n",
    "            else:\n",
    "                if g == 'geography_id':\n",
    "                    df_terr = df_terr.join(new_col_df,on=[g,spc,sg,d,p],how='outer_coalesce')\n",
    "                elif g == 'region_geography_id':\n",
    "                    df_reg = df_reg.join(new_col_df,on=[g,spc,sg,d,p],how='outer_coalesce')\n",
    "                elif g == 'area_geography_id':\n",
    "                    df_area = df_area.join(new_col_df,on=[g,spc,sg,d,p],how='outer_coalesce')\n",
    "                elif g == 'nation_geography_id':\n",
    "                    df_nation = df_nation.join(new_col_df,on=[g,spc,sg,d,p],how='outer_coalesce')\n",
    "    \n",
    "    df_terr = df_terr.fill_null(0)\n",
    "    df_reg = df_reg.fill_null(0)\n",
    "    df_area = df_area.fill_null(0)\n",
    "    df_nation = df_nation.fill_null(0)\n",
    "\n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc24abb6-7593-4b52-bf81-c57c67109d67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_period_iid_metric(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,25)]\n",
    "    df = pl.read_parquet(mxpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,3,6,12,pqtd,ytd for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,4)]).alias(metric+'_3c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,7)]).alias(metric+'_6c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,13)]).alias(metric+'_12c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(4,7)]).alias(metric+'_pqtrc'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,YTD+1)]).alias(metric+'_ytdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(4,7)]).alias(metric+'_3p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(7,13)]).alias(metric+'_6p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(13,25)]).alias(metric+'_12p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(7,10)]).alias(metric+'_pqtrp'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(13,13+YTD)]).alias(metric+'_ytdp'),\n",
    "\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,25)]).alias(metric+'_all')\n",
    "    )\n",
    "\n",
    "    # For Voucher Removal - \n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_3c') -  pl.col(f'v{metric}_3c').alias(f'{metric}_3c'),\n",
    "            pl.col(f'{metric}_6c') -  pl.col(f'v{metric}_6c').alias(f'{metric}_6c'),\n",
    "            pl.col(f'{metric}_12c') -  pl.col(f'v{metric}_12c').alias(f'{metric}_12c'),\n",
    "            pl.col(f'{metric}_pqtrc') -  pl.col(f'v{metric}_pqtrc').alias(f'{metric}_pqtrc'),\n",
    "            pl.col(f'{metric}_ytdc') -  pl.col(f'v{metric}_ytdc').alias(f'{metric}_ytdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_3p') -  pl.col(f'v{metric}_3p').alias(f'{metric}_3p'),\n",
    "            pl.col(f'{metric}_6p') -  pl.col(f'v{metric}_6p').alias(f'{metric}_6p'),\n",
    "            pl.col(f'{metric}_12p') -  pl.col(f'v{metric}_12p').alias(f'{metric}_12p'),\n",
    "            pl.col(f'{metric}_pqtrp') -  pl.col(f'v{metric}_pqtrp').alias(f'{metric}_pqtrp'),\n",
    "            pl.col(f'{metric}_ytdp') -  pl.col(f'v{metric}_ytdp').alias(f'{metric}_ytdp'),\n",
    "            pl.col(f'{metric}_all') -  pl.col(f'v{metric}_all').alias(f'{metric}_all')\n",
    "        ).drop(cols_to_remove)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    return(df.drop(['specialty_group','segment','decile','geography_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a095ff-1cd7-47e7-80c6-f0bbfb60b1f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data Prep : ETA 20 Seconds\n",
    "# Calling Function to Process Summed Metric Values\n",
    "all_products_tuf = get_summed_metric_period('TUF',fetch_products) # this is a tuple containg all 4 rollups\n",
    "all_products_nuf = get_summed_metric_period('NUF',fetch_products)\n",
    "all_products_trx =  get_summed_metric_period('TRX',fetch_products)\n",
    "all_products_nrx =  get_summed_metric_period('NRX',fetch_products)\n",
    "all_products_tun =  get_summed_metric_period('TUN',fetch_products)\n",
    "all_products_nun =  get_summed_metric_period('NUN',fetch_products)\n",
    "\n",
    "# calling function to add parent product rows to it.\n",
    "all_products_tuf = add_parent_product_rows(all_products_tuf)\n",
    "all_products_nuf = add_parent_product_rows(all_products_nuf)\n",
    "all_products_trx = add_parent_product_rows(all_products_trx)\n",
    "all_products_nrx = add_parent_product_rows(all_products_nrx)\n",
    "all_products_tun = add_parent_product_rows(all_products_tun)\n",
    "all_products_nun = add_parent_product_rows(all_products_nun)\n",
    "\n",
    "# Adding Full Rol Ups - \n",
    "all_products_tuf = add_full_rollups(all_products_tuf)\n",
    "all_products_nuf = add_full_rollups(all_products_nuf)\n",
    "all_products_trx = add_full_rollups(all_products_trx)\n",
    "all_products_nrx = add_full_rollups(all_products_nrx)\n",
    "all_products_tun = add_full_rollups(all_products_tun)\n",
    "all_products_nun = add_full_rollups(all_products_nun)\n",
    "\n",
    "# Calling Function to process count of Prec ('metric_period' >= 1 only)\n",
    "all_products_tuf_hcp = get_period_prec_count_metric('TUF',fetch_products)\n",
    "\n",
    "#IID level data for grower and decliner-\n",
    "all_products_tuf_iid = get_summed_period_iid_metric('TUF',fetch_products)\n",
    "all_products_tuf_iid = add_parent_product_rows_iid(all_products_tuf_iid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041b2dc-e4e9-4617-ba24-d295ea4d423d",
   "metadata": {},
   "source": [
    "Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "591ee8c1-0627-44c9-970a-37d9b9cad41d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Basic metrics from data cuts (tuf , nuf , trx etc) - \n",
    "def process_1(df):\n",
    "    hold = [[f'{m}{period}c',f'{m}{period}p'] for m in ['TUN','TRX','TUF','NUN','NRX','NUF']]\n",
    "    for i in range(4):\n",
    "        g = levels[i]\n",
    "        gb_helper = [g,spc,sg,d,p]\n",
    "        f = (\n",
    "            all_products_tun[i][gb_helper + hold[0]]\n",
    "            .join(all_products_trx[i][gb_helper + hold[1]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_tuf[i][gb_helper + hold[2]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_nun[i][gb_helper + hold[3]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_nrx[i][gb_helper + hold[4]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_nuf[i][gb_helper + hold[5]],on = gb_helper,how = 'left')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03cb44b0-ba71-4681-bd32-485f06b790dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Trx, Nrx : Size and Size Change ,  Average Trx\n",
    "def process_2(df,period_num):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                avg_trx_size = pl.col(f'TUN{period}c')/pl.col(f'TRX{period}c'),\n",
    "                avg_nrx_size = pl.col(f'NUN{period}c')/pl.col(f'NRX{period}c'),\n",
    "                prior_avg_trx_size = pl.col(f'TUN{period}p')/pl.col(f'TRX{period}p'),\n",
    "                prior_avg_nrx_size = pl.col(f'NUN{period}p')/pl.col(f'NRX{period}p')\n",
    "            )\n",
    "            .with_columns(\n",
    "                avg_trx_size_ch = pl.col('avg_trx_size')-pl.col('prior_avg_trx_size'),\n",
    "                avg_nrx_size_ch = pl.col('avg_nrx_size')-pl.col('prior_avg_nrx_size')\n",
    "            )\n",
    "            .with_columns(\n",
    "                avg_trx = pl.col(f'TUF{period}c')/period_num #no avg_nrx ? \n",
    "            )\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ab9d73-bdfd-42fa-ad4b-369b04c2f90c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#prc_shr_bus\n",
    "def process_3(df):\n",
    "    for i in range(3):\n",
    "        gmap = geo_code_mapper[[levels[i],levels[i+1]]].unique()\n",
    "        fp = df[i+1].select(levels[i+1],spc,sg,d,p,f'TUF{period}c',f'NUF{period}c')\n",
    "        f = df[i]\n",
    "        ft = (\n",
    "            f\n",
    "            .select(levels[i],spc,sg,d,p,f'TUF{period}c',f'NUF{period}c')\n",
    "            .join(gmap,on = levels[i],how ='left')\n",
    "            .join(fp,on = [levels[i+1],spc,sg,d,p],how='left')\n",
    "            .with_columns(\n",
    "                prc_shr_bus_trx = pl.col(f'TUF{period}c')/pl.col(f'TUF{period}c_right'),\n",
    "                prc_shr_bus_nrx = pl.col(f'NUF{period}c')/pl.col(f'NUF{period}c_right')\n",
    "            )\n",
    "            .drop([levels[i+1],f'TUF{period}c',f'NUF{period}c',f'TUF{period}c_right',f'NUF{period}c_right'])\n",
    "        )\n",
    "        f = f.join(ft,on = [levels[i],spc,sg,d,p],how = 'left')\n",
    "        df[i] = f\n",
    "    df[3] = df[3].with_columns(\n",
    "        pl.lit(None).alias('prc_shr_bus_trx'),pl.lit(None).alias('prc_shr_bus_nrx')\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81b438a-8427-4488-af25-6ac7de24db75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#rr_4v13 _ NEED TO IMPORT THIS FROM WEEKLY BIT : TO DOOOO\n",
    "def process_4(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        source_df = all_products_tuf_wk[i][[levels[i],p,sg,d,spc,'TUF_4c','TUF_13c']]\n",
    "        f = f.join(source_df,on = [levels[i],p,sg,d,spc],how='left').with_columns(\n",
    "            rr_4v13 = (pl.col('TUF_4c')*(13/4)) - pl.col('TUF_13c')\n",
    "        ).drop(['TUF_4c','TUF_13c'])\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "391845db-8128-41e1-ac82-3c40ba81c23b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#count of prec , ind\n",
    "def process_5(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        formula = ((pl.col('num_hcp')-pl.col('pri_num_hcp'))/pl.col('pri_num_hcp')).alias('ind_metric')\n",
    "\n",
    "        source_df = all_products_tuf_hcp[i][[levels[i],spc,sg,d,p,f'cp_TUF{period}c',f'cp_TUF{period}p']]\n",
    "        f = f.join(source_df,[levels[i],spc,sg,d,p],how='left').rename({f'cp_TUF{period}c':'num_hcp',f'cp_TUF{period}p':'pri_num_hcp'}\n",
    "        ).with_columns(pl.col('num_hcp').fill_null(0),pl.col('pri_num_hcp').fill_null(0)\n",
    "        ).with_columns(formula\n",
    "        ).with_columns(pl.col('ind_metric').fill_nan(None)\n",
    "        ).with_columns(\n",
    "            pl.when(pl.col('ind_metric') > 0.02).then(pl.lit('P'))\n",
    "            .when(pl.col('ind_metric') < -0.02).then(pl.lit('Q'))\n",
    "            .otherwise(None).alias('num_hcp_ind')\n",
    "        ).drop(['pri_num_hcp','ind_metric'])\n",
    "\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6650dd4-a1dc-4155-adfd-ab74fb3901ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Num_Of_Prescribers_BnchMrk_Ind\n",
    "def process_bnch_presc(df,metric,ind_col_name):# WORKING CORRECTLY BUT NOT MODULAR \n",
    "    def add_indicator(df, ind_name, col1, col2, col3):\n",
    "        return df.with_columns(\n",
    "            pl.when(pl.col(col1) > pl.col(col2))\n",
    "            .then(pl.lit('A'))\n",
    "            .when(pl.col(col1) < pl.col(col3))\n",
    "            .then(pl.lit('B'))\n",
    "            .when((pl.col(col3) <= pl.col(col1)) & (pl.col(col1) <= pl.col(col2)))\n",
    "            .then(pl.lit('E'))\n",
    "            .otherwise(None)  # You can replace 'N/A' with any default value\n",
    "            .alias(ind_name)\n",
    "        )\n",
    "    #Terr\n",
    "    f = df[0]\n",
    "    nf = f.select([levels[0],p,sg,spc,d,metric])\n",
    "    nf = nf.join(geo_code_mapper,on = levels[0],how = 'left')\n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        all = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_r = nf.group_by([levels[1],p,sg,spc,d]).agg(\n",
    "        rul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        rll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "        .join(nf_r,on=[levels[1],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[1],levels[2],levels[3])\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1', metric, 'nul', 'nll')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind2', metric, 'aul', 'all')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind3', metric, 'rul', 'rll').drop(['nul','nll','aul','all','rul','rll',metric])\n",
    "    f = f.join(nf,on=[levels[0],p, sg, spc, d],how = 'left')\n",
    "    df[0] = f\n",
    "    #Region\n",
    "    f = df[1]\n",
    "    nf = f.select([levels[1],p,sg,spc,d,metric]).join(\n",
    "        geo_code_mapper[['region_geography_id','area_geography_id']].unique(),on = levels[1],how = 'left'\n",
    "    )\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        all = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[2],levels[3])\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1', metric, 'nul', 'nll')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind2', metric, 'aul', 'all')\n",
    "    nf = nf.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3')).drop(['nul','nll','aul','all',metric])\n",
    "    f = f.join(nf,on=[levels[1],p, sg, spc, d],how = 'left')\n",
    "    df[1] = f\n",
    "    #Area\n",
    "    f = df[2]\n",
    "    nf = f.select([levels[2],p,sg,spc,d,metric])\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (nf.join(nf_n, on=[p, sg, spc, d], how='left'))\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1', metric, 'nul', 'nll')\n",
    "    nf = nf.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind2'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3')).drop(['nul','nll',metric])\n",
    "    f = f.join(nf,on=[levels[2],p, sg, spc, d],how = 'left')\n",
    "    df[2] = f\n",
    "    #Nation \n",
    "    f = df[3]\n",
    "    f = f.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind1'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind2'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3'))\n",
    "    df[3] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7ba595e-b88c-4b9a-b1ce-f8f238561c4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_terr_cgd():    #count of growers and decliners -> requires all_products_tuf_iid to be set up before hand\n",
    "    i = 0 #keep it locked\n",
    "    source_df = (\n",
    "        all_products_tuf_iid[['IID','product_id',f'TUF{period}c',f'TUF{period}p']]\n",
    "        .rename({f'TUF{period}c':'cur_vol',f'TUF{period}p':'pri_vol'})\n",
    "        .with_columns(pl.col('cur_vol').round(1),pl.col('pri_vol').round(1))\n",
    "        .with_columns(vol_change = pl.col('cur_vol')-pl.col('pri_vol'))\n",
    "        .with_columns(pl.col('vol_change').round(1))\n",
    "        .join(mp_spec_seg_dec,on = 'IID',how = 'left')\n",
    "        .join(geo_code_mapper,on = levels[0],how ='left') #keeping levels 0 as base data is only on iid-terr level\n",
    "    )\n",
    "    \n",
    "    source_df_reduced = (\n",
    "        source_df\n",
    "        .join(MASTER_UNI.select(['IID','PDRPOptOutFlag']),on='IID',how='left')\n",
    "        .filter(pl.col('PDRPOptOutFlag')!='Y')\n",
    "        .filter(pl.col('segment')=='Target')\n",
    "        .filter((pl.col('pri_vol')!=0) & (pl.col('pri_vol').is_not_null()))\n",
    "        .filter((pl.col('vol_change')!=0))\n",
    "    )\n",
    "    \n",
    "    # for 10th perc\n",
    "    source_df_1 = source_df_reduced.filter(\n",
    "        pl.col('vol_change')<0\n",
    "    )\n",
    "    # for 90th perc\n",
    "    source_df_2 = source_df_reduced.filter(\n",
    "        pl.col('vol_change')>0\n",
    "    )\n",
    "    \n",
    "    source_df_percentile_10 = source_df_1.group_by(levels[i+1],p).agg(\n",
    "        ten_perc = pl.col('vol_change').quantile(0.1,interpolation='linear')\n",
    "    )\n",
    "    \n",
    "    source_df_percentile_90 = source_df_2.group_by(levels[i+1],p).agg(\n",
    "        nin_perc = pl.col('vol_change').quantile(0.9,interpolation='linear')\n",
    "    )\n",
    "    \n",
    "    source_df_percentile = source_df_percentile_10.join(source_df_percentile_90,on=[levels[i+1],p],how='outer_coalesce')\n",
    "    \n",
    "    source_df = source_df.join(source_df_percentile,on=[levels[i+1],p],how='left')\n",
    "    \n",
    "    source_df = source_df.with_columns(\n",
    "        pl.when((pl.col('vol_change')<=0) & (pl.col('vol_change') < pl.col('ten_perc'))).then(pl.lit('DECLINER'))\n",
    "        .when((pl.col('vol_change')>0) & (pl.col('vol_change') > pl.col('nin_perc'))).then(pl.lit('GROWER'))\n",
    "        .otherwise(pl.lit(None))\n",
    "        .alias('TYPE')\n",
    "    )\n",
    "    \n",
    "    # removing PDRP from source : \n",
    "    source_df= source_df.join(MASTER_UNI.select(['IID','PDRPOptOutFlag']),on='IID',how='left')\n",
    "    \n",
    "    source_df = source_df.with_columns(\n",
    "        pl.when(pl.col('PDRPOptOutFlag')=='Y').then(pl.lit('PDRP')).otherwise(pl.col('TYPE')).alias('TYPE')\n",
    "    )\n",
    "    \n",
    "    cg = source_df.filter(pl.col('TYPE')=='GROWER').group_by([levels[i],spc,sg,d,p]).agg(num_growers = pl.col('IID').n_unique())\n",
    "    cd = source_df.filter(pl.col('TYPE')=='DECLINER').group_by([levels[i],spc,sg,d,p]).agg(num_decliners = pl.col('IID').n_unique())\n",
    "    cgd = cg.join(cd,on = [levels[i],spc,sg,d,p],how='outer_coalesce').with_columns(pl.col('num_growers').fill_null(0),pl.col('num_decliners').fill_null(0))\n",
    "    \n",
    "    def add_all_roll_up_cgd(df):\n",
    "        g = levels[i]\n",
    "        p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "        sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "        metric_cols = ['num_growers','num_decliners']\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols)\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (\n",
    "                df.select(main_seq)\n",
    "                .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "                .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "                .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        return(df)\n",
    "    \n",
    "    cgd = add_all_roll_up_cgd(cgd)\n",
    "    return(cgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdebd622-0cca-415b-9b73-60fef8a2c8b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_terr_cnp(cop): # count of new prescribers\n",
    "    i = 0\n",
    "    source_df = (\n",
    "        all_products_tuf_iid[['IID','product_id',f'TUF{period}{cop}',f'TUF_all']]\n",
    "        .rename({f'TUF{period}{cop}':'cur_vol'})\n",
    "        .with_columns(old_volume = pl.col('TUF_all')-pl.col('cur_vol'))\n",
    "        .filter((pl.col('cur_vol')>0) & (pl.col('old_volume')==0)) \n",
    "        .join(mp_spec_seg_dec,on = 'IID',how = 'left')\n",
    "        .filter(pl.col('old_volume')==0)\n",
    "        .with_columns(TYPE = pl.lit('NEW'))\n",
    "        .group_by([levels[i],spc,sg,d,p])\n",
    "        .agg(num_new_prec = pl.col('IID').n_unique())\n",
    "    )\n",
    "    def add_all_roll_up_cnp(df):\n",
    "        g = levels[i]\n",
    "        p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "        sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "        metric_cols = ['num_new_prec']\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols)\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (\n",
    "                df.select(main_seq)\n",
    "                .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "                .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "                .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        return(df)\n",
    "\n",
    "    source_df = add_all_roll_up_cnp(source_df)\n",
    "\n",
    "    return(source_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad6cbf87-8ecf-4acf-9364-bbba7ec1c253",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# num_new_prec ind_metric num_growers num_decliners\n",
    "def process_6(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        formula = ((pl.col('num_new_prec')-pl.col('pri_num_new_prec'))/pl.col('pri_num_new_prec')).alias('ind_metric')\n",
    "        #if at terr level then no need to roll up:\n",
    "        if i==0:\n",
    "            f = (\n",
    "                f\n",
    "                .join(cgd,on=[levels[i],spc,sg,d,p],how='left')\n",
    "                .join(cnp,on=[levels[i],spc,sg,d,p],how='left')\n",
    "                .with_columns(formula)\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('ind_metric') > 0.02).then(pl.lit('P'))\n",
    "                    .when(pl.col('ind_metric') < -0.02).then(pl.lit('Q'))\n",
    "                    .otherwise(None).alias('new_prec_ind')\n",
    "                ).drop(['pri_num_new_prec','ind_metric'])\n",
    "            )\n",
    "        else:\n",
    "            source_df_1 = cgd.join(geo_code_mapper,on = levels[0],how='left').group_by([levels[i],spc,sg,d,p]).agg(\n",
    "                num_growers = pl.col('num_growers').sum(),num_decliners = pl.col('num_decliners').sum()\n",
    "            )\n",
    "            source_df_2 = cnp.join(geo_code_mapper,on = levels[0],how='left').group_by([levels[i],spc,sg,d,p]).agg(\n",
    "                num_new_prec = pl.col('num_new_prec').sum(),pri_num_new_prec = pl.col('pri_num_new_prec').sum()\n",
    "            )\n",
    "            f = (\n",
    "                f\n",
    "                .join(source_df_1,on=[levels[i],spc,sg,d,p],how='left')\n",
    "                .join(source_df_2,on=[levels[i],spc,sg,d,p],how='left')\n",
    "                .with_columns(formula)\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('ind_metric') > 0.02).then(pl.lit('P'))\n",
    "                    .when(pl.col('ind_metric') < -0.02).then(pl.lit('Q'))\n",
    "                    .otherwise(None).alias('new_prec_ind')\n",
    "                ).drop(['pri_num_new_prec','ind_metric'])\n",
    "            )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a5bbbe8-4043-4ae8-a2c0-1816b1e5093d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#trx size change , and nrx size change\n",
    "def process_7(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                pl.when(pl.col('avg_trx_size_ch') > 0.005).then(pl.lit('P'))\n",
    "                .when(pl.col('avg_trx_size_ch') < -0.005).then(pl.lit('Q'))\n",
    "                .when(pl.col('avg_trx_size_ch')==0).then(None)\n",
    "                .otherwise(None).alias('avg_trx_size_ch_ind'),\n",
    "                \n",
    "                pl.when(pl.col('avg_nrx_size_ch') > 0.005).then(pl.lit('P'))\n",
    "                .when(pl.col('avg_nrx_size_ch') < -0.005).then(pl.lit('Q'))\n",
    "                .when(pl.col('avg_nrx_size_ch')==0).then(None)\n",
    "                .otherwise(None).alias('avg_nrx_size_ch_ind'),\n",
    "            )\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c06b8b8b-3336-44d7-a8ab-d2d9cfcc5862",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Convert To Feed Ready data\n",
    "def get_feed(temp1):\n",
    "    drop_cols = [f'{m}{period}{suffix}' for m in ['TUN','TRX','TUF','NUN','NRX','NUF'] for suffix in ['c', 'p']]\n",
    "    drop_cols += ['prior_avg_trx_size','prior_avg_nrx_size','prc_shr_bus_nrx']\n",
    "    for i in range(4):\n",
    "        temp1[i] = temp1[i].drop(drop_cols)\n",
    "    \t\n",
    "    temp1[0] = temp1[0].rename({'geography_id': 'Geography_id'})\n",
    "    temp1[1] = temp1[1].rename({'region_geography_id': 'Geography_id'})\n",
    "    temp1[2] = temp1[2].rename({'area_geography_id': 'Geography_id'})\n",
    "    temp1[3] = temp1[3].rename({'nation_geography_id': 'Geography_id'})\n",
    "    final_feed = temp1[0].vstack(temp1[1]).vstack(temp1[2]).vstack(temp1[3])\n",
    "    #Renaming columns\n",
    "    new_col_mapping = {\n",
    "        'product_id': 'Product_id',\n",
    "        'segment': 'Segment',\n",
    "        'specialty_group': 'Specialty',\n",
    "        'decile': 'Decile',\n",
    "        'prc_shr_bus_trx': 'Share_of_Business_Prc',\n",
    "        'rr_4v13': 'Run_Rate_Change_4v13',\n",
    "        'num_hcp': 'Num_Of_Prescribers',\n",
    "        'num_hcp_ind': 'Num_Of_Prescribers_Ind',\n",
    "        'num_new_prec': 'Num_Of_New_Prescribers',\n",
    "        'new_prec_ind': 'Num_Of_New_Prescribers_Ind',\n",
    "        'num_growers': 'Num_Of_Growers',\n",
    "        'num_decliners': 'Num_Of_Decliners',\n",
    "        'avg_trx_size': 'Avg_TRx_Size',\n",
    "        'avg_trx': 'Avg_TRx',\n",
    "        'avg_trx_size_ch': 'Avg_TRx_Size_Change',\n",
    "        'avg_trx_size_ch_ind': 'Avg_TRx_Size_Change_Ind',\n",
    "        'avg_nrx_size': 'Avg_NRx_Size',\n",
    "        'avg_nrx_size_ch': 'Avg_NRx_Size_Change',\n",
    "        'avg_nrx_size_ch_ind': 'Avg_NRx_Size_Change_Ind'\n",
    "    } \n",
    "    final_feed = final_feed.rename(new_col_mapping)\n",
    "    #required new columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addz = ['Num_Of_New_To_Brand']\n",
    "    col_to_addna = ['Num_Of_Growers_Ind','Num_Of_Decliners_Ind','Num_Of_New_To_Brand_Ind','TRx_Goal','Prc_TRx_Attainment']\n",
    "    # func to add columns with desired value\n",
    "    def addcol(df,columns_to_add,wtl):\n",
    "        for my_col in columns_to_add:\n",
    "            df = df.with_columns(pl.lit(wtl).alias(my_col))\n",
    "        return df\n",
    "    \n",
    "    final_feed = addcol(final_feed,col_to_addrt,'MONTHLY')\n",
    "    final_feed = final_feed.with_columns(pl.lit(f'{pld[PN]}').alias('Period'))\n",
    "    final_feed = addcol(final_feed,col_to_addz,0)\n",
    "    final_feed = addcol(final_feed,col_to_addna,'\\\\N')\n",
    "\n",
    "    #changing values according to feed of SAS. - 06/20\n",
    "    final_feed = final_feed.with_columns(\n",
    "        pl.when(pl.col('Segment')=='ALG-ONLY-TARGET')\n",
    "        .then(pl.lit('AGNT'))\n",
    "        .when(pl.col('Segment')=='Non-Target')\n",
    "        .then(pl.lit('NT'))\n",
    "        .when(pl.col('Segment')=='Target')\n",
    "        .then(pl.lit('T'))\n",
    "        .otherwise(pl.col('Segment'))\n",
    "        .alias('Segment'))\n",
    "    \n",
    "    # arranging columns according to feed\n",
    "    req_cols = ['Geography_id', 'Product_id', 'Segment', 'Specialty', 'ReportType', 'Period', 'Decile', 'Share_of_Business_Prc', \n",
    "                'Run_Rate_Change_4v13', 'Num_Of_Prescribers', 'Num_Of_Prescribers_Ind', 'Num_Of_Prescribers_BnchMrk_Ind1', \n",
    "                'Num_Of_Prescribers_BnchMrk_Ind2', 'Num_Of_Prescribers_BnchMrk_Ind3', 'Num_Of_New_Prescribers', 'Num_Of_New_Prescribers_Ind', \n",
    "                'Num_Of_Growers', 'Num_Of_Growers_Ind', 'Num_Of_Decliners', 'Num_Of_Decliners_Ind', 'Avg_TRx_Size', 'Avg_TRx_Size_BnchMrk_Ind1', \n",
    "                'Avg_TRx_Size_BnchMrk_Ind2', 'Avg_TRx_Size_BnchMrk_Ind3', 'Avg_TRx', 'Num_Of_New_To_Brand', 'Num_Of_New_To_Brand_Ind', \n",
    "                'TRx_Goal', 'Prc_TRx_Attainment', 'Avg_TRx_Size_Change', 'Avg_TRx_Size_Change_Ind', 'Avg_TRx_Size_Change_BnchMrk_Ind1', \n",
    "                'Avg_TRx_Size_Change_BnchMrk_Ind2', 'Avg_TRx_Size_Change_BnchMrk_Ind3', 'Avg_NRx_Size', 'Avg_NRx_Size_BnchMrk_Ind1', \n",
    "                'Avg_NRx_Size_BnchMrk_Ind2', 'Avg_NRx_Size_BnchMrk_Ind3', 'Avg_NRx_Size_Change', 'Avg_NRx_Size_Change_Ind', \n",
    "                'Avg_NRx_Size_Change_BnchMrk_Ind1', 'Avg_NRx_Size_Change_BnchMrk_Ind2', 'Avg_NRx_Size_Change_BnchMrk_Ind3']\n",
    "    final_feed = final_feed.select(req_cols)\n",
    "    \n",
    "     #-----------------------------------------#'\n",
    "    \n",
    "    columns_to_round10 = ['Share_of_Business_Prc']\n",
    "    columns_to_round3 = ['Run_Rate_Change_4v13','Avg_TRx_Size','Avg_TRx','Avg_NRx_Size',]\n",
    "    columns_to_round1 = ['Avg_TRx_Size_Change','Avg_NRx_Size_Change']\n",
    "    #columns_to_round0 = ['Num_Of_New_Prescribers','Num_Of_Growers','Num_Of_Decliners']\n",
    "\n",
    "    final_feed = final_feed.with_columns([\n",
    "        *[pl.col(col).round(1).alias(col) for col in columns_to_round1],\n",
    "        *[pl.col(col).round(3).alias(col) for col in columns_to_round3],\n",
    "        *[pl.col(col).round(10).alias(col) for col in columns_to_round10],\n",
    "    ])\n",
    "\n",
    "    #OVerrides -\n",
    "    final_feed = final_feed.with_columns(pl.col('Num_Of_New_Prescribers').replace(None,0))\n",
    "\n",
    "    return (final_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd29202-c923-4370-8279-b95f8e3c8ea9",
   "metadata": {},
   "source": [
    "Period Loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa3df3b2-f297-44a3-a954-c470421df344",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for trvializing formula : \n",
    "p,sg,spc,d = 'product_id','segment','specialty_group','decile'\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "pld = {1:'1-MONTH',2:'3-MONTHS',3:'6-MONTHS',4:'12-MONTHS',6:'PQTD',7:'YTD'}\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/GeoSummary/Monthly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd2ea8fa-d266-4489-8af2-3d8f41860142",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 1!\n",
      "Exported Feed 2!\n",
      "Exported Feed 3!\n",
      "Exported Feed 4!\n",
      "Exported Feed 6!\n",
      "Exported Feed 7!\n"
     ]
    }
   ],
   "source": [
    "for period_num,PN in zip([1,3,6,12,'pqtr','ytd'],[1,2,3,4,6,7]):\n",
    "    # period_num,PN = 1,1\n",
    "    period = f'_{period_num}'\n",
    "    \n",
    "    temp1 = [pl.DataFrame() for _ in range(4)] # creating an empty dataframe holder list obj\n",
    "    temp1 = process_1(temp1)\n",
    "    \n",
    "    if PN == 6:\n",
    "        temp1 = process_2(temp1, 3)\n",
    "    elif PN == 7:\n",
    "        temp1 = process_2(temp1, YTD)\n",
    "    else:\n",
    "        temp1 = process_2(temp1, period_num)\n",
    "    \n",
    "    temp1 = process_3(temp1)\n",
    "    temp1 = process_4(temp1)\n",
    "    temp1 = process_5(temp1)\n",
    "    temp1 = process_bnch_presc(temp1,'num_hcp','Num_Of_Prescribers')\n",
    "    cgd = get_terr_cgd()\n",
    "    cnp = (\n",
    "        get_terr_cnp('c')\n",
    "        .join(\n",
    "            get_terr_cnp('p').rename({'num_new_prec':'pri_num_new_prec'}),\n",
    "            on = [levels[0],p,sg,d,spc],how = 'outer_coalesce'\n",
    "        ).with_columns(\n",
    "            pl.col('num_new_prec').fill_null(0),pl.col('pri_num_new_prec').fill_null(0)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    temp1 = process_6(temp1)\n",
    "    temp1 = process_7(temp1)\n",
    "    \n",
    "    # Reusing Function from HCP benchmark , assuming same upper and lower limit logic is applied.\n",
    "    #Avg_TRx_Size_BnchMrk_Ind\n",
    "    temp1 = process_bnch_presc(temp1,'avg_trx_size','Avg_TRx_Size')\n",
    "    #Avg_TRx_Size_Change_BnchMrk_Ind\n",
    "    temp1 = process_bnch_presc(temp1,'avg_trx_size_ch','Avg_TRx_Size_Change')\n",
    "    #Avg_NRx_Size_BnchMrk_Ind\n",
    "    temp1 = process_bnch_presc(temp1,'avg_nrx_size','Avg_NRx_Size')\n",
    "    #Avg_NRx_Size_Change_BnchMrk_Ind\n",
    "    temp1 = process_bnch_presc(temp1,'avg_nrx_size_ch','Avg_NRx_Size_Change')\n",
    "    \n",
    "    feed_dataset = get_feed(temp1)\n",
    "    \n",
    "     #===================================================\n",
    "    feed_dataset = feed_dataset.to_pandas()\n",
    "    string_columns = feed_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "    feed_dataset[string_columns] = feed_dataset[string_columns].fillna('\\\\N')\n",
    "    feed_dataset = feed_dataset.replace([np.nan, np.inf, -np.inf,'NaN'], '\\\\N')\n",
    "    feed_dataset.to_csv(f'{OUT}Monthly_GeoSummary_SalesKPIs_P{PN}_Feed.txt', sep='|', lineterminator='\\r\\n',index=False)\n",
    "    print(f'Exported Feed {PN}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
