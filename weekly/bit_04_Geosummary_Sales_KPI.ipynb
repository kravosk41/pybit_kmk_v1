{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GS Sales KPI pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:00.012015Z",
     "iopub.status.busy": "2024-10-04T09:46:00.000991Z",
     "iopub.status.idle": "2024-10-04T09:46:00.700746Z",
     "shell.execute_reply": "2024-10-04T09:46:00.699868Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import datetime, timedelta,date\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:00.704259Z",
     "iopub.status.busy": "2024-10-04T09:46:00.703805Z",
     "iopub.status.idle": "2024-10-04T09:46:00.708948Z",
     "shell.execute_reply": "2024-10-04T09:46:00.708191Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:00.712871Z",
     "iopub.status.busy": "2024-10-04T09:46:00.712625Z",
     "iopub.status.idle": "2024-10-04T09:46:00.716510Z",
     "shell.execute_reply": "2024-10-04T09:46:00.715562Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:00.719966Z",
     "iopub.status.busy": "2024-10-04T09:46:00.719484Z",
     "iopub.status.idle": "2024-10-04T09:46:02.644450Z",
     "shell.execute_reply": "2024-10-04T09:46:02.643191Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "load('MASTER_UNI')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL'] # only these products are to be read from lax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:02.649643Z",
     "iopub.status.busy": "2024-10-04T09:46:02.648636Z",
     "iopub.status.idle": "2024-10-04T09:46:02.663292Z",
     "shell.execute_reply": "2024-10-04T09:46:02.661952Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal - \n",
    "def get_lin_voucher():\n",
    "    vch = pl.read_parquet(f'{xpn}LIN_VOUCHER.parquet') # n_rows=500\n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod in ['LIN1','LIN2','LIN3']: # LINV\n",
    "        vch_prod = (\n",
    "            vch.select(\n",
    "                pl.col('IID'),\n",
    "                pl.col(f'{prod}TUF1').alias(f'vTUF_1c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,5)]).alias(f'vTUF_4c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,14)]).alias(f'vTUF_13c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,27)]).alias(f'vTUF_26c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,num_weeks_rx+1)]).alias(f'vTUF_qtdc'),\n",
    "                pl.col(f'{prod}TUF2').alias(f'vTUF_1p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(5,9)]).alias(f'vTUF_4p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(14,27)]).alias(f'vTUF_13p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(27,53)]).alias(f'vTUF_26p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(14,14+num_weeks_rx)]).alias(f'vTUF_qtdp'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,106)]).alias(f'vTUF_all') #added 105 week datacut\n",
    "            )\n",
    "            .with_columns(pl.lit(f'LI{prod[-1]}').alias('PROD_CD'))\n",
    "        )\n",
    "        if prod[-1] == '1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "\n",
    "    # voucher_mapping = {'LI1': 4, 'LI2': 5, 'LI3': 3, 'LIV': 2}\n",
    "    # vch1 = vch1.with_columns(pl.col('PROD_CD').replace(voucher_mapping,return_dtype=pl.Int64).alias('product_id')).fill_null(0)#.drop('PROD_CD')\n",
    "    vch1 = vch1.fill_null(0)\n",
    "\n",
    "    return(vch1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:02.666973Z",
     "iopub.status.busy": "2024-10-04T09:46:02.666610Z",
     "iopub.status.idle": "2024-10-04T09:46:02.695815Z",
     "shell.execute_reply": "2024-10-04T09:46:02.694758Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get_summed_metric_period -> Function to fetch Summed Metric values for all 5 periods - 1,4,13,26,qtd (for both current and prior)\n",
    "# Input 1 : metric -> str -> supply the metric name you want to fetch (TUF,NUF,TRX etc.)\n",
    "# Input 2 : prod_cd -> list -> supply the list of product codes you want to filter for while reading the data.\n",
    "# Output : The output dataframes are at 4 levels : Territory, Region, Area, Nation.\n",
    "\n",
    "def get_summed_metric_period(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,53)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,5)]).alias(metric+'_4c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,14)]).alias(metric+'_13c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,27)]).alias(metric+'_26c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,num_weeks_rx+1)]).alias(metric+'_qtdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(5,9)]).alias(metric+'_4p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,27)]).alias(metric+'_13p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(27,53)]).alias(metric+'_26p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,14+num_weeks_rx)]).alias(metric+'_qtdp')\n",
    "    )\n",
    "    \n",
    "    # For Voucher Removal - \n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_4c') -  pl.col(f'v{metric}_4c').alias(f'{metric}_4c'),\n",
    "            pl.col(f'{metric}_13c') -  pl.col(f'v{metric}_13c').alias(f'{metric}_13c'),\n",
    "            pl.col(f'{metric}_26c') -  pl.col(f'v{metric}_26c').alias(f'{metric}_26c'),\n",
    "            pl.col(f'{metric}_qtdc') -  pl.col(f'v{metric}_qtdc').alias(f'{metric}_qtdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_4p') -  pl.col(f'v{metric}_4p').alias(f'{metric}_4p'),\n",
    "            pl.col(f'{metric}_13p') -  pl.col(f'v{metric}_13p').alias(f'{metric}_13p'),\n",
    "            pl.col(f'{metric}_26p') -  pl.col(f'v{metric}_26p').alias(f'{metric}_26p'),\n",
    "            pl.col(f'{metric}_qtdp') -  pl.col(f'v{metric}_qtdp').alias(f'{metric}_qtdp')\n",
    "        ).drop(cols_to_remove)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    metrics_to_calc = pl.col(metric+'_1c').sum().alias(metric+'_1c'),pl.col(metric+'_4c').sum().alias(metric+'_4c'),pl.col(metric+'_13c').sum().alias(metric+'_13c'),pl.col(metric+'_26c').sum().alias(metric+'_26c'),\\\n",
    "        pl.col(metric+'_qtdc').sum().alias(metric+'_qtdc'),pl.col(metric+'_1p').sum().alias(metric+'_1p'),pl.col(metric+'_4p').sum().alias(metric+'_4p'),pl.col(metric+'_13p').sum().alias(metric+'_13p'),\\\n",
    "        pl.col(metric+'_26p').sum().alias(metric+'_26p'),pl.col(metric+'_qtdp').sum().alias(metric+'_qtdp')\n",
    "    \n",
    "    df_terr = df.group_by(['geography_id','specialty_group','segment','decile','PROD_CD']).agg(metrics_to_calc)\n",
    "\n",
    "    df_reg = df.join(geo_code_mapper[['geography_id','region_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['region_geography_id','specialty_group','segment','decile','PROD_CD']).agg(metrics_to_calc)\n",
    "\n",
    "    df_area = df.join(geo_code_mapper[['geography_id','area_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['area_geography_id','specialty_group','segment','decile','PROD_CD']).agg(metrics_to_calc)\n",
    "\n",
    "    df_nation = df.join(geo_code_mapper[['geography_id','nation_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['nation_geography_id','specialty_group','segment','decile','PROD_CD']).agg(metrics_to_calc)\n",
    "\n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:02.699478Z",
     "iopub.status.busy": "2024-10-04T09:46:02.699202Z",
     "iopub.status.idle": "2024-10-04T09:46:02.708279Z",
     "shell.execute_reply": "2024-10-04T09:46:02.707368Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows(all_prod_df):\n",
    "    # converting tuple to list , because i cant assign the processed df back to it\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    for i in range(4): \n",
    "        df = all_prod_df[i]\n",
    "        agg_dict = {}\n",
    "        for col in df.columns[5:]:\n",
    "            agg_dict[col] = pl.col(col).sum()\n",
    "        \n",
    "        join_cols = df.columns[0:4]\n",
    "\n",
    "        df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "        df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "        df_2_35 = df_2_35.group_by(join_cols + ['parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "        df_1 = df.group_by(join_cols).agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "        # stack 1, 2_35 with df and return\n",
    "        df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "        vstack_helper = df.columns\n",
    "        df = df.vstack(\n",
    "            df_2_35.select(vstack_helper)\n",
    "        ).vstack(\n",
    "            df_1.select(vstack_helper)\n",
    "        )\n",
    "\n",
    "        all_prod_df[i] = df\n",
    "    return(tuple(all_prod_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:02.711875Z",
     "iopub.status.busy": "2024-10-04T09:46:02.711162Z",
     "iopub.status.idle": "2024-10-04T09:46:02.721135Z",
     "shell.execute_reply": "2024-10-04T09:46:02.720161Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows_iid(df):\n",
    "    agg_dict = {}\n",
    "    for col in df.columns[2:]:\n",
    "        agg_dict[col] = pl.col(col).sum()\n",
    "    \n",
    "    #join_cols = ['geography_id','plan_type','PlanID','IID']\n",
    "\n",
    "    df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    \n",
    "    df_1 = df.group_by('IID').agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:02.726392Z",
     "iopub.status.busy": "2024-10-04T09:46:02.725838Z",
     "iopub.status.idle": "2024-10-04T09:46:02.739826Z",
     "shell.execute_reply": "2024-10-04T09:46:02.738867Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_full_rollups(all_prod_df):\n",
    "    # converting the tuple of dfs into a list for processing\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    # for trivializing formulas - \n",
    "    p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    \n",
    "    #Looping over 4 levels (terr,reg,area,nation)\n",
    "    for i in range(4):\n",
    "        df = all_prod_df[i]\n",
    "        g = df.columns[0] #should contain geo level\n",
    "        metric_cols = df.columns[4:-1] #should contain the tuf / nuf columns\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols) #used for vstack later\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        #### Processing Done ####\n",
    "        df = (\n",
    "            df.select(main_seq)\n",
    "            .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "            .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "            .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        # Store Data Back :\n",
    "        all_prod_df[i] = df\n",
    "    \n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:02.743853Z",
     "iopub.status.busy": "2024-10-04T09:46:02.743501Z",
     "iopub.status.idle": "2024-10-04T09:46:02.791732Z",
     "shell.execute_reply": "2024-10-04T09:46:02.790859Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_period_prec_count_metric(metric,prod_cd):\n",
    "    # Variable Aliases to Trvialize : \n",
    "    p,sg,spc,d = 'product_id','segment','specialty_group','decile'\n",
    "    par = 'parent_product_id'\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    time_periods = [f'{metric}_1c',f'{metric}_4c',f'{metric}_13c',f'{metric}_26c',f'{metric}_qtdc'\n",
    "                    ,f'{metric}_1p',f'{metric}_4p',f'{metric}_13p',f'{metric}_26p',f'{metric}_qtdp'] #time periods\n",
    "    levels = ['geography_id','region_geography_id', 'area_geography_id', 'nation_geography_id'] #group levels\n",
    "    \n",
    "    # READ DATA FROM LAX AND GET DATA CUTS FOR EACH PERIOD CUR AND PRIOR AT IID AND PROD_CD LEVEL\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,53)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,5)]).alias(metric+'_4c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,14)]).alias(metric+'_13c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,27)]).alias(metric+'_26c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,num_weeks_rx+1)]).alias(metric+'_qtdc'),\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(5,9)]).alias(metric+'_4p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,27)]).alias(metric+'_13p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(27,53)]).alias(metric+'_26p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,14+num_weeks_rx)]).alias(metric+'_qtdp')\n",
    "    )\n",
    "    \n",
    "    # REMOVE VOUCHER -\n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_4c') -  pl.col(f'v{metric}_4c').alias(f'{metric}_4c'),\n",
    "            pl.col(f'{metric}_13c') -  pl.col(f'v{metric}_13c').alias(f'{metric}_13c'),\n",
    "            pl.col(f'{metric}_26c') -  pl.col(f'v{metric}_26c').alias(f'{metric}_26c'),\n",
    "            pl.col(f'{metric}_qtdc') -  pl.col(f'v{metric}_qtdc').alias(f'{metric}_qtdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_4p') -  pl.col(f'v{metric}_4p').alias(f'{metric}_4p'),\n",
    "            pl.col(f'{metric}_13p') -  pl.col(f'v{metric}_13p').alias(f'{metric}_13p'),\n",
    "            pl.col(f'{metric}_26p') -  pl.col(f'v{metric}_26p').alias(f'{metric}_26p'),\n",
    "            pl.col(f'{metric}_qtdp') -  pl.col(f'v{metric}_qtdp').alias(f'{metric}_qtdp')\n",
    "        ).drop(cols_to_remove)\n",
    "    \n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "    # Adding Geo Hier\n",
    "    df = df.join(geo_code_mapper,on='geography_id')\n",
    "    # joining with prod_mapping - \n",
    "    df = df.join(prod_mapping[['product_id','parent_product_id','code']],left_on='PROD_CD',right_on='code').drop('PROD_CD')\n",
    "    \n",
    "    def util1(df,g,period):\n",
    "        main_seq = [g,p,sg,d,spc,f'cp_{period}']\n",
    "        main_seq2 = [g,par,sg,d,spc,f'cp_{period}']\n",
    "        main_seq3 = [g,sg,d,spc,f'cp_{period}']\n",
    "        # Removing Rows Where Volume < 0\n",
    "        df_filtered = df.filter(pl.col(period) > 0)\n",
    "        \n",
    "        # For Child Products Only :\n",
    "        # First Round :\n",
    "        sg_df = (df_filtered.group_by([g,p,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df_filtered.group_by([g,p,sg,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df_filtered.group_by([g,p,d,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df_filtered.group_by([g,p,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df_filtered.group_by([g,p,d]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df_filtered.group_by([g,p,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc),d_roll_up.alias(d)).select(main_seq))\n",
    "        #Third Round -\n",
    "        sg_d_spc_df = (df_filtered.group_by([g,p]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        whole_df = (df_filtered.group_by([g,p,sg,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).select(main_seq))\n",
    "        df_filtered_child_all_rollups = (whole_df.vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "    \n",
    "        # For Parent Products 2_35:\n",
    "        df_filtered_2_35 = df_filtered.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "        # First Round :\n",
    "        sg_df = (df_filtered_2_35.group_by([g,par,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg)).select(main_seq2))\n",
    "        d_df = (df_filtered_2_35.group_by([g,par,sg,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(d_roll_up.alias(d)).select(main_seq2))\n",
    "        spc_df = (df_filtered_2_35.group_by([g,par,d,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc)).select(main_seq2))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df_filtered_2_35.group_by([g,par,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq2))\n",
    "        sg_spc_df = (df_filtered_2_35.group_by([g,par,d]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq2))\n",
    "        d_spc_df = (df_filtered_2_35.group_by([g,par,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc),d_roll_up.alias(d)).select(main_seq2))\n",
    "        #Third Round -\n",
    "        sg_d_spc_df = (df_filtered_2_35.group_by([g,par]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq2))\n",
    "        whole_df = (df_filtered_2_35.group_by([g,par,sg,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).select(main_seq2))\n",
    "        df_filtered_2_35_all_rollups = (whole_df.vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df)).rename({par:p})\n",
    "    \n",
    "        # For All Products - \n",
    "        # First Round :\n",
    "        sg_df = (df_filtered.group_by([g,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg)).select(main_seq3))\n",
    "        d_df = (df_filtered.group_by([g,sg,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(d_roll_up.alias(d)).select(main_seq3))\n",
    "        spc_df = (df_filtered.group_by([g,d,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc)).select(main_seq3))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df_filtered.group_by([g,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq3))\n",
    "        sg_spc_df = (df_filtered.group_by([g,d]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq3))\n",
    "        d_spc_df = (df_filtered.group_by([g,sg]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(spc_roll_up.alias(spc),d_roll_up.alias(d)).select(main_seq3))\n",
    "        #Third Round -\n",
    "        sg_d_spc_df = (df_filtered.group_by([g]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq3))\n",
    "        whole_df = (df_filtered.group_by([g,sg,d,spc]).agg(pl.col('IID').n_unique().alias(f'cp_{period}')).select(main_seq3))\n",
    "        df_filtered_lax_all_rollups = (\n",
    "            whole_df.vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df)\n",
    "            .with_columns(pl.lit(1).alias(p)).select(main_seq).with_columns(pl.col(p).cast(pl.Int64))\n",
    "        )\n",
    "    \n",
    "        # stack 1, 2_35 ,child\n",
    "        df_filtered_all_prod_all_roll = df_filtered_child_all_rollups.vstack(df_filtered_2_35_all_rollups).vstack(df_filtered_lax_all_rollups)\n",
    "    \n",
    "        return(df_filtered_all_prod_all_roll)\n",
    "    \n",
    "    df_terr = None\n",
    "    df_reg = None\n",
    "    df_area = None\n",
    "    df_nation = None #initialize the output dfs\n",
    "    \n",
    "    for period in time_periods:\n",
    "        for g in levels:\n",
    "            new_col_df = util1(df,g,period)\n",
    "            # If the dataframes are not initialized, assign df_period to them\n",
    "            if g == 'geography_id' and df_terr is None:\n",
    "                df_terr = new_col_df\n",
    "            elif g == 'region_geography_id' and df_reg is None:\n",
    "                df_reg = new_col_df\n",
    "            elif g == 'area_geography_id' and df_area is None:\n",
    "                df_area = new_col_df\n",
    "            elif g == 'nation_geography_id' and df_nation is None:\n",
    "                df_nation = new_col_df\n",
    "            else:\n",
    "                if g == 'geography_id':\n",
    "                    df_terr = df_terr.join(new_col_df,on=[g,spc,sg,d,p],how='outer_coalesce')\n",
    "                elif g == 'region_geography_id':\n",
    "                    df_reg = df_reg.join(new_col_df,on=[g,spc,sg,d,p],how='outer_coalesce')\n",
    "                elif g == 'area_geography_id':\n",
    "                    df_area = df_area.join(new_col_df,on=[g,spc,sg,d,p],how='outer_coalesce')\n",
    "                elif g == 'nation_geography_id':\n",
    "                    df_nation = df_nation.join(new_col_df,on=[g,spc,sg,d,p],how='outer_coalesce')\n",
    "    \n",
    "    df_terr = df_terr.fill_null(0)\n",
    "    df_reg = df_reg.fill_null(0)\n",
    "    df_area = df_area.fill_null(0)\n",
    "    df_nation = df_nation.fill_null(0)\n",
    "\n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:02.796167Z",
     "iopub.status.busy": "2024-10-04T09:46:02.795522Z",
     "iopub.status.idle": "2024-10-04T09:46:02.908890Z",
     "shell.execute_reply": "2024-10-04T09:46:02.908107Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_period_iid_metric(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,106)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,5)]).alias(metric+'_4c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,14)]).alias(metric+'_13c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,27)]).alias(metric+'_26c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,num_weeks_rx+1)]).alias(metric+'_qtdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(5,9)]).alias(metric+'_4p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,27)]).alias(metric+'_13p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(27,53)]).alias(metric+'_26p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,14+num_weeks_rx)]).alias(metric+'_qtdp'),\n",
    "\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,106)]).alias(metric+'_all')\n",
    "    )\n",
    "\n",
    "    # For Voucher Removal - \n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_4c') -  pl.col(f'v{metric}_4c').alias(f'{metric}_4c'),\n",
    "            pl.col(f'{metric}_13c') -  pl.col(f'v{metric}_13c').alias(f'{metric}_13c'),\n",
    "            pl.col(f'{metric}_26c') -  pl.col(f'v{metric}_26c').alias(f'{metric}_26c'),\n",
    "            pl.col(f'{metric}_qtdc') -  pl.col(f'v{metric}_qtdc').alias(f'{metric}_qtdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_4p') -  pl.col(f'v{metric}_4p').alias(f'{metric}_4p'),\n",
    "            pl.col(f'{metric}_13p') -  pl.col(f'v{metric}_13p').alias(f'{metric}_13p'),\n",
    "            pl.col(f'{metric}_26p') -  pl.col(f'v{metric}_26p').alias(f'{metric}_26p'),\n",
    "            pl.col(f'{metric}_qtdp') -  pl.col(f'v{metric}_qtdp').alias(f'{metric}_qtdp'),\n",
    "            pl.col(f'{metric}_all') -  pl.col(f'v{metric}_all').alias(f'{metric}_all')\n",
    "        ).drop(cols_to_remove)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    return(df.drop(['specialty_group','segment','decile','geography_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:02.912425Z",
     "iopub.status.busy": "2024-10-04T09:46:02.911906Z",
     "iopub.status.idle": "2024-10-04T09:46:55.679381Z",
     "shell.execute_reply": "2024-10-04T09:46:55.678554Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data Prep : ETA 20 Seconds\n",
    "# Calling Function to Process Summed Metric Values\n",
    "all_products_tuf = get_summed_metric_period('TUF',fetch_products) # this is a tuple containg all 4 rollups\n",
    "all_products_nuf = get_summed_metric_period('NUF',fetch_products)\n",
    "all_products_trx =  get_summed_metric_period('TRX',fetch_products)\n",
    "all_products_nrx =  get_summed_metric_period('NRX',fetch_products)\n",
    "all_products_tun =  get_summed_metric_period('TUN',fetch_products)\n",
    "all_products_nun =  get_summed_metric_period('NUN',fetch_products)\n",
    "\n",
    "# calling function to add parent product rows to it.\n",
    "all_products_tuf = add_parent_product_rows(all_products_tuf)\n",
    "all_products_nuf = add_parent_product_rows(all_products_nuf)\n",
    "all_products_trx = add_parent_product_rows(all_products_trx)\n",
    "all_products_nrx = add_parent_product_rows(all_products_nrx)\n",
    "all_products_tun = add_parent_product_rows(all_products_tun)\n",
    "all_products_nun = add_parent_product_rows(all_products_nun)\n",
    "\n",
    "# Adding Full Rol Ups - \n",
    "all_products_tuf = add_full_rollups(all_products_tuf)\n",
    "all_products_nuf = add_full_rollups(all_products_nuf)\n",
    "all_products_trx = add_full_rollups(all_products_trx)\n",
    "all_products_nrx = add_full_rollups(all_products_nrx)\n",
    "all_products_tun = add_full_rollups(all_products_tun)\n",
    "all_products_nun = add_full_rollups(all_products_nun)\n",
    "\n",
    "# Calling Function to process count of Prec ('metric_period' >= 1 only)\n",
    "all_products_tuf_hcp = get_period_prec_count_metric('TUF',fetch_products)\n",
    "# # calling function to add parent product rows to it.\n",
    "# all_products_tuf_hcp = add_parent_product_rows(all_products_tuf_hcp)\n",
    "# # adding full rollups \n",
    "# all_products_tuf_hcp = add_full_rollups(all_products_tuf_hcp)\n",
    "\n",
    "#IID level data for grower and decliner-\n",
    "all_products_tuf_iid = get_summed_period_iid_metric('TUF',fetch_products)\n",
    "all_products_tuf_iid = add_parent_product_rows_iid(all_products_tuf_iid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:55.686685Z",
     "iopub.status.busy": "2024-10-04T09:46:55.686270Z",
     "iopub.status.idle": "2024-10-04T09:46:57.813541Z",
     "shell.execute_reply": "2024-10-04T09:46:57.812375Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported all_products_tuf 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported all_products_tuf 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported all_products_tuf 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported all_products_tuf 3\n"
     ]
    }
   ],
   "source": [
    "# Exporting for BIT Monthly -\n",
    "#all_products_tuf\n",
    "for i in range(4):\n",
    "    all_products_tuf[i].to_pandas().to_parquet(f'{dflib}all_products_tuf_{i}.parquet')\n",
    "    print('exported all_products_tuf' ,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:57.819053Z",
     "iopub.status.busy": "2024-10-04T09:46:57.818682Z",
     "iopub.status.idle": "2024-10-04T09:46:57.915826Z",
     "shell.execute_reply": "2024-10-04T09:46:57.912191Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_1(df):\n",
    "    hold = [[f'{m}{period}c',f'{m}{period}p'] for m in ['TUN','TRX','TUF','NUN','NRX','NUF']]\n",
    "    for i in range(4):\n",
    "        g = levels[i]\n",
    "        gb_helper = [g,spc,sg,d,p]\n",
    "        f = (\n",
    "            all_products_tun[i][gb_helper + hold[0]]\n",
    "            .join(all_products_trx[i][gb_helper + hold[1]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_tuf[i][gb_helper + hold[2]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_nun[i][gb_helper + hold[3]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_nrx[i][gb_helper + hold[4]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_nuf[i][gb_helper + hold[5]],on = gb_helper,how = 'left')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "# Trx, Nrx : Size and Size Change ,  Average Trx\n",
    "def process_2(df,period_num):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                avg_trx_size = pl.col(f'TUN{period}c')/pl.col(f'TRX{period}c'),\n",
    "                avg_nrx_size = pl.col(f'NUN{period}c')/pl.col(f'NRX{period}c'),\n",
    "                prior_avg_trx_size = pl.col(f'TUN{period}p')/pl.col(f'TRX{period}p'),\n",
    "                prior_avg_nrx_size = pl.col(f'NUN{period}p')/pl.col(f'NRX{period}p')\n",
    "            )\n",
    "            .with_columns(\n",
    "                avg_trx_size_ch = pl.col('avg_trx_size')-pl.col('prior_avg_trx_size'),\n",
    "                avg_nrx_size_ch = pl.col('avg_nrx_size')-pl.col('prior_avg_nrx_size')\n",
    "            )\n",
    "            .with_columns(\n",
    "                avg_trx = pl.col(f'TUF{period}c')/period_num #no avg_nrx ? \n",
    "            )\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\t\n",
    "#prc_shr_bus\n",
    "def process_3(df):\n",
    "    for i in range(3):\n",
    "        gmap = geo_code_mapper[[levels[i],levels[i+1]]].unique()\n",
    "        fp = df[i+1].select(levels[i+1],spc,sg,d,p,f'TUF{period}c',f'NUF{period}c')\n",
    "        f = df[i]\n",
    "        ft = (\n",
    "            f\n",
    "            .select(levels[i],spc,sg,d,p,f'TUF{period}c',f'NUF{period}c')\n",
    "            .join(gmap,on = levels[i],how ='left')\n",
    "            .join(fp,on = [levels[i+1],spc,sg,d,p],how='left')\n",
    "            .with_columns(\n",
    "                prc_shr_bus_trx = pl.col(f'TUF{period}c')/pl.col(f'TUF{period}c_right'),\n",
    "                prc_shr_bus_nrx = pl.col(f'NUF{period}c')/pl.col(f'NUF{period}c_right')\n",
    "            )\n",
    "            .drop([levels[i+1],f'TUF{period}c',f'NUF{period}c',f'TUF{period}c_right',f'NUF{period}c_right'])\n",
    "        )\n",
    "        f = f.join(ft,on = [levels[i],spc,sg,d,p],how = 'left')\n",
    "        df[i] = f\n",
    "    df[3] = df[3].with_columns(\n",
    "        pl.lit(None).alias('prc_shr_bus_trx'),pl.lit(None).alias('prc_shr_bus_nrx')\n",
    "    )\n",
    "\n",
    "    return(df)\n",
    "\n",
    "#rr_4v13\n",
    "def process_4(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        source_df = all_products_tuf[i][[levels[i],p,sg,d,spc,'TUF_4c','TUF_13c']]\n",
    "        f = f.join(source_df,on = [levels[i],p,sg,d,spc],how='left').with_columns(\n",
    "            rr_4v13 = (pl.col('TUF_4c')*(13/4)) - pl.col('TUF_13c')\n",
    "        ).drop(['TUF_4c','TUF_13c'])\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "#count of prec , ind\n",
    "def process_5(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        formula = ((pl.col('num_hcp')-pl.col('pri_num_hcp'))/pl.col('pri_num_hcp')).alias('ind_metric')\n",
    "\n",
    "        source_df = all_products_tuf_hcp[i][[levels[i],spc,sg,d,p,f'cp_TUF{period}c',f'cp_TUF{period}p']]\n",
    "        f = f.join(source_df,[levels[i],spc,sg,d,p],how='left').rename({f'cp_TUF{period}c':'num_hcp',f'cp_TUF{period}p':'pri_num_hcp'}\n",
    "        ).with_columns(pl.col('num_hcp').fill_null(0),pl.col('pri_num_hcp').fill_null(0)\n",
    "        ).with_columns(formula\n",
    "        ).with_columns(pl.col('ind_metric').fill_nan(None)\n",
    "        ).with_columns(\n",
    "            pl.when(pl.col('ind_metric') > 0.02).then(pl.lit('P'))\n",
    "            .when(pl.col('ind_metric') < -0.02).then(pl.lit('Q'))\n",
    "            .otherwise(None).alias('num_hcp_ind')\n",
    "        ).drop(['pri_num_hcp','ind_metric'])\n",
    "\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\t\n",
    "#Num_Of_Prescribers_BnchMrk_Ind\n",
    "def process_bnch_presc(df,metric,ind_col_name):# WORKING CORRECTLY BUT NOT MODULAR \n",
    "    def add_indicator(df, ind_name, col1, col2, col3):\n",
    "        return df.with_columns(\n",
    "            pl.when(pl.col(col1) > pl.col(col2))\n",
    "            .then(pl.lit('A'))\n",
    "            .when(pl.col(col1) < pl.col(col3))\n",
    "            .then(pl.lit('B'))\n",
    "            .when((pl.col(col3) <= pl.col(col1)) & (pl.col(col1) <= pl.col(col2)))\n",
    "            .then(pl.lit('E'))\n",
    "            .otherwise(None)  # You can replace 'N/A' with any default value\n",
    "            .alias(ind_name)\n",
    "        )\n",
    "    #Terr\n",
    "    f = df[0]\n",
    "    nf = f.select([levels[0],p,sg,spc,d,metric])\n",
    "    nf = nf.join(geo_code_mapper,on = levels[0],how = 'left')\n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        all = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_r = nf.group_by([levels[1],p,sg,spc,d]).agg(\n",
    "        rul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        rll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "        .join(nf_r,on=[levels[1],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[1],levels[2],levels[3])\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1', metric, 'nul', 'nll')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind2', metric, 'aul', 'all')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind3', metric, 'rul', 'rll').drop(['nul','nll','aul','all','rul','rll',metric])\n",
    "    f = f.join(nf,on=[levels[0],p, sg, spc, d],how = 'left')\n",
    "    df[0] = f\n",
    "    #Region\n",
    "    f = df[1]\n",
    "    nf = f.select([levels[1],p,sg,spc,d,metric]).join(\n",
    "        geo_code_mapper[['region_geography_id','area_geography_id']].unique(),on = levels[1],how = 'left'\n",
    "    )\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        all = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[2],levels[3])\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1', metric, 'nul', 'nll')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind2', metric, 'aul', 'all')\n",
    "    nf = nf.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3')).drop(['nul','nll','aul','all',metric])\n",
    "    f = f.join(nf,on=[levels[1],p, sg, spc, d],how = 'left')\n",
    "    df[1] = f\n",
    "    #Area\n",
    "    f = df[2]\n",
    "    nf = f.select([levels[2],p,sg,spc,d,metric])\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (nf.join(nf_n, on=[p, sg, spc, d], how='left'))\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1', metric, 'nul', 'nll')\n",
    "    nf = nf.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind2'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3')).drop(['nul','nll',metric])\n",
    "    f = f.join(nf,on=[levels[2],p, sg, spc, d],how = 'left')\n",
    "    df[2] = f\n",
    "    #Nation \n",
    "    f = df[3]\n",
    "    f = f.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind1'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind2'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3'))\n",
    "    df[3] = f\n",
    "    return(df)\n",
    "\t\n",
    "def get_terr_cgd():    #count of growers and decliners -> requires all_products_tuf_iid to be set up before hand\n",
    "    i = 0 #keep it locked\n",
    "    source_df = (\n",
    "        all_products_tuf_iid[['IID','product_id',f'TUF{period}c',f'TUF{period}p']]\n",
    "        .rename({f'TUF{period}c':'cur_vol',f'TUF{period}p':'pri_vol'})\n",
    "        .with_columns(pl.col('cur_vol').round(1),pl.col('pri_vol').round(1))\n",
    "        .with_columns(vol_change = pl.col('cur_vol')-pl.col('pri_vol'))\n",
    "        .with_columns(pl.col('vol_change').round(1))\n",
    "        .join(mp_spec_seg_dec,on = 'IID',how = 'left')\n",
    "        .join(geo_code_mapper,on = levels[0],how ='left') #keeping levels 0 as base data is only on iid-terr level\n",
    "    )\n",
    "    \n",
    "    source_df_reduced = (\n",
    "        source_df\n",
    "        .join(MASTER_UNI.select(['IID','PDRPOptOutFlag']),on='IID',how='left')\n",
    "        .filter(pl.col('PDRPOptOutFlag')!='Y')\n",
    "        .filter(pl.col('segment')=='Target')\n",
    "        .filter((pl.col('pri_vol')!=0) & (pl.col('pri_vol').is_not_null()))\n",
    "        .filter((pl.col('vol_change')!=0))\n",
    "    )\n",
    "    \n",
    "    # for 10th perc\n",
    "    source_df_1 = source_df_reduced.filter(\n",
    "        pl.col('vol_change')<0\n",
    "    )\n",
    "    # for 90th perc\n",
    "    source_df_2 = source_df_reduced.filter(\n",
    "        pl.col('vol_change')>0\n",
    "    )\n",
    "    \n",
    "    source_df_percentile_10 = source_df_1.group_by(levels[i+1],p).agg(\n",
    "        ten_perc = pl.col('vol_change').quantile(0.1,interpolation='linear')\n",
    "    )\n",
    "    \n",
    "    source_df_percentile_90 = source_df_2.group_by(levels[i+1],p).agg(\n",
    "        nin_perc = pl.col('vol_change').quantile(0.9,interpolation='linear')\n",
    "    )\n",
    "    \n",
    "    source_df_percentile = source_df_percentile_10.join(source_df_percentile_90,on=[levels[i+1],p],how='outer_coalesce')\n",
    "    \n",
    "    source_df = source_df.join(source_df_percentile,on=[levels[i+1],p],how='left')\n",
    "    \n",
    "    source_df = source_df.with_columns(\n",
    "        pl.when((pl.col('vol_change')<=0) & (pl.col('vol_change') < pl.col('ten_perc'))).then(pl.lit('DECLINER'))\n",
    "        .when((pl.col('vol_change')>0) & (pl.col('vol_change') > pl.col('nin_perc'))).then(pl.lit('GROWER'))\n",
    "        .otherwise(pl.lit(None))\n",
    "        .alias('TYPE')\n",
    "    )\n",
    "    \n",
    "    # removing PDRP from source : \n",
    "    source_df= source_df.join(MASTER_UNI.select(['IID','PDRPOptOutFlag']),on='IID',how='left')\n",
    "    \n",
    "    source_df = source_df.with_columns(\n",
    "        pl.when(pl.col('PDRPOptOutFlag')=='Y').then(pl.lit('PDRP')).otherwise(pl.col('TYPE')).alias('TYPE')\n",
    "    )\n",
    "    \n",
    "    cg = source_df.filter(pl.col('TYPE')=='GROWER').group_by([levels[i],spc,sg,d,p]).agg(num_growers = pl.col('IID').n_unique())\n",
    "    cd = source_df.filter(pl.col('TYPE')=='DECLINER').group_by([levels[i],spc,sg,d,p]).agg(num_decliners = pl.col('IID').n_unique())\n",
    "    cgd = cg.join(cd,on = [levels[i],spc,sg,d,p],how='outer_coalesce').with_columns(pl.col('num_growers').fill_null(0),pl.col('num_decliners').fill_null(0))\n",
    "    \n",
    "    def add_all_roll_up_cgd(df):\n",
    "        g = levels[i]\n",
    "        p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "        sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "        metric_cols = ['num_growers','num_decliners']\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols)\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (\n",
    "                df.select(main_seq)\n",
    "                .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "                .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "                .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        return(df)\n",
    "    \n",
    "    cgd = add_all_roll_up_cgd(cgd)\n",
    "    return(cgd)\n",
    "\t\n",
    "def get_terr_cnp(cop): # count of new prescribers\n",
    "    i = 0\n",
    "    source_df = (\n",
    "        all_products_tuf_iid[['IID','product_id',f'TUF{period}{cop}',f'TUF_all']]\n",
    "        .rename({f'TUF{period}{cop}':'cur_vol'})\n",
    "        .with_columns(old_volume = pl.col('TUF_all')-pl.col('cur_vol'))\n",
    "        .filter((pl.col('cur_vol')>0) & (pl.col('old_volume')==0)) \n",
    "        .join(mp_spec_seg_dec,on = 'IID',how = 'left')\n",
    "        .filter(pl.col('old_volume')==0)\n",
    "        .with_columns(TYPE = pl.lit('NEW'))\n",
    "        .group_by([levels[i],spc,sg,d,p])\n",
    "        .agg(num_new_prec = pl.col('IID').n_unique())\n",
    "    )\n",
    "    def add_all_roll_up_cnp(df):\n",
    "        g = levels[i]\n",
    "        p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "        sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "        metric_cols = ['num_new_prec']\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols)\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (\n",
    "                df.select(main_seq)\n",
    "                .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "                .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "                .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        return(df)\n",
    "\n",
    "    source_df = add_all_roll_up_cnp(source_df)\n",
    "\n",
    "    return(source_df)\n",
    "\t\n",
    "# num_new_prec ind_metric num_growers num_decliners\n",
    "def process_6(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        formula = ((pl.col('num_new_prec')-pl.col('pri_num_new_prec'))/pl.col('pri_num_new_prec')).alias('ind_metric')\n",
    "        #if at terr level then no need to roll up:\n",
    "        if i==0:\n",
    "            f = (\n",
    "                f\n",
    "                .join(cgd,on=[levels[i],spc,sg,d,p],how='left')\n",
    "                .join(cnp,on=[levels[i],spc,sg,d,p],how='left')\n",
    "                .with_columns(formula)\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('ind_metric') > 0.02).then(pl.lit('P'))\n",
    "                    .when(pl.col('ind_metric') < -0.02).then(pl.lit('Q'))\n",
    "                    .otherwise(None).alias('new_prec_ind')\n",
    "                ).drop(['pri_num_new_prec','ind_metric'])\n",
    "            )\n",
    "        else:\n",
    "            source_df_1 = cgd.join(geo_code_mapper,on = levels[0],how='left').group_by([levels[i],spc,sg,d,p]).agg(\n",
    "                num_growers = pl.col('num_growers').sum(),num_decliners = pl.col('num_decliners').sum()\n",
    "            )\n",
    "            source_df_2 = cnp.join(geo_code_mapper,on = levels[0],how='left').group_by([levels[i],spc,sg,d,p]).agg(\n",
    "                num_new_prec = pl.col('num_new_prec').sum(),pri_num_new_prec = pl.col('pri_num_new_prec').sum()\n",
    "            )\n",
    "            f = (\n",
    "                f\n",
    "                .join(source_df_1,on=[levels[i],spc,sg,d,p],how='left')\n",
    "                .join(source_df_2,on=[levels[i],spc,sg,d,p],how='left')\n",
    "                .with_columns(formula)\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('ind_metric') > 0.02).then(pl.lit('P'))\n",
    "                    .when(pl.col('ind_metric') < -0.02).then(pl.lit('Q'))\n",
    "                    .otherwise(None).alias('new_prec_ind')\n",
    "                ).drop(['pri_num_new_prec','ind_metric'])\n",
    "            )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\t\n",
    "#trx size change , and nrx size change\n",
    "def process_7(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                pl.when(pl.col('avg_trx_size_ch') > 0.005).then(pl.lit('P'))\n",
    "                .when(pl.col('avg_trx_size_ch') < -0.005).then(pl.lit('Q'))\n",
    "                .when(pl.col('avg_trx_size_ch')==0).then(None)\n",
    "                .otherwise(None).alias('avg_trx_size_ch_ind'),\n",
    "                \n",
    "                pl.when(pl.col('avg_nrx_size_ch') > 0.005).then(pl.lit('P'))\n",
    "                .when(pl.col('avg_nrx_size_ch') < -0.005).then(pl.lit('Q'))\n",
    "                .when(pl.col('avg_nrx_size_ch')==0).then(None)\n",
    "                .otherwise(None).alias('avg_nrx_size_ch_ind'),\n",
    "            )\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:57.921880Z",
     "iopub.status.busy": "2024-10-04T09:46:57.921349Z",
     "iopub.status.idle": "2024-10-04T09:46:57.936987Z",
     "shell.execute_reply": "2024-10-04T09:46:57.935961Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Convert To Feed Ready data\n",
    "def get_feed(temp1):\n",
    "    drop_cols = [f'{m}{period}{suffix}' for m in ['TUN','TRX','TUF','NUN','NRX','NUF'] for suffix in ['c', 'p']]\n",
    "    drop_cols += ['prior_avg_trx_size','prior_avg_nrx_size','prc_shr_bus_nrx']\n",
    "    for i in range(4):\n",
    "        temp1[i] = temp1[i].drop(drop_cols)\n",
    "    \t\n",
    "    temp1[0] = temp1[0].rename({'geography_id': 'Geography_id'})\n",
    "    temp1[1] = temp1[1].rename({'region_geography_id': 'Geography_id'})\n",
    "    temp1[2] = temp1[2].rename({'area_geography_id': 'Geography_id'})\n",
    "    temp1[3] = temp1[3].rename({'nation_geography_id': 'Geography_id'})\n",
    "    final_feed = temp1[0].vstack(temp1[1]).vstack(temp1[2]).vstack(temp1[3])\n",
    "    #Renaming columns\n",
    "    new_col_mapping = {\n",
    "        'product_id': 'Product_id',\n",
    "        'segment': 'Segment',\n",
    "        'specialty_group': 'Specialty',\n",
    "        'decile': 'Decile',\n",
    "        'prc_shr_bus_trx': 'Share_of_Business_Prc',\n",
    "        'rr_4v13': 'Run_Rate_Change_4v13',\n",
    "        'num_hcp': 'Num_Of_Prescribers',\n",
    "        'num_hcp_ind': 'Num_Of_Prescribers_Ind',\n",
    "        'num_new_prec': 'Num_Of_New_Prescribers',\n",
    "        'new_prec_ind': 'Num_Of_New_Prescribers_Ind',\n",
    "        'num_growers': 'Num_Of_Growers',\n",
    "        'num_decliners': 'Num_Of_Decliners',\n",
    "        'avg_trx_size': 'Avg_TRx_Size',\n",
    "        'avg_trx': 'Avg_TRx',\n",
    "        'avg_trx_size_ch': 'Avg_TRx_Size_Change',\n",
    "        'avg_trx_size_ch_ind': 'Avg_TRx_Size_Change_Ind',\n",
    "        'avg_nrx_size': 'Avg_NRx_Size',\n",
    "        'avg_nrx_size_ch': 'Avg_NRx_Size_Change',\n",
    "        'avg_nrx_size_ch_ind': 'Avg_NRx_Size_Change_Ind'\n",
    "    } \n",
    "    final_feed = final_feed.rename(new_col_mapping)\n",
    "    #required new columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addp = ['Period']\n",
    "    col_to_addz = ['Num_Of_New_To_Brand']\n",
    "    col_to_addna = ['Num_Of_Growers_Ind','Num_Of_Decliners_Ind','Num_Of_New_To_Brand_Ind','TRx_Goal','Prc_TRx_Attainment']\n",
    "    # func to add columns with desired value\n",
    "    def addcol(df,columns_to_add,wtl):\n",
    "        for my_col in columns_to_add:\n",
    "            df = df.with_columns(pl.lit(wtl).alias(my_col))\n",
    "        return df\n",
    "    \n",
    "    final_feed = addcol(final_feed,col_to_addrt,'WEEKLY')\n",
    "    final_feed = addcol(final_feed,col_to_addp,f'{period_num}-WEEK')\n",
    "    final_feed = addcol(final_feed,col_to_addz,0)\n",
    "    final_feed = addcol(final_feed,col_to_addna,'\\\\N')\n",
    "\n",
    "    #changing values according to feed of SAS. - 06/20\n",
    "    final_feed = final_feed.with_columns(\n",
    "        pl.when(pl.col('Segment')=='ALG-ONLY-TARGET')\n",
    "        .then(pl.lit('AGNT'))\n",
    "        .when(pl.col('Segment')=='Non-Target')\n",
    "        .then(pl.lit('NT'))\n",
    "        .when(pl.col('Segment')=='Target')\n",
    "        .then(pl.lit('T'))\n",
    "        .otherwise(pl.col('Segment'))\n",
    "        .alias('Segment'))\n",
    "    \n",
    "    # arranging columns according to feed\n",
    "    req_cols = ['Geography_id', 'Product_id', 'Segment', 'Specialty', 'ReportType', 'Period', 'Decile', 'Share_of_Business_Prc', \n",
    "                'Run_Rate_Change_4v13', 'Num_Of_Prescribers', 'Num_Of_Prescribers_Ind', 'Num_Of_Prescribers_BnchMrk_Ind1', \n",
    "                'Num_Of_Prescribers_BnchMrk_Ind2', 'Num_Of_Prescribers_BnchMrk_Ind3', 'Num_Of_New_Prescribers', 'Num_Of_New_Prescribers_Ind', \n",
    "                'Num_Of_Growers', 'Num_Of_Growers_Ind', 'Num_Of_Decliners', 'Num_Of_Decliners_Ind', 'Avg_TRx_Size', 'Avg_TRx_Size_BnchMrk_Ind1', \n",
    "                'Avg_TRx_Size_BnchMrk_Ind2', 'Avg_TRx_Size_BnchMrk_Ind3', 'Avg_TRx', 'Num_Of_New_To_Brand', 'Num_Of_New_To_Brand_Ind', \n",
    "                'TRx_Goal', 'Prc_TRx_Attainment', 'Avg_TRx_Size_Change', 'Avg_TRx_Size_Change_Ind', 'Avg_TRx_Size_Change_BnchMrk_Ind1', \n",
    "                'Avg_TRx_Size_Change_BnchMrk_Ind2', 'Avg_TRx_Size_Change_BnchMrk_Ind3', 'Avg_NRx_Size', 'Avg_NRx_Size_BnchMrk_Ind1', \n",
    "                'Avg_NRx_Size_BnchMrk_Ind2', 'Avg_NRx_Size_BnchMrk_Ind3', 'Avg_NRx_Size_Change', 'Avg_NRx_Size_Change_Ind', \n",
    "                'Avg_NRx_Size_Change_BnchMrk_Ind1', 'Avg_NRx_Size_Change_BnchMrk_Ind2', 'Avg_NRx_Size_Change_BnchMrk_Ind3']\n",
    "    final_feed = final_feed.select(req_cols)\n",
    "    \n",
    "     #-----------------------------------------#'\n",
    "    \n",
    "    columns_to_round10 = ['Share_of_Business_Prc']\n",
    "    columns_to_round3 = ['Run_Rate_Change_4v13','Avg_TRx_Size','Avg_TRx','Avg_NRx_Size',]\n",
    "    columns_to_round1 = ['Avg_TRx_Size_Change','Avg_NRx_Size_Change']\n",
    "    #columns_to_round0 = ['Num_Of_New_Prescribers','Num_Of_Growers','Num_Of_Decliners']\n",
    "\n",
    "    final_feed = final_feed.with_columns([\n",
    "        *[pl.col(col).round(1).alias(col) for col in columns_to_round1],\n",
    "        *[pl.col(col).round(3).alias(col) for col in columns_to_round3],\n",
    "        *[pl.col(col).round(10).alias(col) for col in columns_to_round10],\n",
    "    ])\n",
    "\n",
    "    #OVerrides -\n",
    "    final_feed = final_feed.with_columns(pl.col('Num_Of_New_Prescribers').replace(None,0))\n",
    "\n",
    "    return (final_feed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period Loop-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:57.945508Z",
     "iopub.status.busy": "2024-10-04T09:46:57.944757Z",
     "iopub.status.idle": "2024-10-04T09:46:57.956105Z",
     "shell.execute_reply": "2024-10-04T09:46:57.954876Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for trvializing formula : \n",
    "p,sg,spc,d = 'product_id','segment','specialty_group','decile'\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/GeoSummary/Weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:46:57.963549Z",
     "iopub.status.busy": "2024-10-04T09:46:57.963003Z",
     "iopub.status.idle": "2024-10-04T09:47:22.026171Z",
     "shell.execute_reply": "2024-10-04T09:47:22.025366Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 1!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 2!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 3!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 4!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 5!\n"
     ]
    }
   ],
   "source": [
    "# Calling Functions and Exporting Feeds-\n",
    "for period_num,PN in zip([1,4,13,26,'qtd'],[1,2,3,4,5]):\n",
    "    period = f'_{period_num}'\n",
    "\n",
    "    # if PN > 1:\n",
    "    #     break;\n",
    "      \n",
    "    temp1 = [pl.DataFrame() for _ in range(4)] # creating an empty dataframe holder list obj\n",
    "    temp1 = process_1(temp1)\n",
    "    if PN == 5:\n",
    "        temp1 = process_2(temp1, num_weeks_rx)\n",
    "    else:\n",
    "        temp1 = process_2(temp1, period_num)\n",
    "\n",
    "    temp1 = process_3(temp1)\n",
    "    temp1 = process_4(temp1)\n",
    "    temp1 = process_5(temp1)\n",
    "    temp1 = process_bnch_presc(temp1,'num_hcp','Num_Of_Prescribers')\n",
    "    cgd = get_terr_cgd()\n",
    "    cnp = (\n",
    "        get_terr_cnp('c')\n",
    "        .join(\n",
    "            get_terr_cnp('p').rename({'num_new_prec':'pri_num_new_prec'}),\n",
    "            on = [levels[0],p,sg,d,spc],how = 'outer_coalesce'\n",
    "        ).with_columns(\n",
    "            pl.col('num_new_prec').fill_null(0),pl.col('pri_num_new_prec').fill_null(0)\n",
    "        )\n",
    "    )\n",
    "    temp1 = process_6(temp1)\n",
    "    temp1 = process_7(temp1)\n",
    "    # Reusing Function from HCP benchmark , assuming same upper and lower limit logic is applied.\n",
    "    #Avg_TRx_Size_BnchMrk_Ind\n",
    "    temp1 = process_bnch_presc(temp1,'avg_trx_size','Avg_TRx_Size')\n",
    "    #Avg_TRx_Size_Change_BnchMrk_Ind\n",
    "    temp1 = process_bnch_presc(temp1,'avg_trx_size_ch','Avg_TRx_Size_Change')\n",
    "    #Avg_NRx_Size_BnchMrk_Ind\n",
    "    temp1 = process_bnch_presc(temp1,'avg_nrx_size','Avg_NRx_Size')\n",
    "    #Avg_NRx_Size_Change_BnchMrk_Ind\n",
    "    temp1 = process_bnch_presc(temp1,'avg_nrx_size_ch','Avg_NRx_Size_Change')\n",
    "\n",
    "    feed_dataset = get_feed(temp1)\n",
    "     #===================================================\n",
    "    feed_dataset = feed_dataset.to_pandas()\n",
    "    string_columns = feed_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "    feed_dataset[string_columns] = feed_dataset[string_columns].fillna('\\\\N')\n",
    "    feed_dataset = feed_dataset.replace([np.nan, np.inf, -np.inf,'NaN'], ['\\\\N','\\\\N','\\\\N','\\\\N'])\n",
    "    feed_dataset.to_csv(f'{OUT}Weekly_GeoSummary_SalesKPIs_P{PN}_Feed.txt', sep='|', lineterminator='\\r\\n',index=False)\n",
    "    print(f'Exported Feed {PN}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
