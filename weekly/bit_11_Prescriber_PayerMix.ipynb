{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prescriber Payermix Feed\n",
    "- in sas this view is connected / dependant on a dataset from mgd care view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "monthly_data_date = js['monthly_data_date']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "pln = f's3://{bucket}/PYADM/weekly/archive/20240705/plantrak/'# hardcoded will remove later\n",
    "mpln = f's3://{bucket}/PYADM/monthly/archive/{monthly_data_date}/plantrak/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "load('MASTER_UNI')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORMULARY\n",
    "group_type_mapping = {\n",
    "    'HIX' : 'Commerical','Com' : 'Commerical','Cash' : 'Cash','Voucher':'Voucher',\n",
    "    'FFS' : 'FFS','Mgd Medicaid' : 'Mgd Medicaid','Part D' : 'Part D','MAC A' : 'Others',\n",
    "}\n",
    "\n",
    "def classify_plan_class(status):\n",
    "    status = status.upper()\n",
    "    if status[:7] == \"COVERED\" or status[:6] == \"ON PDL\":\n",
    "        return \"COVERED\"\n",
    "    elif status[:9] == \"PREFERRED\":\n",
    "        return \"PREFERRED\"\n",
    "    elif status[:13] == \"NON-PREFERRED\":\n",
    "        return \"NON PREFERRED\"\n",
    "    elif status[:7] == \"NON-PDL\" or status[:11] == \"NOT COVERED\":\n",
    "        return \"NOT COVERED\"\n",
    "    else:\n",
    "        return \"N_A\"\n",
    "\n",
    "fm = pl.read_parquet(pln+'FORMULARY.parquet',columns = ['IMS_PLAN_ID','GROUP_TYPE','FORMULARY_GROUP_STATUS','PFAM_CD','PFAM_NAME','IRWD_FGN_NAME','BRAND'])\n",
    "fm = fm.with_columns(\n",
    "        pl.when(pl.col('BRAND')=='IBR')\n",
    "        .then(pl.lit('IRL'))\n",
    "        .otherwise(pl.col('BRAND'))\n",
    "        .alias('BRAND'))\n",
    "    \n",
    "fm = fm.filter((pl.col('PFAM_CD')==(pl.col('BRAND'))) | (pl.col('BRAND')==''))\n",
    "fm = (\n",
    "    fm\n",
    "    .with_columns(\n",
    "        pl.col('GROUP_TYPE').map_elements(lambda x: group_type_mapping.get(x,'Others'), return_dtype=pl.Utf8) #NOTE : IF new plan types flow , they will go to Others by default\n",
    "        .fill_null('Others')\n",
    "        .alias('plan_type'),\n",
    "        pl.col('IMS_PLAN_ID').cast(pl.Int64)\n",
    "    )\n",
    "    .rename({'IMS_PLAN_ID':'PlanID'})\n",
    "    .drop('GROUP_TYPE')\n",
    "    .with_columns(pl.col('FORMULARY_GROUP_STATUS').fill_null(pl.lit('N_A')))\n",
    "    .with_columns(pl.col('FORMULARY_GROUP_STATUS').map_elements(classify_plan_class,return_dtype=pl.String).alias('plan_class'))\n",
    "    .drop('FORMULARY_GROUP_STATUS')\n",
    "    # .join(prod_mapping[['product_id','code']],left_on = 'PFAM_CD',right_on='code',how='left').drop('PFAM_CD')#change \n",
    "    # .filter(pl.col('product_id').is_not_null())# change\n",
    "    .unique()\n",
    ")\n",
    "fm = fm.with_columns(\n",
    "    pl.when(pl.col(\"PlanID\") == 13670614)\n",
    "    .then(pl.lit('Others'))\n",
    "    .otherwise(pl.col(\"plan_type\"))\n",
    "    .alias(\"plan_type\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_1 = fm.select(['PlanID','IRWD_FGN_NAME','plan_type']).unique(subset='PlanID')\n",
    "fm_2 = fm.select(['PlanID','PFAM_CD','PFAM_NAME','BRAND','plan_type','plan_class'])#change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLANTRAK - 6 MONTH TUF at PROD , IID , PLAN LEVEL\n",
    "ln = (\n",
    "    pl.read_parquet(mpln+'LAX_N.parquet',columns=['IID','MonthKey','PFAM_CD','PROD_CD','PlanID','TUF']) #read req cols only\n",
    "    .rename({'MonthKey':'PeriodKey'})\n",
    "    .filter(pl.col('PROD_CD').is_in(fetch_products)) #only keep data for BIT products\n",
    "    .with_columns(pl.col('PeriodKey').cast(pl.Utf8).str.to_date(\"%Y%m%d\")) #Convert Categorical column Back to date\n",
    "    # .join(prod_mapping[['product_id','code']],left_on = 'PROD_CD',right_on='code',how='left')# change\n",
    "    # .drop('PROD_CD')\n",
    ")\n",
    "cut_off_date = ln['PeriodKey'].unique().sort(descending=True)[5] #filter date to only keep 6 months of data\n",
    "\n",
    "ln = ln.filter(pl.col('PeriodKey')>= cut_off_date)\n",
    "\n",
    "ln = (\n",
    "    ln\n",
    "    .filter(pl.col('PeriodKey')>= cut_off_date)\n",
    "    .group_by(['IID','PlanID','PFAM_CD','PROD_CD']) #Rolling up lax_n for 6 month TUF at : IID and PlanID, prod level. (#productKey will remove later)\n",
    "    .agg(TUF = pl.col('TUF').sum()) #6 month TUF # change above ,'PROD_CD'\n",
    "    .unique()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_1 = (ln.join(fm_1,on='PlanID',how='left'))\n",
    "\n",
    "ln_2 = (\n",
    "    ln_1.join(fm_2, on=['PlanID', 'PFAM_CD'], how='left') # change\n",
    "    .with_columns(\n",
    "        pl.col('plan_type').fill_null(pl.lit('Others')),\n",
    "        pl.col('plan_class').fill_null(pl.lit('N_A'))\n",
    "        \n",
    "    )\n",
    ")\n",
    "ln_2 = ln_2.with_columns(ln_2[\"PlanID\"].cast(pl.Utf8).alias(\"PlanID\")) \\\n",
    "                         .filter(~pl.col(\"PlanID\").str.slice(0, 6).eq(\"000002\")) \\\n",
    "                         .with_columns(pl.col(\"PlanID\").cast(pl.Int64).alias(\"PlanID\"))\n",
    "\n",
    "ln_2 = ln_2.filter(~pl.col('plan_type').is_in(['Voucher'])).join(prod_mapping[['product_id','code']],left_on = 'PROD_CD',right_on='code',how='left').drop('PROD_CD')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing medicad before pivoting for plan class\n",
    "\n",
    "ln1_reduced = ln_2.filter(~pl.col('plan_type').is_in(['Mgd Medicaid', 'FFS']))\n",
    "\n",
    "ln_c = ln1_reduced.pivot(\n",
    "    values = 'TUF',index = ['IID','product_id'],\n",
    "    columns = 'plan_class',\n",
    "    aggregate_function = 'sum'\n",
    ").fill_null(0)\n",
    "\n",
    "ln_t = ln_2.pivot(\n",
    "    values = 'TUF',index = ['IID','product_id'],\n",
    "    columns = 'plan_type',\n",
    "    aggregate_function = 'sum'\n",
    ").fill_null(0)\n",
    "\n",
    "# Adding Parent Product Rows\n",
    "def add_parent_product_rows(df):\n",
    "    agg_dict = {}\n",
    "    for col in df.columns[2:]:\n",
    "        agg_dict[col] = pl.col(col).sum()\n",
    "\n",
    "    df = df.join(prod_mapping[['product_id','parent_product_id']], on = 'product_id', how = 'left')\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    \n",
    "    df_1 = df.group_by('IID').agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)\n",
    "ln_c = add_parent_product_rows(ln_c)\n",
    "ln_t = add_parent_product_rows(ln_t)\n",
    "\n",
    "ln2 = ln_c.join(ln_t,on=['IID','product_id'],how='left').unique() #outer_coalesce\n",
    "\n",
    "ln2 = (\n",
    "    ln2\n",
    "    .with_columns(plan_type_sum = pl.sum_horizontal(['Part D', 'Commerical', 'Mgd Medicaid', 'FFS', 'Cash', 'Others']))\n",
    "    .with_columns(\n",
    "        Commerical_prc = pl.col('Commerical')/pl.col('plan_type_sum'),\n",
    "        Medicare_Part_D_prc = pl.col('Part D')/pl.col('plan_type_sum'),\n",
    "        Managed_Medicaid_prc = pl.col('Mgd Medicaid')/pl.col('plan_type_sum'),\n",
    "        FFS_prc = pl.col('FFS')/pl.col('plan_type_sum'),\n",
    "        Cash_prc = pl.col('Cash')/pl.col('plan_type_sum'),\n",
    "        Others_prc = pl.col('Others')/pl.col('plan_type_sum')\n",
    "    )\n",
    "    .with_columns(\n",
    "        qc = pl.sum_horizontal(['Commerical_prc','Medicare_Part_D_prc','Managed_Medicaid_prc', 'FFS_prc', 'Cash_prc', 'Others_prc'])\n",
    "    )# ONLY FOR QC | Will use it to drop 0 TUF rows\n",
    "    .filter(~pl.col('qc').is_nan())\n",
    "    .drop(['Part D', 'Commerical', 'Mgd Medicaid', 'FFS', 'Cash', 'Others','plan_type_sum','qc'])\n",
    "    # .with_columns(\n",
    "    #     pl.lit(None).alias('Managed_Medicaid_prc'),\n",
    "    #     pl.lit(None).alias('FFS_prc')\n",
    "    # )# removing data from mgd , ffs\n",
    ")\n",
    "\n",
    "ln2 = ln2.join(\n",
    "    MASTER_UNI.with_columns((pl.col('LastName')+', '+pl.col('FirstName')).alias('Physician_Name')).select(['IID','Physician_Name']),\n",
    "    on='IID',how='inner'#was left before - done to reduce extra obs\n",
    ")\n",
    "\n",
    "# PDRP Override :\n",
    "conversion_columns = ln2.columns[2:-1]\n",
    "ln2 = ln2.join(MASTER_UNI[['IID','PDRPOptOutFlag']],on='IID',how='left')\n",
    "for col in conversion_columns:\n",
    "    ln2 = ln2.with_columns(\n",
    "        pl.when(pl.col(\"PDRPOptOutFlag\") == \"Y\").then(pl.lit('\\\\N')).otherwise(pl.col(col)).alias(col)\n",
    "    )\n",
    "ln2 = ln2.drop('PDRPOptOutFlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG : What to do with plans with more than one coverage status ? eg : IID 1631628"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Feed Creation -\n",
    "final_feed = ln2\n",
    "\n",
    "#Renaming existing columns according to feed\n",
    "column_mapping = {\n",
    "    \"IID\": \"Physician_ID\",\n",
    "    \"product_id\": \"Product_id\",\n",
    "    \"PREFERRED\": \"Preferred\",\n",
    "    \"COVERED\": \"Covered\",\n",
    "    \"NOT COVERED\": \"Not_Covered\",\n",
    "    \"N_A\": \"Not_Available\",\n",
    "    \"Commerical_prc\": \"Commercial\",\n",
    "    \"Medicare_Part_D_prc\": \"Medicare_Part_D\",\n",
    "    \"Managed_Medicaid_prc\": \"Managed_Medicaid\",\n",
    "    \"FFS_prc\": \"FFS\",\n",
    "    \"Cash_prc\": \"Cash\",\n",
    "    \"Others_prc\": \"Other\"\n",
    "}\n",
    "final_feed = final_feed.rename(column_mapping)\n",
    "#required new columns for feed\n",
    "col_to_addrt = ['ReportType']\n",
    "col_to_addna = ['Covered_PA_ST','Unknown','Not_Applicable']#Not_Applicable also have 0 in feed\n",
    "# func to add columns with desired value\n",
    "def addcol(df,columns_to_add,wtl):\n",
    "    for my_col in columns_to_add:\n",
    "        df = df.with_columns(pl.lit(wtl).alias(my_col))\n",
    "    return df\n",
    "final_feed = addcol(final_feed,col_to_addrt,'WEEKLY')\n",
    "final_feed = addcol(final_feed,col_to_addna,'\\\\N')\n",
    "# rearranging columns accoring to feed.\n",
    "req_cols = [\"Physician_Name\",\n",
    "    \"Physician_ID\",\n",
    "    \"Product_id\",\n",
    "    \"ReportType\",\n",
    "    \"Preferred\",\n",
    "    \"Covered\",\n",
    "    \"Not_Covered\",\n",
    "    \"Not_Available\",\n",
    "    \"Commercial\",\n",
    "    \"Medicare_Part_D\",\n",
    "    \"Managed_Medicaid\",\n",
    "    \"FFS\",\n",
    "    \"Cash\",\n",
    "    \"Other\",\n",
    "    \"Covered_PA_ST\",\n",
    "    \"Unknown\",\n",
    "    \"Not_Applicable\"]\n",
    "final_feed = final_feed.select(req_cols)# final data set.\n",
    "#===================================================================#\n",
    "cols = ['Preferred', 'Covered', 'Not_Covered', 'Not_Available','Commercial', 'Medicare_Part_D', 'Cash', 'Other','Managed_Medicaid','FFS']\n",
    "for col in cols:\n",
    "    final_feed = final_feed.with_columns(\n",
    "    pl.col(col)\n",
    "    .replace(['\\\\N'], None)\n",
    "    )\n",
    "final_feed = final_feed.with_columns([pl.col(col).cast(pl.Float64).alias(col) for col in cols])\n",
    "\n",
    "columns_to_round3 = ['Preferred', 'Covered', 'Not_Covered', 'Not_Available']\n",
    "columns_to_round10 = ['Commercial', 'Medicare_Part_D', 'Cash', 'Other','Managed_Medicaid','FFS']\n",
    "\n",
    "final_feed = final_feed.with_columns([\n",
    "    *[pl.col(col).round(3).alias(col) for col in columns_to_round3],\n",
    "    *[pl.col(col).round(10).alias(col) for col in columns_to_round10]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting -\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/Prescriber/Weekly/'\n",
    "#===================================================\n",
    "final_feed = final_feed.to_pandas()\n",
    "#Select columns of type 'object' (string)\n",
    "string_columns = final_feed.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "final_feed[string_columns] = final_feed[string_columns].fillna('\\\\N')\n",
    "final_feed = final_feed.replace('NaN', '\\\\N')\n",
    "final_feed = final_feed.replace([np.nan, np.inf, -np.inf], '\\\\N')\n",
    "final_feed.to_csv(f'{OUT}Weekly_Prescriber_PayerMix_Feed.txt', sep='|',lineterminator='\\r\\n',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
