{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presc Trend Feed v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:05:56.356857Z",
     "iopub.status.busy": "2024-05-31T09:05:56.356454Z",
     "iopub.status.idle": "2024-05-31T09:05:57.016418Z",
     "shell.execute_reply": "2024-05-31T09:05:57.013529Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime, timedelta,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:05:57.026482Z",
     "iopub.status.busy": "2024-05-31T09:05:57.025694Z",
     "iopub.status.idle": "2024-05-31T09:05:57.032124Z",
     "shell.execute_reply": "2024-05-31T09:05:57.030838Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:05:57.037815Z",
     "iopub.status.busy": "2024-05-31T09:05:57.037199Z",
     "iopub.status.idle": "2024-05-31T09:05:57.047578Z",
     "shell.execute_reply": "2024-05-31T09:05:57.042252Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:05:57.055583Z",
     "iopub.status.busy": "2024-05-31T09:05:57.055268Z",
     "iopub.status.idle": "2024-05-31T09:05:58.335670Z",
     "shell.execute_reply": "2024-05-31T09:05:58.333960Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:05:58.344838Z",
     "iopub.status.busy": "2024-05-31T09:05:58.343829Z",
     "iopub.status.idle": "2024-05-31T09:05:58.353069Z",
     "shell.execute_reply": "2024-05-31T09:05:58.351696Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_volumes(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,14)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "    rename_dict = dict(zip(columns[2:],['Vol'+str(i)+'_'+metric for i in range(1,14)]))\n",
    "    df = df.rename(rename_dict)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null()\n",
    "    ).drop(['specialty_group','segment','decile','geography_id'])\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:05:58.360216Z",
     "iopub.status.busy": "2024-05-31T09:05:58.359812Z",
     "iopub.status.idle": "2024-05-31T09:05:58.374857Z",
     "shell.execute_reply": "2024-05-31T09:05:58.370344Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##### adding parent product rows\n",
    "def add_parent_product_rows(df): #pass dataframe with all products and 13 week volumes for both metrics here\n",
    "\n",
    "    agg_dict  = {}\n",
    "\n",
    "    for i in range(1,14):\n",
    "        for metric in ['TUF','NUF']:\n",
    "            col_name = f'Vol{i}_{metric}'\n",
    "            agg_dict[col_name] = pl.col(col_name).sum()\n",
    "\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    df_1 = df.group_by(['IID']).agg(**agg_dict).with_columns(product_id = pl.lit(1)\n",
    "    ).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:05:58.385906Z",
     "iopub.status.busy": "2024-05-31T09:05:58.384797Z",
     "iopub.status.idle": "2024-05-31T09:06:00.870950Z",
     "shell.execute_reply": "2024-05-31T09:06:00.868750Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data Prep \n",
    "all_products_volume_tuf = get_volumes('TUF',fetch_products)\n",
    "all_products_volume_nuf = get_volumes('NUF',fetch_products)\n",
    "all_products_volume = all_products_volume_tuf.join(all_products_volume_nuf,on = ['IID','PROD_CD'],how='left')\n",
    "\n",
    "#for sub level groups -\n",
    "prod_mapping1 = prod_mapping[['product_id','parent_product_id','code']]#.filter(pl.col('parent_product_id')!=1)\n",
    "\n",
    "all_products_volume = all_products_volume.join(prod_mapping1,left_on='PROD_CD',right_on='code',how='left')\n",
    "all_products_volume = add_parent_product_rows(all_products_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:06:00.890896Z",
     "iopub.status.busy": "2024-05-31T09:06:00.889773Z",
     "iopub.status.idle": "2024-05-31T09:06:00.913023Z",
     "shell.execute_reply": "2024-05-31T09:06:00.911928Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Volume\n",
    "#this will increase nobs on temp1 as its one IID to many prod rows\n",
    "def add_volume_cols(df): \n",
    "    return(df.join(all_products_volume,on='IID',how='left').filter(pl.col('product_id').is_not_null()))\n",
    "\n",
    "# Share\n",
    "def process_share(df):\n",
    "    rename_dict = {}\n",
    "    expn_dict = {}\n",
    "    for i in range(1,14):\n",
    "        for metric in ['TUF','NUF']:\n",
    "            rename_dict[f'Vol{i}_{metric}'] = f'lax_Vol{i}_{metric}'\n",
    "            expn_dict[f'Shr{i}_{metric}'] = pl.col(f'Vol{i}_{metric}')/pl.col(f'lax_Vol{i}_{metric}')\n",
    "\n",
    "    df_1 = df.filter(pl.col('product_id')==1).rename(rename_dict).drop(['product_id','geography_id']) # this will contain LAX volumes for each IID\n",
    "    df = df.join(df_1,on='IID',how='left'\n",
    "    ).with_columns(**expn_dict).drop(list(rename_dict.values()))\n",
    "\n",
    "    return(df)\n",
    "\n",
    "# Trend\n",
    "def process_trend(df,metric):\n",
    "    THRE_13 = 1/26\n",
    "    THRE_4 = 1/10\n",
    "    #THRE = 1/4 # not used\n",
    "\n",
    "    df2 = df.select(['IID','geography_id','product_id']+[f'Vol{i}_{metric}' for i in range(1,14)])\n",
    "    #AVG_TUF\n",
    "    df2 = df2.with_columns(\n",
    "        AVG_TUF = pl.mean_horizontal([f'Vol{i}_{metric}' for i in range(1,14)])\n",
    "    )\n",
    "\n",
    "    #SLOPE_13\n",
    "    AVG_TUF = pl.col('AVG_TUF') #just to make formatting easier (polars.expr.expr.Expr obj)\n",
    "    df2 = df2.with_columns(\n",
    "        SLOPE_13 = (\n",
    "        -5.5 * (pl.col(f'Vol13_{metric}') - AVG_TUF) \n",
    "        -4.5 * (pl.col(f'Vol12_{metric}') - AVG_TUF) \n",
    "        -3.5 * (pl.col(f'Vol11_{metric}') - AVG_TUF) \n",
    "        -2.5 * (pl.col(f'Vol10_{metric}') - AVG_TUF) \n",
    "        -1.5 * (pl.col(f'Vol9_{metric}') - AVG_TUF) \n",
    "        -0.5 * (pl.col(f'Vol8_{metric}') - AVG_TUF) \n",
    "        +0.5 * (pl.col(f'Vol6_{metric}') - AVG_TUF) \n",
    "        +1.5 * (pl.col(f'Vol5_{metric}') - AVG_TUF) \n",
    "        +2.5 * (pl.col(f'Vol4_{metric}') - AVG_TUF) \n",
    "        +3.5 * (pl.col(f'Vol3_{metric}') - AVG_TUF) \n",
    "        +4.5 * (pl.col(f'Vol2_{metric}') - AVG_TUF) \n",
    "        +5.5 * (pl.col(f'Vol1_{metric}') - AVG_TUF)\n",
    "        ) / 143\n",
    "    )\n",
    "\n",
    "    #AVG_TUF_4\n",
    "    df2 = df2.with_columns(\n",
    "        AVG_TUF_4 = pl.mean_horizontal([f'Vol{i}_{metric}' for i in range(5,9)])\n",
    "    )\n",
    "\n",
    "    #SLOPE_4\n",
    "    AVG_TUF_4 = pl.col('AVG_TUF_4') # just for formatting\n",
    "    df2 = df2.with_columns(\n",
    "        SLOPE_4 = (\n",
    "        -1.5 * (pl.col(f'Vol4_{metric}') - AVG_TUF_4) \n",
    "        -0.5 * (pl.col(f'Vol3_{metric}') - AVG_TUF_4) \n",
    "        +0.5 * (pl.col(f'Vol2_{metric}') - AVG_TUF_4) \n",
    "        +1.5 * (pl.col(f'Vol1_{metric}') - AVG_TUF_4)\n",
    "        ) / 15\n",
    "    )\n",
    "\n",
    "    #INDICATOR_SLOPE13\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('SLOPE_13')>THRE_13).then(pl.lit(1))\n",
    "        .when(pl.col('SLOPE_13')<-1*THRE_13).then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_SLOPE13')\n",
    "    )\n",
    "\n",
    "    #INDICATOR_SLOPE4\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('SLOPE_4')>THRE_4).then(pl.lit(1))\n",
    "        .when(pl.col('SLOPE_4')<-1*THRE_4).then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_SLOPE4')\n",
    "    )\n",
    "\n",
    "    #PEAK_DETECTOR\n",
    "    cols_1_13 = [f'Vol{i}_{metric}' for i in range(1,14)]\n",
    "    df2 = df2.with_columns(\n",
    "        PEAK_DETECTOR = pl.max_horizontal(cols_1_13)/pl.sum_horizontal(cols_1_13)\n",
    "    )\n",
    "\n",
    "    #INDICATOR_PEAK\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('PEAK_DETECTOR')>= 0.5).then(pl.lit(1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_PEAK')\n",
    "    )\n",
    "\n",
    "    #FINAL_SLOPE\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when((pl.col('INDICATOR_SLOPE13') == 1) & (pl.col('INDICATOR_SLOPE4') >= 0))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 0) & (pl.col('INDICATOR_SLOPE4') == 1))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == -1) & (pl.col('INDICATOR_SLOPE4') <= 0))\n",
    "            .then(pl.lit(-1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 0) & (pl.col('INDICATOR_SLOPE4') == -1))\n",
    "            .then(pl.lit(-1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == -1) & (pl.col('INDICATOR_SLOPE4') == 1) & (pl.col('SLOPE_4') >= 1))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 1) & (pl.col('INDICATOR_SLOPE4') == -1) & (pl.col('SLOPE_4') <= -1))\n",
    "            .then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0))\n",
    "        .alias('FINAL_SLOPE')\n",
    "    )\n",
    "\n",
    "    #GROWDECL\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('FINAL_SLOPE') == 1)\n",
    "            .then(pl.lit(\"P\"))\n",
    "        .when(pl.col('FINAL_SLOPE') == -1)\n",
    "            .then(pl.lit(\"Q\"))\n",
    "        .otherwise(pl.lit(\"S\"))\n",
    "        .alias(f'Trend_{metric}')\n",
    "    )\n",
    "\n",
    "    df2 = df2.select(['IID','product_id',f'Trend_{metric}'])\n",
    "    df = df.join(df2,on = ['IID','product_id'],how = 'left')\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:06:00.924536Z",
     "iopub.status.busy": "2024-05-31T09:06:00.923874Z",
     "iopub.status.idle": "2024-05-31T09:06:03.024421Z",
     "shell.execute_reply": "2024-05-31T09:06:03.023414Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calling Functions and Exporting Feeds-\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/Prescriber/Weekly/'\n",
    "\n",
    "temp1 = mp_spec_seg_dec.select(['IID','geography_id'])\n",
    "temp1 = add_volume_cols(temp1)\n",
    "temp1 = process_share(temp1)\n",
    "\n",
    "# For removal of extra rows -\n",
    "temp1 = temp1.with_columns(\n",
    "    flag_sum = pl.sum_horizontal([f'Vol{i}_TUF' for i in range(1,14)])\n",
    ").filter(pl.col('flag_sum')!=0).drop('flag_sum')\n",
    "\n",
    "temp1 = process_trend(temp1,'TUF')\n",
    "temp1 = process_trend(temp1,'NUF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:06:03.033207Z",
     "iopub.status.busy": "2024-05-31T09:06:03.032871Z",
     "iopub.status.idle": "2024-05-31T09:06:03.403022Z",
     "shell.execute_reply": "2024-05-31T09:06:03.402229Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Converting to Feed ready data\n",
    "\n",
    "temp1 = temp1.with_columns(ReportType = pl.lit('WEEKLY'))\n",
    "\n",
    "trx_cols = ['IID','geography_id','product_id','ReportType'] + [col for col in temp1.columns if '_TUF' in col ]\n",
    "nrx_cols = ['IID','geography_id','product_id','ReportType'] + [col for col in temp1.columns if '_NUF' in col ]\n",
    "\n",
    "temp1_TUF = temp1.select(trx_cols).with_columns(Metric = pl.lit('TRX'))\n",
    "temp1_NUF = temp1.select(nrx_cols).with_columns(Metric = pl.lit('NRX'))\n",
    "\n",
    "for df_name in ['temp1_TUF','temp1_NUF']: #renaming for vstack\n",
    "    for col in globals()[df_name].columns:\n",
    "        if col.endswith('_TUF'):\n",
    "            globals()[df_name] = globals()[df_name].rename({col: col.replace('_TUF', '')})\n",
    "        elif col.endswith('_NUF'):\n",
    "            globals()[df_name] = globals()[df_name].rename({col: col.replace('_NUF', '')})\n",
    "\n",
    "# Setting up Sequence\n",
    "final_sequence = ['IID','geography_id','product_id','Metric','ReportType','Trend'] + [f'Vol{i}' for i in range(1,14)] + [f'Shr{i}' for i in range(1,14)]\n",
    "\n",
    "temp1_TUF = temp1_TUF.select(final_sequence).rename({'IID':'Physician_ID','geography_id':'Geography_id','product_id':'Product_id'})\n",
    "temp1_NUF = temp1_NUF.select(final_sequence).rename({'IID':'Physician_ID','geography_id':'Geography_id','product_id':'Product_id'})\n",
    "\n",
    "temp2 = temp1_TUF.vstack(temp1_NUF) # final dataframe\n",
    "\n",
    "for new_col in ['DS1_Vol','DS2_Vol']:\n",
    "    for i in range(1,14):\n",
    "        col_name = f'{new_col}{i}'\n",
    "        temp2 = temp2.with_columns(pl.lit('\\\\N').alias(col_name)) #change this to /N later ? # null --> \\N (harsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:06:03.409142Z",
     "iopub.status.busy": "2024-05-31T09:06:03.408809Z",
     "iopub.status.idle": "2024-05-31T09:06:43.064472Z",
     "shell.execute_reply": "2024-05-31T09:06:43.062841Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presc Trend Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "# Export\n",
    "temp2.to_pandas().to_csv(f'{OUT}Weekly_Prescriber_Trend_Feed.txt', sep='|')\n",
    "print('Presc Trend Feed Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prescriber Trend Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T09:06:43.068933Z",
     "iopub.status.busy": "2024-05-31T09:06:43.067937Z",
     "iopub.status.idle": "2024-05-31T09:06:43.148845Z",
     "shell.execute_reply": "2024-05-31T09:06:43.146820Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presc X Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "rx_date = datetime.strptime(data_date,'%Y%m%d')\n",
    "list_of_dates = [rx_date]\n",
    "serial_no = [i for i in range(1,14)]\n",
    "for i in range(1,13):\n",
    "    date_val = rx_date - timedelta(days = 7*i)\n",
    "    list_of_dates.append(date_val)\n",
    "\n",
    "\n",
    "date_df = pl.DataFrame(\n",
    "    {\n",
    "        'X':serial_no,\n",
    "        'Name':list_of_dates\n",
    "    }\n",
    ")\n",
    "\n",
    "date_df = date_df.with_columns(\n",
    "   date_df['Name'].dt.strftime('%m/%d/%Y')\n",
    ")\n",
    "\n",
    "date_df.to_pandas().to_csv(f'{OUT}Weekly_Prescriber_X_Feed.txt', sep='|')\n",
    "print('Presc X Feed Exported !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
