{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f34425-570f-426b-8445-419e260ba4a5",
   "metadata": {},
   "source": [
    "### GS Sales Activity v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a546890e-c348-4507-ba0c-133a303e3471",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import datetime, timedelta,date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7f9655-5d51-4e38-ba27-8232ab28dad2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "curr_date = datetime.strptime(js['curr_date'], '%Y-%m-%d').date()\n",
    "quarter_start = datetime.strptime(js['quarter_start'], '%Y-%m-%d').date()\n",
    "quarter_end = datetime.strptime(js['quarter_end'], '%Y-%m-%d').date()\n",
    "qtr_data = js['qtr_data']\n",
    "num_weeks_calls = js['num_weeks_calls']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "num_of_months = js['num_of_months']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "geo = f's3://{bucket}/PYADM/quaterly/{qtr_data}/geography/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07053fe5-ca6d-41f0-a4c9-be46264e7d10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')\n",
    "\n",
    "def intck(interval, start_date, end_date):\n",
    "    if interval == 'DAY':\n",
    "        return (end_date - start_date).days\n",
    "    elif interval == 'MONTH':\n",
    "        rd = relativedelta(end_date, start_date)\n",
    "        return rd.years * 12 + rd.months\n",
    "    elif interval == 'WEEK':\n",
    "        return (end_date - start_date).days // 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6e749e-8e2a-444c-9fce-c67f8a95e4c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "load('temp_calls')\n",
    "load('temp_samples')\n",
    "load('temp_abbv')\n",
    "load('mp_spec_seg_dec')\n",
    "load('hierarchy',geo)\n",
    "load('wd_raw')\n",
    "load('lirwd_call_plan')\n",
    "load('laxdn_geoid_sum')\n",
    "load('MASTER_UNI')\n",
    "load('roster')\n",
    "\n",
    "geo_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/GeographyMapping.txt',separator='|')\n",
    "geo_mapping = geo_mapping.with_columns(\n",
    "    Code = pl.when(pl.col('Code')!= 'NATION').then(pl.lit('1111-')+pl.col('Code')).otherwise(pl.col('Code'))\n",
    ")\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "\n",
    "geo_id_full = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "\n",
    "#fixes for vortex import -> Probably caused by Polars Upgrades\n",
    "temp_calls = temp_calls.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_samples = temp_samples.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_abbv = temp_abbv.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "wd_raw = wd_raw.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "laxdn_geoid_sum = laxdn_geoid_sum.with_columns(pl.col('geography_id').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9aa10c-656f-4c5f-8fea-26031bc69cdc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Processing  1. temp calls  2. temp samples 3. temp abbv datasets\n",
    "# - doubt is physiian terr id same as salesrepterrid for every record?\n",
    "temp_calls_mp_spec = (\n",
    "    temp_calls\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")\n",
    "# NOTE -\n",
    "# Combining MP and dropping null geo\n",
    "# Adding Area and Region Code\n",
    "# Adding Working Day\n",
    "# Adding call_freq_quarter\n",
    "\n",
    "# For Supproting Calc ->\n",
    "#geo_code_mapper = temp_calls_mp_spec[['geography_id','region_geography_id','area_geography_id','nation_geography_id']].unique()\n",
    "geo_code_mapper = geo_id_full\n",
    "geo_code_mapper.to_pandas().to_parquet(dflib+'geo_code_mapper.parquet') #exporting for other code use\n",
    "\n",
    "###\n",
    "temp_samples_mp_spec = (\n",
    "    temp_samples\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")\n",
    "###\n",
    "temp_abbv_mp_spec = (\n",
    "    temp_abbv\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12d2f4-4fc7-4f19-8b03-12ff90b64589",
   "metadata": {},
   "source": [
    "### Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39407b19-ccf6-4121-8c48-6043249a63f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets Count of HCP For All Rollups (Just for prod LIN)\n",
    "def get_num_hcp_counts():\n",
    "    # get presc_count :\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    #source_df = source_df.filter(pl.col('call_freq_quarter').is_not_null()) # should i change this to segment = target ? \n",
    "    source_df = source_df.filter(pl.col('segment')=='Target')\n",
    "    for g in levels:\n",
    "        df = (\n",
    "            source_df\n",
    "            .group_by([g,sg,p,spc,d]) # Remove p as only linzess data present ? .\n",
    "            .agg(num_hcps = pl.col('AttendeeIID').n_unique())\n",
    "        )\n",
    "        \n",
    "        main_seq = [g,p,sg,d,spc] + ['num_hcps']\n",
    "        agg_dict = {'num_hcps':pl.col('AttendeeIID').n_unique()}\n",
    "        \n",
    "        # First Round - \n",
    "        sg_df = (source_df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (source_df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (source_df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (source_df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (source_df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (source_df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (source_df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    \n",
    "    return(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34906293-b228-4e34-80ae-aa4174fffbf9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets Call_Freq_Quarter for all rollups -\n",
    "def get_call_freq_quarter_vals():\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    for g in levels:\n",
    "        df = (\n",
    "            source_df\n",
    "            .group_by([g,sg,p,spc,d]) # Remove p as only linzess data present ? .\n",
    "            .agg(\n",
    "                call_freq_quarter = pl.col('call_freq_quarter').sum(), # doubt\n",
    "            )\n",
    "        )\n",
    "        main_seq = [g,p,sg,d,spc] + ['call_freq_quarter']\n",
    "        agg_dict = {'call_freq_quarter':pl.col('call_freq_quarter').sum()}\n",
    "        \n",
    "        # First Round - \n",
    "        sg_df = (source_df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (source_df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (source_df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (source_df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (source_df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (source_df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (source_df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    \n",
    "    return(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523ff1d8-62e5-485b-8c72-0c43ecceab65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets days_in_field counts for MAX rollup row only - \n",
    "def get_days_in_field_counts():\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    for g in levels:\n",
    "        df = (\n",
    "            source_df\n",
    "            .group_by([g,sg,p,spc,d])\n",
    "            .agg(days_in_field = pl.col('days_in_field').mean()) # WD (using mean here because all vals are same))\n",
    "        )\n",
    "        main_seq = [g,p,sg,d,spc] + ['days_in_field']\n",
    "        agg_dict = {'days_in_field':pl.col('days_in_field').mean()}\n",
    "        # First Round - \n",
    "        sg_df = (source_df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (source_df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (source_df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (source_df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (source_df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (source_df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (source_df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac86949-f55f-4fa5-9bfb-ed6c3c302bcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets count of calls for all rollsups for a given source (temp_call)\n",
    "def get_num_calls_counts(source,agg_var,fl=None):\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    source_df = source.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "    if fl == 1: #dropping invalid calls from temp_calls only | NOT temp_abbv\n",
    "        source_df = (\n",
    "            source_df\n",
    "            .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "            .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "            .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "        )\n",
    "    for g in levels:\n",
    "        df = (\n",
    "            source_df\n",
    "            .group_by([g,sg,p,spc,d]) # Remove p as only linzess data present ? .\n",
    "            .agg(pl.col('CallID').n_unique().alias(agg_var))\n",
    "        )\n",
    "        main_seq = [g,p,sg,d,spc] + [agg_var]\n",
    "        agg_dict = {agg_var:pl.col('CallID').n_unique()}\n",
    "        # First Round - \n",
    "        sg_df = (source_df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (source_df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (source_df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (source_df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (source_df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (source_df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (source_df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84e6341-ba92-4e64-8bbf-cec3466628c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets coutn of abbv visits for all rollups\n",
    "def get_num_abbv_calls_counts(agg_var):\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "\n",
    "    ab = (\n",
    "        temp_abbv_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .group_by('AttendeeIID').agg(ab_calls = pl.col('CallID').n_unique())\n",
    "        .rename({'AttendeeIID':'IID'})\n",
    "        .join(mp_spec_seg_dec,on='IID',how='left')\n",
    "    )\n",
    "    \n",
    "    iw = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "        .group_by('AttendeeIID').agg(iw_calls = pl.col('CallID').n_unique())\n",
    "        .rename({'AttendeeIID':'IID'})\n",
    "        .join(mp_spec_seg_dec,on='IID',how='left')\n",
    "        .join(mp_spec_seg_dec.filter(segment = 'Target'),on = ['IID','geography_id','specialty_group','segment','decile'], how = 'outer_coalesce')\n",
    "        .join(ab,on = ['IID','geography_id','specialty_group','segment','decile'],how = 'left')\n",
    "        .join(geo_code_mapper,on='geography_id',how='left')\n",
    "        .with_columns(pl.lit('LIN').alias(p))\n",
    "    )\n",
    "\n",
    "    for g in levels:\n",
    "        df = (\n",
    "            iw\n",
    "            .group_by([g,sg,p,spc,d]) # Remove p as only linzess data present ? .\n",
    "            .agg(pl.col('ab_calls').sum().alias(agg_var))\n",
    "        )\n",
    "        main_seq = [g,p,sg,d,spc] + [agg_var]\n",
    "        agg_dict = {agg_var:pl.col('ab_calls').sum()}\n",
    "        # First Round - \n",
    "        sg_df = (iw.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (iw.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (iw.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (iw.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (iw.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (iw.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (iw.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86132a5-cad8-4659-ae32-59e947306b9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets Count of Calls for just geo rollups.\n",
    "def get_total_calls_counts(): #Total Calls Per Geo -\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    res = []\n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    for g in levels:\n",
    "        df = (\n",
    "            source_df\n",
    "            .group_by([g])\n",
    "            .agg(total_calls = pl.col('CallID').n_unique())\n",
    "        )\n",
    "        res.append(df)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6b0deaa-825f-4094-a310-656223f7a477",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets Count of targets for all rollups-\n",
    "def get_target_hcps_counts():\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    source_df = mp_spec_seg_dec.filter(pl.col('segment')=='Target').join(geo_code_mapper,on = levels[0],how = 'left')\n",
    "    for g in levels:\n",
    "        df = source_df.group_by([g,sg,d,spc]).agg(target_hcps = pl.col('IID').n_unique())\n",
    "        main_seq = [g,sg,d,spc] + ['target_hcps']\n",
    "        agg_dict = {'target_hcps':pl.col('IID').n_unique()}\n",
    "        # First Round - \n",
    "        sg_df = (source_df.group_by([g,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (source_df.group_by([g,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (source_df.group_by([g,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (source_df.group_by([g,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (source_df.group_by([g,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (source_df.group_by([g,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (source_df.group_by([g]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d018054-e0a2-4528-a986-888bef9adda3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets count of 13 week targets for all rollups -\n",
    "def get_tgts_13wks_counts():\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    source_df = temp_calls_mp_spec.filter((pl.col('call_week')<=13)&(pl.col('segment')=='Target'))\n",
    "    for g in levels:\n",
    "        df = source_df.group_by([g,sg,p,spc,d]).agg(tgts_13wks = pl.col('AttendeeIID').n_unique())\n",
    "        main_seq = [g,sg,p,d,spc] + ['tgts_13wks']\n",
    "        agg_dict = {'tgts_13wks':pl.col('AttendeeIID').n_unique()}\n",
    "        # First Round - \n",
    "        sg_df = (source_df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (source_df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (source_df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (source_df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (source_df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (source_df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (source_df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e807ea-10e2-4a26-8757-dfcb60433bbf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets count of sampled HCPs for all rollups-\n",
    "def get_num_sample_hcp_counts():\n",
    "    def samples_hcp_utl(product_input):\n",
    "        p,sg,spc,d = product_input,'segment','specialty_group','decile'\n",
    "        levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "        sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "        res = []\n",
    "        source_df = (\n",
    "            temp_samples_mp_spec\n",
    "            .filter(pl.col('sample_week')<=num_weeks_calls)\n",
    "            .filter(pl.col('CallDate')>= quarter_start)\n",
    "            .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "            .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "            .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "            ### TO GET SUB  PRODUCT ####\n",
    "            .with_columns(\n",
    "                pl.when(pl.col('CallProductDescription')==\"145 mcg\").then(pl.lit('LI1'))\n",
    "                .when(pl.col('CallProductDescription')==\"290 mcg\").then(pl.lit('LI2'))\n",
    "                .when(pl.col('CallProductDescription')==\"72 mcg\").then(pl.lit('LI3'))\n",
    "                .otherwise(None)\n",
    "                .alias('sub_product')\n",
    "            )\n",
    "            .filter(segment = 'Target')\n",
    "        )\n",
    "        for g in levels:\n",
    "            df = (source_df.group_by([g,sg,p,spc,d]).agg(num_sample_hcps = pl.col('AttendeeIID').n_unique()))\n",
    "            main_seq = [g,p,sg,d,spc] + ['num_sample_hcps']\n",
    "            agg_dict = {'num_sample_hcps':pl.col('AttendeeIID').n_unique()}\n",
    "            # First Round - \n",
    "            sg_df = (source_df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "            d_df = (source_df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "            spc_df = (source_df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "            # Second Round - \n",
    "            sg_d_df = (source_df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "            sg_spc_df = (source_df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "            d_spc_df = (source_df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "            # Third Round\n",
    "            sg_d_spc_df = (source_df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "            df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "            if product_input != 'product':\n",
    "                df = df.rename({'sub_product':'product'})\n",
    "            res.append(df)\n",
    "        return (res)\n",
    "\n",
    "    res_LINF = samples_hcp_utl('product')\n",
    "    res_LINP = samples_hcp_utl('sub_product')\n",
    "\n",
    "    res = []\n",
    "    for g,i in zip(levels,[0,1,2,3]):\n",
    "        frame = res_LINP[i]\n",
    "        frame_parent = res_LINF[i]\n",
    "        frame_pivoted = (\n",
    "            frame\n",
    "            .pivot(values=\"num_sample_hcps\", index=[g, \"segment\", \"decile\", \"specialty_group\"], columns=\"product\")\n",
    "            .rename({\"LI1\": \"num_sample_hcps_LI1\", \"LI2\": \"num_sample_hcps_LI2\", \"LI3\": \"num_sample_hcps_LI3\"})\n",
    "        ).fill_null(0)\n",
    "    \n",
    "        frame_final = frame_parent.join(frame_pivoted, on = [g, \"segment\", \"decile\", \"specialty_group\"] ,how = 'outer_coalesce')\n",
    "        res.append(frame_final)\n",
    "\n",
    "    return(res)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a6b1f84-2c35-45a2-bd68-5154ed8fa29c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets count of samples for all rollups-\n",
    "def get_num_samples_count():\n",
    "    def samples_util(product_input):\n",
    "        p,sg,spc,d = product_input,'segment','specialty_group','decile'\n",
    "        levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "        sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "        res = []\n",
    "        source_df = (\n",
    "            temp_samples_mp_spec\n",
    "            .filter(pl.col('sample_week')<=num_weeks_calls)\n",
    "            .filter(pl.col('CallDate')>= quarter_start)\n",
    "            .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "            .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "            .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "            ### TO GET SUB  PRODUCT ####\n",
    "            .with_columns(\n",
    "                pl.when(pl.col('CallProductDescription')==\"145 mcg\").then(pl.lit('LI1'))\n",
    "                .when(pl.col('CallProductDescription')==\"290 mcg\").then(pl.lit('LI2'))\n",
    "                .when(pl.col('CallProductDescription')==\"72 mcg\").then(pl.lit('LI3'))\n",
    "                .otherwise(None)\n",
    "                .alias('sub_product')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for g in levels:\n",
    "            df = (\n",
    "                source_df\n",
    "                .group_by([g,sg,p,spc,d])\n",
    "                .agg(pl.col('CallProductQuantity').sum().alias('num_samples'))\n",
    "            )\n",
    "            main_seq = [g,p,sg,d,spc] + ['num_samples']\n",
    "            agg_dict = {'num_samples':pl.col('CallProductQuantity').sum()}\n",
    "            # First Round - \n",
    "            sg_df = (source_df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "            d_df = (source_df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "            spc_df = (source_df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "            # Second Round - \n",
    "            sg_d_df = (source_df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "            sg_spc_df = (source_df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "            d_spc_df = (source_df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "            # Third Round\n",
    "            sg_d_spc_df = (source_df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "            df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "            \n",
    "            if product_input != 'product':\n",
    "                df = df.rename({'sub_product':'product'})\n",
    "            res.append(df)\n",
    "        return(res)\n",
    "    \n",
    "    \n",
    "    res_LINF = samples_util('product')\n",
    "    res_LINP = samples_util('sub_product')\n",
    "\n",
    "    res = []\n",
    "    for g,i in zip(levels,[0,1,2,3]):\n",
    "        frame = res_LINP[i]\n",
    "        frame_parent = res_LINF[i]\n",
    "        frame_pivoted = (\n",
    "            frame\n",
    "            .pivot(values=\"num_samples\", index=[g, \"segment\", \"decile\", \"specialty_group\"], columns=\"product\")\n",
    "            .rename({\"LI1\": \"num_samples_LI1\", \"LI2\": \"num_samples_LI2\", \"LI3\": \"num_samples_LI3\"})\n",
    "        ).fill_null(0)\n",
    "    \n",
    "        frame_final = frame_parent.join(frame_pivoted, on = [g, \"segment\", \"decile\", \"specialty_group\"] ,how = 'outer_coalesce')\n",
    "        res.append(frame_final)\n",
    "    \n",
    "\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dba23639-e96b-433e-8a7b-61bc502972e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets Count of samples for just geo rollups.\n",
    "def get_total_samples_counts(): #Total Samples Per Geo -\n",
    "    p,sg,spc,d = 'sub_product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    res = []\n",
    "    source_df = (\n",
    "        temp_samples_mp_spec\n",
    "        .filter(pl.col('sample_week')<=num_weeks_calls)\n",
    "        .filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "        ### TO GET SUB  PRODUCT ####\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('CallProductDescription')==\"145 mcg\").then(pl.lit('LI1'))\n",
    "            .when(pl.col('CallProductDescription')==\"290 mcg\").then(pl.lit('LI2'))\n",
    "            .when(pl.col('CallProductDescription')==\"72 mcg\").then(pl.lit('LI3'))\n",
    "            .otherwise(None)\n",
    "            .alias('sub_product')\n",
    "        )\n",
    "    )\n",
    "    for g in levels:\n",
    "        df = (\n",
    "            source_df\n",
    "            .group_by([g,p])\n",
    "            .agg(total_samples = pl.col('CallProductQuantity').sum())\n",
    "        )\n",
    "        res.append(df)\n",
    "\n",
    "    res2 = []\n",
    "    for g,i in zip(levels,[0,1,2,3]):\n",
    "        frame = res[i]\n",
    "        frame_pivoted = (\n",
    "            frame\n",
    "            .pivot(values=\"total_samples\", index=[g], columns=\"sub_product\")\n",
    "            .rename({\"LI1\": \"total_samples_LI1\", \"LI2\": \"total_samples_LI2\", \"LI3\": \"total_samples_LI3\"})\n",
    "        ).fill_null(0).with_columns(\n",
    "            product = pl.lit('LIN'),\n",
    "            total_samples_LIN = pl.sum_horizontal(['total_samples_LI1','total_samples_LI2','total_samples_LI3'])\n",
    "        ).select([g,'total_samples_LIN','total_samples_LI1','total_samples_LI2','total_samples_LI3'])\n",
    "\n",
    "        \n",
    "        res2.append(frame_pivoted)\n",
    "    \n",
    "    return(res2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9054b395-e508-4dd4-a8b0-f8a507728b12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets count of targets with more than 3 calls at all rollups - \n",
    "def get_tgts3_counts(): #Num_Of_QTD_Tgts_3Plus_Calls\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "        .filter(segment = 'Target')\n",
    "    )\n",
    "    source_df = (\n",
    "        source_df\n",
    "        .group_by('AttendeeIID').agg(pl.col('CallID').n_unique().alias('calls'))\n",
    "        .filter(pl.col('calls')>=3)\n",
    "        .join(mp_spec_seg_dec,left_on='AttendeeIID',right_on='IID')\n",
    "        .join(geo_code_mapper,on='geography_id',how='left').drop('calls')\n",
    "    )\n",
    "    for g in levels:\n",
    "        df = source_df.group_by([g,sg,d,spc]).agg(tgts3 = pl.col('AttendeeIID').n_unique())\n",
    "        main_seq = [g,sg,d,spc] + ['tgts3']\n",
    "        agg_dict = {'tgts3':pl.col('AttendeeIID').n_unique()}\n",
    "        # First Round - \n",
    "        sg_df = (source_df.group_by([g,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (source_df.group_by([g,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (source_df.group_by([g,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (source_df.group_by([g,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (source_df.group_by([g,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (source_df.group_by([g,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (source_df.group_by([g]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e2a613-f394-48e0-8ea1-5edbe65860c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets count of HCPs who are considered Optimal , Bellow for all rollups \n",
    "def get_optimal_below_counts():\n",
    "    col = 'optimal'\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    source_df = (\n",
    "        source_df\n",
    "        .group_by('AttendeeIID').agg(month_call_count = pl.col('call_month').n_unique())\n",
    "        .join(mp_spec_seg_dec,left_on='AttendeeIID',right_on='IID')\n",
    "        .join(geo_code_mapper,on='geography_id',how='left').drop('calls')\n",
    "    )\n",
    "    source_df_o = source_df.filter(pl.col('month_call_count')==num_of_months).filter(segment = 'Target')\n",
    "    \n",
    "    res = []\n",
    "    for g in levels:\n",
    "        main_seq = [g,d,spc] + [col]\n",
    "        df = source_df_o.group_by([g,d,spc]).agg(pl.col('AttendeeIID').n_unique().alias(col))\n",
    "        d_df = source_df_o.group_by([g,spc]).agg(pl.col('AttendeeIID').n_unique().alias(col)).with_columns(d_roll_up.alias(d)).select(main_seq)\n",
    "        spc_df = source_df_o.group_by([g,d]).agg(pl.col('AttendeeIID').n_unique().alias(col)).with_columns(spc_roll_up.alias(spc)).select(main_seq)\n",
    "        d_spc_df = source_df_o.group_by([g]).agg(pl.col('AttendeeIID').n_unique().alias(col)).with_columns(spc_roll_up.alias(spc),d_roll_up.alias(d)).select(main_seq)\n",
    "        df = (df.select(main_seq).vstack(d_df).vstack(spc_df).vstack(d_spc_df))\n",
    "    \n",
    "        df1 = (\n",
    "            df.with_columns(segment = pl.lit('UNI'))\n",
    "            .vstack(df.with_columns(segment = pl.lit('Target')))\n",
    "            .vstack(df.with_columns(pl.lit(0).cast(pl.UInt32).alias(col),pl.lit('ALG-ONLY-TARGET').alias('segment')))\n",
    "            .vstack(df.with_columns(pl.lit(0).cast(pl.UInt32).alias(col),pl.lit('Non-Target').alias('segment')))\n",
    "            .select([g,d,spc,sg,col])\n",
    "        )\n",
    "        # Pulling count of targets to subtract and get bellow HCPs - \n",
    "        df1 = (\n",
    "            df1\n",
    "            .join(target_hcps_counts[levels.index(g)],on = [g,d,spc,sg],how='left')\n",
    "            .with_columns(\n",
    "                pl.when(pl.col('segment').is_in(['UNI','Target'])).then(pl.col('target_hcps')-pl.col('optimal')).otherwise(pl.lit(0).cast(pl.UInt32)).alias('below')\n",
    "            )\n",
    "            .drop('target_hcps')\n",
    "        )\n",
    "        res.append(df1)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41dcfaaf-b146-48d5-847c-5b9b57fdfed1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets count of mean number of called months in last 12 months at all rollups-\n",
    "def get_called_12m_counts():\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    \n",
    "    ###\n",
    "    iw = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "        .group_by('AttendeeIID').agg(iw_calls = pl.col('CallID').n_unique())\n",
    "        .rename({'AttendeeIID':'IID'})\n",
    "        .join(mp_spec_seg_dec,on='IID',how='left')\n",
    "        .join(mp_spec_seg_dec.filter(segment = 'Target'),on = ['IID','geography_id','specialty_group','segment','decile'], how = 'outer_coalesce')\n",
    "    \n",
    "    )\n",
    "    qtd_iw_iids = list(iw['IID'].unique())\n",
    "    ###\n",
    "    source_df = (\n",
    "         temp_calls_mp_spec.filter(pl.col('call_month')<=12)\n",
    "        .filter(pl.col('AttendeeIID').is_in(qtd_iw_iids))\n",
    "        .group_by('AttendeeIID').agg(called_months = pl.col('call_month').n_unique())\n",
    "        .join(mp_spec_seg_dec,left_on='AttendeeIID',right_on='IID')\n",
    "        .join(geo_code_mapper,on='geography_id',how='left')\n",
    "    )\n",
    "    terr = pl.DataFrame()\n",
    "    g = levels[0]\n",
    "    df = source_df.group_by([g,sg,d,spc]).agg(called_12m = pl.col('called_months').mean())\n",
    "    main_seq = [g,sg,d,spc] + ['called_12m']\n",
    "    agg_dict = {'called_12m':pl.col('called_months').mean()}\n",
    "    # First Round - \n",
    "    sg_df = (source_df.group_by([g,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "    d_df = (source_df.group_by([g,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "    spc_df = (source_df.group_by([g,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "    # Second Round - \n",
    "    sg_d_df = (source_df.group_by([g,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "    sg_spc_df = (source_df.group_by([g,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "    d_spc_df = (source_df.group_by([g,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "    # Third Round\n",
    "    sg_d_spc_df = (source_df.group_by([g]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "    df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "    terr = df\n",
    "    res.append(terr)\n",
    "    \n",
    "    #Region = Mean of Area - \n",
    "    reg = (\n",
    "        terr.join(geo_code_mapper,on = 'geography_id',how='left')\n",
    "        .group_by(['region_geography_id',sg,d,spc]).agg(called_12m = pl.col('called_12m').mean())\n",
    "    )\n",
    "    res.append(reg)\n",
    "    \n",
    "    \n",
    "    #Area = Mean of Region\n",
    "    area = (\n",
    "        reg\n",
    "        .join(geo_code_mapper.select(['region_geography_id','area_geography_id']).unique(),on='region_geography_id',how='left')\n",
    "        .group_by(['area_geography_id',sg,d,spc]).agg(called_12m = pl.col('called_12m').mean())\n",
    "    )\n",
    "    res.append(area)\n",
    "    \n",
    "    #Nation = Mean of Area\n",
    "    nation = (\n",
    "        area\n",
    "        .join(geo_code_mapper.select(['area_geography_id','nation_geography_id']).unique(),on='area_geography_id',how='left')\n",
    "        .group_by(['nation_geography_id',sg,d,spc]).agg(called_12m = pl.col('called_12m').mean())\n",
    "    )\n",
    "    res.append(nation)\n",
    "    \n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a037ddfd-df5d-4d55-a23e-600e9e59f588",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gets call frequency for all roll ups -\n",
    "def get_call_freq_counts():\n",
    "    p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "    levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    res = []\n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    source_df = (\n",
    "        source_df\n",
    "        .group_by('AttendeeIID').agg(calls = pl.col('CallID').n_unique())\n",
    "        .join(mp_spec_seg_dec,left_on='AttendeeIID',right_on='IID')\n",
    "        .join(geo_code_mapper,on='geography_id',how='left')\n",
    "    )\n",
    "    for g in levels:\n",
    "        df = source_df.group_by([g,sg,d,spc]).agg(call_freq = pl.col('calls').mean())\n",
    "        main_seq = [g,sg,d,spc] + ['call_freq']\n",
    "        agg_dict = {'call_freq':pl.col('calls').mean()}\n",
    "        # First Round - \n",
    "        sg_df = (source_df.group_by([g,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (source_df.group_by([g,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (source_df.group_by([g,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (source_df.group_by([g,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (source_df.group_by([g,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (source_df.group_by([g,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (source_df.group_by([g]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        df = (df.select(main_seq).vstack(sg_df).vstack(d_df).vstack(spc_df).vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df).vstack(sg_d_spc_df))\n",
    "        res.append(df)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62c32d7e-2870-4241-bbf6-dbed6ae8a0c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DATA PREP - Function Calls -\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "num_hcp_counts = get_num_hcp_counts()\n",
    "call_freq_quarter_vals = get_call_freq_quarter_vals()\n",
    "days_in_field_counts = get_days_in_field_counts()\n",
    "num_calls_counts = get_num_calls_counts(temp_calls_mp_spec,'num_calls',1)\n",
    "total_calls_counts = get_total_calls_counts()\n",
    "num_abbv_calls_counts = get_num_abbv_calls_counts('num_abbv_calls')\n",
    "target_hcps_counts = get_target_hcps_counts()\n",
    "tgts_13wks_counts = get_tgts_13wks_counts()\n",
    "num_sample_hcp_counts = get_num_sample_hcp_counts()\n",
    "num_samples_count = get_num_samples_count()\n",
    "total_samples_counts = get_total_samples_counts()\n",
    "tgts3_counts = get_tgts3_counts()\n",
    "optimal_below_counts = get_optimal_below_counts()\n",
    "called_12m_counts = get_called_12m_counts()\n",
    "call_freq_counts= get_call_freq_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93425c22-72be-406d-8fbf-5f7ecd3f6943",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45c696-2b94-4c56-8515-533aed9771ab",
   "metadata": {},
   "source": [
    "### Functions - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e798da4b-af9d-4fb4-973a-446fc280a7df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# num_calls , num_hcps, days_in_field, call_freq_quarter, total_calls , num_abbv_calls_\n",
    "def process_1(df):\n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    for i in range(4):\n",
    "        g = levels[i]\n",
    "        f1 = num_calls_counts[i] #num_calls from here\n",
    "        f2 = num_hcp_counts[i] # num_hcps from here\n",
    "        f3 = (call_freq_quarter_vals[i]) #call freq qtr here\n",
    "        f4 = num_abbv_calls_counts[i]\n",
    "        f5 = total_calls_counts[i]\n",
    "        f6 = days_in_field_counts[i]\n",
    "        f = (\n",
    "            f1\n",
    "            .join(f2,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f3,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f4,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f6,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f5,on=[g],how='outer_coalesce')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55d65579-882a-4e18-8b8e-945e53de458d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# target_hcps, tgts_13wks, called_12m,optimal,below, tgts3 , wk_qtd\n",
    "# num_samples ,num_sample_hcps ,total_samples , call_freq \n",
    "def process_2(df):\n",
    "    source_df = (laxdn_geoid_sum.join(geo_code_mapper,on=levels[0],how = 'left'))\n",
    "    for i in range(4):\n",
    "        g = levels[i]\n",
    "        f1 = target_hcps_counts[i]\n",
    "        f2 = tgts_13wks_counts[i]\n",
    "        f3 = called_12m_counts[i].with_columns(pl.lit('LIN').alias(p))\n",
    "        f4 = optimal_below_counts[i].with_columns(pl.lit('LIN').alias(p))\n",
    "        f6 = tgts3_counts[i].with_columns(pl.lit('LIN').alias(p))\n",
    "        f7 = num_samples_count[i]\n",
    "        f8 = num_sample_hcp_counts[i]\n",
    "        f9 = total_samples_counts[i]\n",
    "        f10 = call_freq_counts[i].with_columns(pl.lit('LIN').alias(p))\n",
    "        fn = (\n",
    "            source_df\n",
    "            .group_by(levels[i])\n",
    "            .agg(\n",
    "                wk_qtd_LIN = pl.col('wk_qtd_LIN').sum(),\n",
    "                wk_qtd_LI1 = pl.col('wk_qtd_LI1').sum(),\n",
    "                wk_qtd_LI2 = pl.col('wk_qtd_LI2').sum(),\n",
    "                wk_qtd_LI3 = pl.col('wk_qtd_LI3').sum(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        f = (\n",
    "            f1.with_columns(pl.lit('LIN').alias(p))\n",
    "            .join(f2,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f7,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f8,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f3,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f4,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f6,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f10,on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "            .join(f9,on=[g],how='left')\n",
    "            .join(fn,on=[g],how='left')\n",
    "        ).fill_null(0)\n",
    "\n",
    "        # merging with process_1()\n",
    "        f = f.join(df[i],on=[g,p,sg,spc,d],how='outer_coalesce')\n",
    "        df[i] = f\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43e93452-919a-4b53-a02f-c02af4538fd0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#call_distribution, \n",
    "def process_3(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = f.with_columns(\n",
    "            call_distribution = pl.col('num_calls') / pl.col('total_calls'),\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c097c01e-b560-40f4-8b14-e5c9f8f4506c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#calls_per_day\n",
    "def process_3_1(df):\n",
    "    f_mr = (\n",
    "        df[0]\n",
    "        .filter((pl.col('segment')=='UNI')&(pl.col('decile')=='0-10')&(pl.col('specialty_group')=='ALL SPEC'))\n",
    "        .with_columns(calls_per_day = pl.col('num_calls')/pl.col('days_in_field'))\n",
    "        .select(['geography_id','calls_per_day'])\n",
    "    )\n",
    "    # Terr Level - \n",
    "    ft = df[0].join(f_mr,on = 'geography_id',how='left')\n",
    "    df[0] = ft\n",
    "    # All Other levels are avg of their children - \n",
    "    ft = ft.join(geo_code_mapper,on = levels[0], how = 'left')\n",
    "    for i in range(1,4):\n",
    "        f = df[i]\n",
    "        ft_rollup = ft.group_by(levels[i]).agg(calls_per_day = pl.col('calls_per_day').mean())\n",
    "        f = f.join(ft_rollup, on = levels[i],how = 'left')\n",
    "        df[i] = f\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a16f20bb-acd5-4307-a05b-63541784068e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#qtd reach , 13 week reach, QTD_Tgts_Not_Reached\n",
    "def process_4(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                prc_reach = pl.col('num_hcps')/pl.col('target_hcps'),\n",
    "                tgt_reach_13wk = pl.col('tgts_13wks')/pl.col('target_hcps'),\n",
    "                #qtd_tgt_nreach = pl.col('target_hcps')- pl.col('num_hcps')\n",
    "                qtd_tgt_nreach = pl.when(pl.col('num_hcps').is_null()).then(pl.col('target_hcps')).otherwise(pl.col('target_hcps') - pl.col('num_hcps'))\n",
    "            )\n",
    "            .drop('tgts_13wks')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db77978a-9304-4df2-b986-390f280789ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Trx Per Call\n",
    "def process_5(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        f = f.with_columns(\n",
    "            rx_per_call = pl.col('wk_qtd_LIN')/pl.col('num_calls'),\n",
    "            rx_per_call_LI1 = pl.col('wk_qtd_LI1')/pl.col('num_calls'),\n",
    "            rx_per_call_LI2 = pl.col('wk_qtd_LI2')/pl.col('num_calls'),\n",
    "            rx_per_call_LI3 = pl.col('wk_qtd_LI3')/pl.col('num_calls'),\n",
    "        )\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "addcdfc8-fc77-4acb-930d-e4e46268bbd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sample_distribution, prc_sampled_phy, ,rx_per_sample\n",
    "def process_6(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                sample_distribution = pl.col('num_samples')/pl.col('total_samples_LIN'),\n",
    "                \n",
    "                prc_sampled_phy_LIN = pl.col('num_sample_hcps')/pl.col('target_hcps'),\n",
    "                prc_sampled_phy_LI1 = pl.col('num_sample_hcps_LI1')/pl.col('target_hcps'),\n",
    "                prc_sampled_phy_LI2 = pl.col('num_sample_hcps_LI2')/pl.col('target_hcps'),\n",
    "                prc_sampled_phy_LI3 = pl.col('num_sample_hcps_LI3')/pl.col('target_hcps'),\n",
    "                \n",
    "                rx_per_sample_LIN = pl.col('wk_qtd_LIN')/pl.col('total_samples_LIN'),\n",
    "                rx_per_sample_LI1 = pl.col('wk_qtd_LI1')/pl.col('total_samples_LI1'),\n",
    "                rx_per_sample_LI2 = pl.col('wk_qtd_LI2')/pl.col('total_samples_LI2'),\n",
    "                rx_per_sample_LI3 = pl.col('wk_qtd_LI3')/pl.col('total_samples_LI3'),\n",
    "            )#.drop('wk_qtd')\n",
    "        )\n",
    "\n",
    "        df[i] = f\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3698c00-dbd2-4e6f-93af-f276ff8aafea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# avg_sample_per_hcp\n",
    "def process_6_1(df):\n",
    "    for i in range(4):\n",
    "        g = levels[i]\n",
    "        f = df[i]\n",
    "        sf = (\n",
    "            f\n",
    "            .select([g,sg,d,spc,p,'num_samples','num_sample_hcps','num_samples_LI1','num_sample_hcps_LI1','num_samples_LI2','num_sample_hcps_LI2','num_samples_LI3','num_sample_hcps_LI3'])\n",
    "            .filter(segment = 'Target')\n",
    "            .with_columns(\n",
    "                avg_sample_per_hcp_LIN = pl.col('num_samples')/pl.col('num_sample_hcps'),\n",
    "                avg_sample_per_hcp_LI1 = pl.col('num_samples_LI1')/pl.col('num_sample_hcps_LI1'),\n",
    "                avg_sample_per_hcp_LI2 = pl.col('num_samples_LI2')/pl.col('num_sample_hcps_LI2'),\n",
    "                avg_sample_per_hcp_LI3 = pl.col('num_samples_LI3')/pl.col('num_sample_hcps_LI3'),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.col('avg_sample_per_hcp_LIN').replace(np.nan,0),\n",
    "                pl.col('avg_sample_per_hcp_LI1').replace(np.nan,0),\n",
    "                pl.col('avg_sample_per_hcp_LI2').replace(np.nan,0),\n",
    "                pl.col('avg_sample_per_hcp_LI3').replace(np.nan,0),\n",
    "            )\n",
    "        )\n",
    "        sf_uni = (sf.drop('segment').with_columns(pl.lit('UNI').alias('segment')).select(sf.columns))\n",
    "        sf = sf.vstack(sf_uni).drop(['num_samples','num_sample_hcps','num_samples_LI1','num_sample_hcps_LI1','num_samples_LI2','num_sample_hcps_LI2','num_samples_LI3','num_sample_hcps_LI3'])\n",
    "        f = (f.join(sf,on = [g,sg,d,spc,p],how='left'))\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e3c9aa2-696f-4f37-a630-f4868af6a69f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#call_freq_goal_prc\n",
    "def process_7(df):\n",
    "    formula_helper_1 = intck('DAY',quarter_start,curr_date) / intck('DAY',quarter_start,quarter_end)\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = f.with_columns(\n",
    "            call_freq_goal_prc = pl.when(pl.col('call_freq_quarter').is_null()).then(None\n",
    "            ).otherwise(pl.col('total_calls')/(pl.col('call_freq_quarter') * formula_helper_1))\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c89463f5-303f-4645-a30c-22770f235dda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# adding upper and lower limits : (use prc_reach)\n",
    "# flow -\n",
    "## for terr level -\n",
    "# three cols , ind1,ind2,ind3\n",
    "# for ind1 ->nation , ind2->area , ind3->region\n",
    "# to create an upper limit and lower limit \n",
    "# upper : meadian + 0.5*stddev\n",
    "# lower : median - 0.5*stddev\n",
    "#bench_ind : if var > upper then A | if var < lower then B | if lower <= var <= upper then E\n",
    "\n",
    "def process_reach_benchmark(df): # WORKING CORRECTLY BUT NOT MODULAR , PLEASE UPDATE STRUCTURE \n",
    "    def add_indicator(df, ind_name, col1, col2, col3):\n",
    "        return df.with_columns(\n",
    "            pl.when(pl.col(col1) > pl.col(col2))\n",
    "            .then(pl.lit('A'))\n",
    "            .when(pl.col(col1) < pl.col(col3))\n",
    "            .then(pl.lit('B'))\n",
    "            .when((pl.col(col3) < pl.col(col1)) & (pl.col(col1) < pl.col(col2)))\n",
    "            .then(pl.lit('E'))\n",
    "            .otherwise(None)  # You can replace 'N/A' with any default value\n",
    "            .alias(ind_name)\n",
    "        )\n",
    "    #Terr\n",
    "    f = df[0]\n",
    "    nf = f.select([levels[0],p,sg,spc,d,'prc_reach'])\n",
    "    nf = nf.join(geo_code_mapper,on = levels[0],how = 'left')\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        nll = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        all = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf_r = nf.group_by([levels[1],p,sg,spc,d]).agg(\n",
    "        rul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        rll = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "        .join(nf_r,on=[levels[1],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[1],levels[2],levels[3])\n",
    "\n",
    "\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind1', 'prc_reach', 'nul', 'nll')\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind2', 'prc_reach', 'aul', 'all')\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind3', 'prc_reach', 'rul', 'rll').drop(['nul','nll','aul','all','rul','rll','prc_reach'])\n",
    "\n",
    "    f = f.join(nf,on=[levels[0],p, sg, spc, d],how = 'left')\n",
    "    df[0] = f\n",
    "    #Region\n",
    "    f = df[1]\n",
    "    nf = f.select([levels[1],p,sg,spc,d,'prc_reach']).join(\n",
    "        geo_code_mapper[['region_geography_id','area_geography_id']].unique(),on = levels[1],how = 'left'\n",
    "    )\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        nll = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        all = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[2],levels[3])\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind1', 'prc_reach', 'nul', 'nll')\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind2', 'prc_reach', 'aul', 'all')\n",
    "    nf = nf.with_columns(pl.lit(None).alias('Reach_Prc_BnchMrk_Ind3')).drop(['nul','nll','aul','all','prc_reach'])\n",
    "    f = f.join(nf,on=[levels[1],p, sg, spc, d],how = 'left')\n",
    "    df[1] = f\n",
    "    #Area\n",
    "    f = df[2]\n",
    "    nf = f.select([levels[2],p,sg,spc,d,'prc_reach'])\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        nll = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf = (nf.join(nf_n, on=[p, sg, spc, d], how='left'))\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind1', 'prc_reach', 'nul', 'nll')\n",
    "    nf = nf.with_columns(pl.lit(None).alias('Reach_Prc_BnchMrk_Ind2'),pl.lit(None).alias('Reach_Prc_BnchMrk_Ind3')).drop(['nul','nll','prc_reach'])\n",
    "    f = f.join(nf,on=[levels[2],p, sg, spc, d],how = 'left')\n",
    "    df[2] = f\n",
    "    #Nation \n",
    "    f = df[3]\n",
    "    f = f.with_columns(pl.lit(None).alias('Reach_Prc_BnchMrk_Ind1'),pl.lit(None).alias('Reach_Prc_BnchMrk_Ind2'),pl.lit(None).alias('Reach_Prc_BnchMrk_Ind3'))\n",
    "    df[3] = f\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95dd1ee5-2743-4d5d-b77b-822ae07e342c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Prc_Of_Optimal_And_Above\n",
    "def process_8(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = f.with_columns(perc_opt_above = pl.col('optimal')/(pl.col('optimal')+pl.col('below')))\n",
    "        df[i] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a40949b-7fe5-4374-9d5d-7f5bb2408132",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c6cf742-b7de-46bd-9c3a-529d48b7778e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Convert To Feed Ready data\n",
    "def get_feed(temp1):\n",
    "    # Get FEED PROTOYPING ->\n",
    "    temp1[0] = temp1[0].rename({'geography_id': 'Geography_id'})\n",
    "    temp1[1] = temp1[1].rename({'region_geography_id': 'Geography_id'}).select(temp1[0].columns)\n",
    "    temp1[2] = temp1[2].rename({'area_geography_id': 'Geography_id'}).select(temp1[0].columns)\n",
    "    temp1[3] = temp1[3].rename({'nation_geography_id': 'Geography_id'}).select(temp1[0].columns)\n",
    "    \n",
    "    final_feed = temp1[0].vstack(temp1[1]).vstack(temp1[2]).vstack(temp1[3])\n",
    "    final_feed = final_feed.drop([\n",
    "        'num_hcps','call_freq_quarter','total_calls',\n",
    "        'total_samples_LIN','total_samples_LI1','total_samples_LI2','total_samples_LI3',\n",
    "        'wk_qtd_LIN','wk_qtd_LI1','wk_qtd_LI2','wk_qtd_LI3'\n",
    "    ])\n",
    "    \n",
    "    #fix for product_id\n",
    "    pm = prod_mapping.with_columns(pl.lit('LIN').alias('product')).select(['product','product_id'])\n",
    "    final_feed2 = final_feed.join(pm,on='product',how='left').drop('product')\n",
    "    \n",
    "    # Split into two parts ->\n",
    "    final_feed2_LIN = final_feed2.filter(~pl.col('product_id').is_in([3,4,5])) # For Product Code 2 and other products  \n",
    "    final_feed2_LIP = final_feed2.filter(pl.col('product_id').is_in([3,4,5])) # For Sub Products\n",
    "    \n",
    "    final_feed2_LIN = (\n",
    "        final_feed2_LIN\n",
    "        .drop([\n",
    "            'num_samples_LI1','num_samples_LI2','num_samples_LI3',\n",
    "            'num_sample_hcps_LI1','num_sample_hcps_LI2','num_sample_hcps_LI3',\n",
    "            'rx_per_call_LI1','rx_per_call_LI2','rx_per_call_LI3',\n",
    "            'prc_sampled_phy_LI1','prc_sampled_phy_LI2','prc_sampled_phy_LI3',\n",
    "            'rx_per_sample_LI1','rx_per_sample_LI2','rx_per_sample_LI3',\n",
    "            'avg_sample_per_hcp_LI1','avg_sample_per_hcp_LI2','avg_sample_per_hcp_LI3'\n",
    "        ])\n",
    "        .rename({\n",
    "            'prc_sampled_phy_LIN' : 'prc_sampled_phy',\n",
    "            'rx_per_sample_LIN' : 'rx_per_sample',\n",
    "            'avg_sample_per_hcp_LIN' : 'avg_sample_per_hcp'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    column_parings = {\n",
    "        'num_samples' : ['num_samples_LI3','num_samples_LI1','num_samples_LI2'],\n",
    "        'num_sample_hcps' : ['num_sample_hcps_LI3','num_sample_hcps_LI1','num_sample_hcps_LI2'],\n",
    "        'rx_per_call' : ['rx_per_call_LI3','rx_per_call_LI1','rx_per_call_LI2'],\n",
    "        'prc_sampled_phy_LIN' : ['prc_sampled_phy_LI3','prc_sampled_phy_LI1','prc_sampled_phy_LI2'],\n",
    "        'rx_per_sample_LIN' : ['rx_per_sample_LI3','rx_per_sample_LI1','rx_per_sample_LI2'],\n",
    "        'avg_sample_per_hcp_LIN' : ['avg_sample_per_hcp_LI3','avg_sample_per_hcp_LI1','avg_sample_per_hcp_LI2']\n",
    "    }\n",
    "    \n",
    "    sub_product_copy_logic = []\n",
    "    for k,v in column_parings.items():\n",
    "        expn = (\n",
    "            pl.when(pl.col('product_id')==3).then(pl.col(v[0]))\n",
    "            .when(pl.col('product_id')==4).then(pl.col(v[1]))\n",
    "            .when(pl.col('product_id')==5).then(pl.col(v[2]))\n",
    "            .otherwise(pl.col(k)).alias(k)\n",
    "        )\n",
    "        sub_product_copy_logic.append(expn)\n",
    "    \n",
    "    final_feed2_LIP = (\n",
    "        final_feed2_LIP\n",
    "        .with_columns(*sub_product_copy_logic)\n",
    "        .drop([\n",
    "            'num_samples_LI1','num_samples_LI2','num_samples_LI3',\n",
    "            'num_sample_hcps_LI1','num_sample_hcps_LI2','num_sample_hcps_LI3',\n",
    "            'rx_per_call_LI1','rx_per_call_LI2','rx_per_call_LI3',\n",
    "            'prc_sampled_phy_LI1','prc_sampled_phy_LI2','prc_sampled_phy_LI3',\n",
    "            'rx_per_sample_LI1','rx_per_sample_LI2','rx_per_sample_LI3',\n",
    "            'avg_sample_per_hcp_LI1','avg_sample_per_hcp_LI2','avg_sample_per_hcp_LI3'\n",
    "        ])\n",
    "        .rename({\n",
    "            'prc_sampled_phy_LIN' : 'prc_sampled_phy',\n",
    "            'rx_per_sample_LIN' : 'rx_per_sample',\n",
    "            'avg_sample_per_hcp_LIN' : 'avg_sample_per_hcp'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # Joining Back Together - >\n",
    "    final_feed3 = final_feed2_LIN.vstack(final_feed2_LIP)\n",
    "    \n",
    "    #renaming columns \n",
    "    col_mapping = {\n",
    "        'product_id':'Product_id',\n",
    "        'segment':'Segment',\n",
    "        'decile':'Decile',\n",
    "        'specialty_group':'Specialty',\n",
    "        'num_calls':'Num_Of_Calls',\n",
    "        'days_in_field':'Days_In_Field',\n",
    "        'num_abbv_calls':'ABBV_Visits',\n",
    "        'target_hcps':'Num_Of_Targets',\n",
    "        'num_sample_hcps':'Num_Of_Sampled_Physicians',\n",
    "        'num_samples':'Total_Samples',\n",
    "        'tgts3':'Num_Of_QTD_Tgts_3Plus_Calls',\n",
    "        'below':'Below',\n",
    "        'optimal':'Optimal',\n",
    "        'call_distribution':'Call_Distribution',\n",
    "        'calls_per_day':'Calls_Per_Day',\n",
    "        'call_freq':'Frequency',\n",
    "        'prc_reach':'Prc_Reach',\n",
    "        'tgt_reach_13wk':'Thirteen_Week_Tgt_Reach',\n",
    "        'qtd_tgt_nreach':'QTD_Tgts_Not_Reached',\n",
    "        'called_12m':'Num_Of_Called_Months_12M',\n",
    "        'rx_per_call':'Rx_Per_Call',\n",
    "        'sample_distribution':'Sample_Distribution',\n",
    "        'prc_sampled_phy':'Prc_Of_Sampled_Physicians',\n",
    "        'avg_sample_per_hcp':'Avg_Samples_Per_HCP',\n",
    "        'rx_per_sample':'Rx_Per_Sample',\n",
    "        'call_freq_goal_prc':'Call_Freq_Goal_Prc',\n",
    "        'perc_opt_above':'Prc_Of_Optimal_And_Above'\n",
    "    }\n",
    "    final_feed = final_feed3.rename(col_mapping)\n",
    "    \n",
    "    # required columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addp = ['Period']\n",
    "    col_to_adds = ['No_Call','Above']\n",
    "    col_to_addna = ['Prc_Of_Surveyed_HCPs','Called_1_Time', 'Called_2_Times', 'Called_3_Times', \n",
    "                    'Called_4_Times', 'Called_5_Times', 'Called_6_Times', 'Total_Num_Of_Called_2_Times', \n",
    "                    'Prc_Of_Called_2_Times','Pharmacy_Calls_Per_Day', 'Num_Of_Calls2', 'Calls1', 'Calls2', \n",
    "                    'Calls3', 'Calls4', 'Calls5', 'Calls6', 'Calls7', 'Calls8', 'Calls9', 'Calls10', 'Calls11', \n",
    "                    'Calls12', 'Calls13', 'Calls14', 'Calls15', 'Calls16', 'Calls17', 'Calls18', 'Calls19', 'Calls20']\n",
    "                    \n",
    "    # func to add columns with desired value\n",
    "    def addcol(df,columns_to_add,wtl):\n",
    "        for my_col in columns_to_add:\n",
    "            df = df.with_columns(pl.lit(wtl).alias(my_col))\n",
    "        return df\n",
    "    \n",
    "    final_feed = addcol(final_feed,col_to_addrt,'WEEKLY')\n",
    "    final_feed = addcol(final_feed,col_to_addp,f'{period_num}-WEEK')\n",
    "    final_feed = addcol(final_feed,col_to_adds,'.')\n",
    "    final_feed = addcol(final_feed,col_to_addna,'\\\\N')\n",
    "    \n",
    "    # changing value of column to match with sas - 06/21\n",
    "    final_feed = final_feed.with_columns(\n",
    "        pl.when(pl.col('Segment')=='ALG-ONLY-TARGET')\n",
    "        .then(pl.lit('AGNT'))\n",
    "        .when(pl.col('Segment')=='Target')\n",
    "        .then(pl.lit('T'))\n",
    "        .when(pl.col('Segment')=='Non-Target')\n",
    "        .then(pl.lit('NT'))\n",
    "        .otherwise(pl.col('Segment'))\n",
    "        .alias('Segment'))\n",
    "    \n",
    "    # arranging columns according to feed\n",
    "    req_col = ['Geography_id', 'Product_id', 'Segment', 'Specialty', 'ReportType', 'Period', 'Decile', \n",
    "               'Call_Distribution', 'Num_Of_Targets', 'Num_Of_Calls', 'Calls_Per_Day', 'Call_Freq_Goal_Prc',\n",
    "                'Prc_Reach', 'Reach_Prc_BnchMrk_Ind1', 'Reach_Prc_BnchMrk_Ind2', 'Reach_Prc_BnchMrk_Ind3', \n",
    "                'Frequency', 'Rx_Per_Call', 'Days_In_Field', 'No_Call', 'Below', 'Optimal', 'Above', \n",
    "                'Prc_Of_Optimal_And_Above', 'Sample_Distribution', 'Prc_Of_Sampled_Physicians', \n",
    "                'Total_Samples', 'Avg_Samples_Per_HCP', 'Rx_Per_Sample', 'Prc_Of_Surveyed_HCPs', \n",
    "                'Called_1_Time', 'Called_2_Times', 'Called_3_Times', 'Called_4_Times', 'Called_5_Times',\n",
    "                'Called_6_Times', 'Total_Num_Of_Called_2_Times', 'Prc_Of_Called_2_Times', \n",
    "                'Num_Of_Sampled_Physicians', 'Pharmacy_Calls_Per_Day', 'Num_Of_Calls2', \n",
    "                'Calls1', 'Calls2', 'Calls3', 'Calls4', 'Calls5', 'Calls6', 'Calls7', 'Calls8', \n",
    "                'Calls9', 'Calls10', 'Calls11', 'Calls12', 'Calls13', 'Calls14', 'Calls15', 'Calls16',\n",
    "                'Calls17', 'Calls18', 'Calls19', 'Calls20', 'ABBV_Visits', 'Num_Of_Called_Months_12M',\n",
    "                'Thirteen_Week_Tgt_Reach', 'QTD_Tgts_Not_Reached', 'Num_Of_QTD_Tgts_3Plus_Calls']\n",
    "    final_feed = final_feed.select(req_col)\n",
    "    \n",
    "    #Overrides : \n",
    "    final_feed = final_feed.with_columns(\n",
    "        pl.when(pl.col('Segment')=='NT').then(pl.lit('\\\\N')).otherwise(pl.col('Num_Of_Targets')).alias('Num_Of_Targets')\n",
    "    )\n",
    "    \n",
    "    columns_to_round1 = ['Num_Of_Called_Months_12M','Rx_Per_Call']\n",
    "    columns_to_round10 = ['Thirteen_Week_Tgt_Reach','Calls_Per_Day','Call_Freq_Goal_Prc','Prc_Reach','Frequency','Days_In_Field',]\n",
    "    \n",
    "    final_feed = final_feed.with_columns([\n",
    "        *[pl.col(col).round(1).alias(col) for col in columns_to_round1],\n",
    "        *[pl.col(col).round(10).alias(col) for col in columns_to_round10],\n",
    "    ])\n",
    "    \n",
    "    return(final_feed)\n",
    "               #-----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87c8e1e1-b9b2-4408-a78a-8e6034a5843d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for trvializing formula : \n",
    "p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/GeoSummary/Weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "655ae435-9e25-4957-a5df-0d8692067dcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported : 1\n",
      "Exported : 2\n",
      "Exported : 3\n",
      "Exported : 4\n",
      "Exported : 5\n"
     ]
    }
   ],
   "source": [
    "# Calling all functions and Exporting Feeds\n",
    "for period_num,PN in zip([1,4,13,26,'qtd'],[1,2,3,4,5]):\n",
    "    # if PN > 1:\n",
    "    #     break\n",
    "    \n",
    "    temp1 = [pl.DataFrame() for _ in range(4)] # creating an empty dataframe holder list obj\n",
    "    temp1 = process_1(temp1)\n",
    "    temp1 = process_2(temp1)\n",
    "    temp1 = process_3(temp1)\n",
    "    temp1 = process_3_1(temp1)\n",
    "    temp1 = process_4(temp1)\n",
    "    temp1 = process_5(temp1)\n",
    "    temp1 = process_6(temp1)\n",
    "    temp1 = process_6_1(temp1)\n",
    "    temp1 = process_7(temp1)\n",
    "    temp1 = process_8(temp1)\n",
    "    temp1 = process_reach_benchmark(temp1)\n",
    "    feed_dataset = get_feed(temp1)\n",
    "     #===================================================\n",
    "    feed_dataset = feed_dataset.to_pandas()\n",
    "    # Select columns of type 'object' (string)\n",
    "    string_columns = feed_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "    feed_dataset[string_columns] = feed_dataset[string_columns].fillna('\\\\N')\n",
    "    feed_dataset = feed_dataset.replace('NaN', '\\\\N')\n",
    "\n",
    "    feed_dataset = feed_dataset.replace([np.nan, np.inf, -np.inf], '\\\\N')\n",
    "    feed_dataset.to_csv(f'{OUT}Weekly_GeoSummary_SalesActivity_P{PN}_Feed.txt', sep='|', lineterminator='\\r\\n',index=False)\n",
    "    print('Exported :',PN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626f864-982b-459b-8804-50d737e003d8",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
