{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GS Trend Feed pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:46.327103Z",
     "iopub.status.busy": "2024-10-04T09:47:46.326589Z",
     "iopub.status.idle": "2024-10-04T09:47:46.838270Z",
     "shell.execute_reply": "2024-10-04T09:47:46.837345Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime, timedelta,date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:46.842558Z",
     "iopub.status.busy": "2024-10-04T09:47:46.842093Z",
     "iopub.status.idle": "2024-10-04T09:47:46.847383Z",
     "shell.execute_reply": "2024-10-04T09:47:46.846445Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:46.851207Z",
     "iopub.status.busy": "2024-10-04T09:47:46.850793Z",
     "iopub.status.idle": "2024-10-04T09:47:46.854919Z",
     "shell.execute_reply": "2024-10-04T09:47:46.854181Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:46.858957Z",
     "iopub.status.busy": "2024-10-04T09:47:46.858245Z",
     "iopub.status.idle": "2024-10-04T09:47:47.994775Z",
     "shell.execute_reply": "2024-10-04T09:47:47.993835Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL'] # only these products are to be read from lax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:47.999385Z",
     "iopub.status.busy": "2024-10-04T09:47:47.998404Z",
     "iopub.status.idle": "2024-10-04T09:47:48.008298Z",
     "shell.execute_reply": "2024-10-04T09:47:48.007217Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal -\n",
    "def get_lin_voucher_13vols():    \n",
    "    vch = pl.read_parquet(f'{xpn}LIN_VOUCHER.parquet') \n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod,prod2 in zip(['LI1','LI2','LI3'],['LIN1','LIN2','LIN3']):\n",
    "        rename_dict = dict(zip([f'{prod2}TUF{i}' for i in range(1,14)],[f'vVol{i}_TUF' for i in range(1,14)]))\n",
    "        vch_prod = (\n",
    "            vch.select(['IID'] + [f'{prod2}TUF{i}' for i in range(1,14)])\n",
    "            .rename(rename_dict)\n",
    "            .with_columns(pl.lit(prod).alias('PROD_CD'))\n",
    "        )\n",
    "        if prod == 'LI1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "    vch1 = vch1.fill_null(0)\n",
    "    \n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:48.014061Z",
     "iopub.status.busy": "2024-10-04T09:47:48.013524Z",
     "iopub.status.idle": "2024-10-04T09:47:48.026396Z",
     "shell.execute_reply": "2024-10-04T09:47:48.024454Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_volumes_geos(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,14)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "    rename_dict = dict(zip(columns[2:],['Vol'+str(i)+'_'+metric for i in range(1,14)]))\n",
    "    df = df.rename(rename_dict)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null()\n",
    "    )\n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher_13vols()\n",
    "        df = (\n",
    "            df\n",
    "            .join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "            .with_columns([(pl.col(f'Vol{i}_TUF') - pl.col(f'vVol{i}_TUF')).alias(f'Vol{i}_TUF') for i in range(1,14)])\n",
    "            .drop(dfv.columns[1:-1])\n",
    "        )\n",
    "\n",
    "    volume_cols = [f'Vol{i}_{metric}' for i in range(1,14)]\n",
    "    agg_dict = {col: pl.col(col).sum() for col in volume_cols}\n",
    "\n",
    "    df_terr = df.group_by(['geography_id','specialty_group','segment','decile','PROD_CD']).agg(**agg_dict)\n",
    "\n",
    "    df_reg = df.join(geo_code_mapper[['geography_id','region_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['region_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**agg_dict)\n",
    "\n",
    "    df_area = df.join(geo_code_mapper[['geography_id','area_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['area_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**agg_dict)\n",
    "\n",
    "    df_nation = df.join(geo_code_mapper[['geography_id','nation_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['nation_geography_id','specialty_group','segment','decile','PROD_CD']).agg(**agg_dict)\n",
    "\n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:48.036719Z",
     "iopub.status.busy": "2024-10-04T09:47:48.033141Z",
     "iopub.status.idle": "2024-10-04T09:47:48.047780Z",
     "shell.execute_reply": "2024-10-04T09:47:48.046925Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows(all_prod_df):\n",
    "    # converting tuple to list , because i cant assign the processed df back to it\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    for i in range(4): \n",
    "        df = all_prod_df[i]\n",
    "        agg_dict = {}\n",
    "        for col in df.columns[5:]:\n",
    "            agg_dict[col] = pl.col(col).sum()\n",
    "        \n",
    "        join_cols = df.columns[0:4]\n",
    "\n",
    "        df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "        df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "        df_2_35 = df_2_35.group_by(join_cols + ['parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "        df_1 = df.group_by(join_cols).agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "        # stack 1, 2_35 with df and return\n",
    "        df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "        vstack_helper = df.columns\n",
    "        df = df.vstack(\n",
    "            df_2_35.select(vstack_helper)\n",
    "        ).vstack(\n",
    "            df_1.select(vstack_helper)\n",
    "        )\n",
    "\n",
    "        all_prod_df[i] = df\n",
    "    return(tuple(all_prod_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:48.056139Z",
     "iopub.status.busy": "2024-10-04T09:47:48.055415Z",
     "iopub.status.idle": "2024-10-04T09:47:48.072911Z",
     "shell.execute_reply": "2024-10-04T09:47:48.071971Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_full_rollups(all_prod_df):\n",
    "    # converting the tuple of dfs into a list for processing\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    # for trivializing formulas - \n",
    "    p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    \n",
    "    #Looping over 4 levels (terr,reg,area,nation)\n",
    "    for i in range(4):\n",
    "        df = all_prod_df[i]\n",
    "        g = df.columns[0] #should contain geo level\n",
    "        metric_cols = df.columns[4:-1] #should contain the tuf / nuf columns\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols) #used for vstack later\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        #### Processing Done ####\n",
    "        df = (\n",
    "            df.select(main_seq)\n",
    "            .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "            .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "            .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        # Store Data Back :\n",
    "        all_prod_df[i] = df\n",
    "    \n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:48.079109Z",
     "iopub.status.busy": "2024-10-04T09:47:48.078102Z",
     "iopub.status.idle": "2024-10-04T09:47:50.963766Z",
     "shell.execute_reply": "2024-10-04T09:47:50.963000Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data Prep - ETA 5 Seconds\n",
    "all_products_tuf = get_volumes_geos('TUF',fetch_products)\n",
    "all_products_tuf = add_parent_product_rows(all_products_tuf)\n",
    "all_products_tuf = add_full_rollups(all_products_tuf)\n",
    "\n",
    "all_products_nuf = get_volumes_geos('NUF',fetch_products)\n",
    "all_products_nuf = add_parent_product_rows(all_products_nuf)\n",
    "all_products_nuf = add_full_rollups(all_products_nuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:50.967413Z",
     "iopub.status.busy": "2024-10-04T09:47:50.966873Z",
     "iopub.status.idle": "2024-10-04T09:47:50.974157Z",
     "shell.execute_reply": "2024-10-04T09:47:50.973295Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_1(df):\n",
    "    for i in range(4):\n",
    "        g = levels[i]\n",
    "        gb_helper = [g,spc,sg,d,p]\n",
    "        f = (\n",
    "            all_products_tuf[i]\n",
    "            .join(all_products_nuf[i],on = gb_helper,how = 'left')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\t\n",
    "def process_2(df):\n",
    "    for j in range(4): #changing this from i to j because of a local var conflict\n",
    "        f = df[j]\n",
    "\n",
    "        rename_dict = {}\n",
    "        expn_dict = {}\n",
    "        for i in range(1,14):\n",
    "            for metric in ['TUF','NUF']:\n",
    "                rename_dict[f'Vol{i}_{metric}'] = f'lax_Vol{i}_{metric}'\n",
    "                expn_dict[f'Shr{i}_{metric}'] = pl.col(f'Vol{i}_{metric}')/pl.col(f'lax_Vol{i}_{metric}')\n",
    "\n",
    "        f_1 = f.filter(pl.col(p)==1).rename(rename_dict).drop(p)\n",
    "        f = (\n",
    "            f\n",
    "            .join(f_1,on=[levels[j],spc,sg,d],how='left')\n",
    "            .with_columns(**expn_dict)\n",
    "            .drop(list(rename_dict.values()))\n",
    "        )\n",
    "\n",
    "        df[j] = f\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:50.979679Z",
     "iopub.status.busy": "2024-10-04T09:47:50.979387Z",
     "iopub.status.idle": "2024-10-04T09:47:50.992247Z",
     "shell.execute_reply": "2024-10-04T09:47:50.991535Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Converting To feed ready Data\n",
    "def get_feed(temp1):\n",
    "    temp1[0] = temp1[0].rename({'geography_id': 'Geography_id'})\n",
    "    temp1[1] = temp1[1].rename({'region_geography_id': 'Geography_id'})\n",
    "    temp1[2] = temp1[2].rename({'area_geography_id': 'Geography_id'})\n",
    "    temp1[3] = temp1[3].rename({'nation_geography_id': 'Geography_id'})\n",
    "    final_feed = temp1[0].vstack(temp1[1]).vstack(temp1[2]).vstack(temp1[3])\n",
    "    #function to diving dataframe in two levels('Trx','Nrx')\n",
    "    def select_columns_by_condition(df,metric):\n",
    "        # Get the column names to be excluded based on the condition\n",
    "        excluded_columns = [col for col in df.columns if not col.endswith(metric)]\n",
    "        \n",
    "        # Select all columns except the excluded ones\n",
    "        selected_df = df.select(excluded_columns)\n",
    "        return selected_df\n",
    "    final_feed_nrx = select_columns_by_condition(final_feed,'TUF')# dataframe not including trx columns\n",
    "    final_feed_nrx = final_feed_nrx.with_columns(pl.lit('NRX').alias(\"Metric\"))\n",
    "    final_feed_trx = select_columns_by_condition(final_feed,'NUF')\n",
    "    final_feed_trx = final_feed_trx.with_columns(pl.lit('TRX').alias(\"Metric\"))\n",
    "    #function to remove _trx or _nrx from final_feed_nrx and final_feed_trx\n",
    "    def rename_columns_by_condition(df,metric):\n",
    "        renamed_columns = {col: col[:-4] if col.endswith(metric) else col for col in df.columns}\n",
    "        renamed_df = df.rename(renamed_columns)\n",
    "        return renamed_df\n",
    "    # making trx feed columns and nrx feed columns similar so that we can vstack them\n",
    "    final_feed_nrx = rename_columns_by_condition(final_feed_nrx,'NUF')\n",
    "    final_feed_trx = rename_columns_by_condition(final_feed_trx,'TUF')\n",
    "    final_feed = final_feed_trx.vstack(final_feed_nrx)\n",
    "    #required new columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addna = [\n",
    "        \"Pres1\", \"Pres2\", \"Pres3\", \"Pres4\", \"Pres5\", \"Pres6\", \"Pres7\", \"Pres8\", \"Pres9\", \"Pres10\", \"Pres11\", \"Pres12\", \"Pres13\", \n",
    "        \"DS1_Vol1\", \"DS1_Vol2\", \"DS1_Vol3\", \"DS1_Vol4\", \"DS1_Vol5\", \"DS1_Vol6\", \"DS1_Vol7\", \"DS1_Vol8\", \"DS1_Vol9\", \"DS1_Vol10\", \n",
    "        \"DS1_Vol11\", \"DS1_Vol12\", \"DS1_Vol13\", \"DS2_Vol1\", \"DS2_Vol2\", \"DS2_Vol3\", \"DS2_Vol4\", \"DS2_Vol5\", \"DS2_Vol6\", \"DS2_Vol7\",\n",
    "          \"DS2_Vol8\", \"DS2_Vol9\", \"DS2_Vol10\", \"DS2_Vol11\", \"DS2_Vol12\", \"DS2_Vol13\"\n",
    "    \n",
    "    ]\n",
    "    for my_col in col_to_addna:\n",
    "            final_feed = final_feed.with_columns(pl.lit('\\\\N').alias(my_col))\n",
    "      \n",
    "    final_feed = final_feed.with_columns(pl.lit('WEEKLY').alias('ReportType'))\n",
    "    #Renaming columns\n",
    "    new_col_mapping = {\n",
    "        'product_id': 'Product_id',\n",
    "        'segment': 'Segment',\n",
    "        'specialty_group': 'Specialty',\n",
    "        'decile': 'Decile'\n",
    "    }\n",
    "    final_feed = final_feed.rename(new_col_mapping)\n",
    "\n",
    "    # changing value of column to match with sas - 06/21\n",
    "    final_feed = final_feed.with_columns(\n",
    "        pl.when(pl.col('Segment')=='ALG-ONLY-TARGET')\n",
    "        .then(pl.lit('AGNT'))\n",
    "        .when(pl.col('Segment')=='Target')\n",
    "        .then(pl.lit('T'))\n",
    "        .when(pl.col('Segment')=='Non-Target')\n",
    "        .then(pl.lit('NT'))\n",
    "        .otherwise(pl.col('Segment'))\n",
    "        .alias('Segment'))\n",
    "\n",
    "    # rearranging columns accoring to feed.\n",
    "    req_cols = [\n",
    "        'Geography_id', 'Product_id', 'Segment', 'Specialty', 'Metric', 'ReportType', 'Decile', 'Vol1', 'Vol2', 'Vol3', 'Vol4', 'Vol5', \n",
    "        'Vol6', 'Vol7', 'Vol8', 'Vol9', 'Vol10', 'Vol11', 'Vol12', 'Vol13', 'Shr1', 'Shr2', 'Shr3', 'Shr4', 'Shr5', 'Shr6', 'Shr7', \n",
    "        'Shr8', 'Shr9', 'Shr10', 'Shr11', 'Shr12', 'Shr13', 'Pres1', 'Pres2', 'Pres3', 'Pres4', 'Pres5', 'Pres6', 'Pres7', 'Pres8', \n",
    "        'Pres9', 'Pres10', 'Pres11', 'Pres12', 'Pres13', 'DS1_Vol1', 'DS1_Vol2', 'DS1_Vol3', 'DS1_Vol4', 'DS1_Vol5', 'DS1_Vol6', 'DS1_Vol7', \n",
    "        'DS1_Vol8', 'DS1_Vol9', 'DS1_Vol10', 'DS1_Vol11', 'DS1_Vol12', 'DS1_Vol13', 'DS2_Vol1', 'DS2_Vol2', 'DS2_Vol3', 'DS2_Vol4', 'DS2_Vol5', \n",
    "        'DS2_Vol6', 'DS2_Vol7', 'DS2_Vol8', 'DS2_Vol9', 'DS2_Vol10', 'DS2_Vol11', 'DS2_Vol12', 'DS2_Vol13'\n",
    "    ]\n",
    "    final_feed = final_feed.select(req_cols)\n",
    "    \n",
    "    return(final_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:50.996297Z",
     "iopub.status.busy": "2024-10-04T09:47:50.995778Z",
     "iopub.status.idle": "2024-10-04T09:47:51.000001Z",
     "shell.execute_reply": "2024-10-04T09:47:50.999068Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for trvializing formula : \n",
    "p,sg,spc,d = 'product_id','segment','specialty_group','decile'\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/GeoSummary/Weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:47:51.003091Z",
     "iopub.status.busy": "2024-10-04T09:47:51.002778Z",
     "iopub.status.idle": "2024-10-04T09:48:08.655514Z",
     "shell.execute_reply": "2024-10-04T09:48:08.654755Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Trend Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "# Calling Functions and Exporting Feeds-\n",
    "temp1 = [pl.DataFrame() for _ in range(4)] # creating an empty dataframe holder list obj\n",
    "temp1 = process_1(temp1)\n",
    "temp1 = process_2(temp1)\n",
    "feed_dataset = get_feed(temp1)\n",
    "#===================================================\n",
    "feed_dataset = feed_dataset.to_pandas()\n",
    "# Select columns of type 'object' (string)\n",
    "string_columns = feed_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "feed_dataset[string_columns] = feed_dataset[string_columns].fillna('\\\\N')\n",
    "feed_dataset = feed_dataset.replace('NaN', '\\\\N')\n",
    "feed_dataset = feed_dataset.replace([np.nan, np.inf, -np.inf], ['\\\\N','\\\\N','\\\\N'])\n",
    "feed_dataset.to_csv(f'{OUT}Weekly_GeoSummary_Trend_Feed.txt', sep='|', lineterminator='\\r\\n',index=False)\n",
    "print('GS Trend Feed Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geosummary X Feed -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:48:08.730763Z",
     "iopub.status.busy": "2024-10-04T09:48:08.729886Z",
     "iopub.status.idle": "2024-10-04T09:48:08.793600Z",
     "shell.execute_reply": "2024-10-04T09:48:08.792818Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS X Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "rx_date = datetime.strptime(data_date,'%Y%m%d')\n",
    "list_of_dates = [rx_date]\n",
    "serial_no = [i for i in range(1,14)]\n",
    "for i in range(1,13):\n",
    "    date_val = rx_date - timedelta(days = 7*i)\n",
    "    list_of_dates.append(date_val)\n",
    "\n",
    "\n",
    "date_df = pl.DataFrame(\n",
    "    {\n",
    "        'X':serial_no,\n",
    "        'Name':list_of_dates\n",
    "    }\n",
    ")\n",
    "\n",
    "date_df = date_df.with_columns(\n",
    "   date_df['Name'].dt.strftime('%m/%d/%Y')\n",
    ")\n",
    "\n",
    "date_df.to_pandas().to_csv(f'{OUT}Weekly_GeoSummary_X_Feed.txt', sep='|',lineterminator='\\r\\n',index=False)\n",
    "print('GS X Feed Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
