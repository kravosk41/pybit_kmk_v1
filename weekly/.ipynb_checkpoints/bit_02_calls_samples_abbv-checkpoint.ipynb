{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calls and samples & abbv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime, timedelta,date\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "qtr_data = js['qtr_data']\n",
    "qtr_ntnw = js['qtr_ntnw']\n",
    "fir_nqrt = datetime.strptime(js['fir_nqrt'],'%Y-%m-%d').date()\n",
    "targeting_folder = js['targeting_folder']\n",
    "working_day_file = js['working_day_file']\n",
    "roster_file = js['roster_file']\n",
    "curr_date = datetime.strptime(js['curr_date'], '%Y-%m-%d').date()\n",
    "quarter_start = datetime.strptime(js['quarter_start'], '%Y-%m-%d').date()\n",
    "num_weeks_calls = js['num_weeks_calls']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "curr_date_p26 = curr_date- timedelta(weeks=26)\n",
    "curr_date_p13 = curr_date- timedelta(weeks=13) \n",
    "curr_date_m13 = curr_date - timedelta(weeks=53)\n",
    "\n",
    "bucket = js['bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frzmstr = f's3://{bucket}/PYADM/quaterly/{qtr_data}/reference/'\n",
    "master = f's3://{bucket}/PYADM/weekly/archive/{data_date}/reference/'\n",
    "inex = f's3://{bucket}/PYADM/reference/{qtr_data}/'\n",
    "geo = f's3://{bucket}/PYADM/quaterly/{qtr_data}/geography/'\n",
    "lincall = f's3://{bucket}/PYADM/quaterly/{qtr_data}/target/post/'\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "call = f's3://{bucket}/PYADM/weekly/archive/{data_date}/calls_samples/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intck(interval, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calculates the difference between two dates in terms of the specified interval.\n",
    "\n",
    "    Args:\n",
    "        interval (str): The interval ('DAY', 'MONTH', 'WEEK', etc.).\n",
    "        start_date (datetime.date): The start date.\n",
    "        end_date (datetime.date): The end date.\n",
    "\n",
    "    Returns:\n",
    "        int: The difference between the dates in terms of the specified interval.\n",
    "    \"\"\"\n",
    "    if interval == 'DAY':\n",
    "        return (end_date - start_date).days\n",
    "    elif interval == 'MONTH':\n",
    "        end_date_m = end_date.replace(day=1)\n",
    "        start_date_m = start_date.replace(day = 1)\n",
    "        rd = relativedelta(end_date_m, start_date_m)\n",
    "        return rd.years * 12 + rd.months\n",
    "    elif interval == 'WEEK':\n",
    "        return (end_date - start_date).days // 7\n",
    "    # Add more intervals as needed\n",
    "\n",
    "# Example usage\n",
    "# start_date = date(2023, 1, 1)\n",
    "# end_date = date(2023, 2, 15)\n",
    "# interval = 'DAY'\n",
    "\n",
    "# result = intck(interval, start_date, end_date)\n",
    "# print(f\"Difference in {interval}: {result}\")\n",
    "    \n",
    "def filter_duplicate(df,col,val,new):\n",
    "    dict = {val:new}\n",
    "    filtered_df = df.filter(pl.col(col)==val)\n",
    "    filtered_df = filtered_df.with_columns([\n",
    "        pl.col(col).map_elements(lambda x : dict.get(x,x)).cast(pl.Utf8)\n",
    "    ])\n",
    "    return (df.vstack(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Calls-\n",
    "\n",
    "# Specifiing which columns to keep-\n",
    "read_cols = ['CallID','CallProductDescription','CallDateTime','SalesRepIID','AttendeeIID','CallProductQuantity',\n",
    "             'CALL_VARNM','SalesRepTerritoryID','PhysicianTerritoryID','CallType']\n",
    "\n",
    "# Reading the file from ADM - \n",
    "AC = pl.read_parquet(f'{call}ALL_CALLS.parquet',columns=read_cols)\n",
    "\n",
    "# Adding CallDate column - \n",
    "AC = AC.with_columns(pl.col(\"CallDateTime\").cast(pl.Date).alias('CallDate'))\n",
    "AC = AC.drop(\"CallDateTime\") # dropping redundant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering for just current \n",
    "active_calls = AC.filter(\n",
    "    (pl.col('CallDate') >= curr_date_m13) & (pl.col('CallDate') <= curr_date)\n",
    ") #used to be pr_13_wk_date\n",
    "active_calls = active_calls.with_columns(pl.col('CALL_VARNM').str.slice(3,3).alias('product'))\n",
    "active_calls = active_calls.with_columns(pl.col('CALL_VARNM').str.slice(0,3).alias('source'))\n",
    "active_calls = active_calls.with_columns(pl.col('CALL_VARNM').str.slice(7,2).alias('type'))\n",
    "active_calls = active_calls.with_columns(pl.col('CALL_VARNM').str.slice(6,1).alias('priority'))\n",
    "\n",
    "active_calls = active_calls.drop('CALL_VARNM')\n",
    "active_calls = active_calls.with_columns(pl.col('AttendeeIID').cast(pl.Int64))\n",
    "\n",
    "del AC\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### temp_calls\n",
    "- this should containt all non frx and non sample only rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/*Eliminating all calls made by FRX and Sample Only calls*/\n",
    "temp_calls = active_calls.filter(\n",
    "    (pl.col('source').is_in(['EGM', 'IRN', 'REG', 'RFT', 'ROT', 'RTE', 'RWE', 'RZO'])) & ~\n",
    "    (pl.col('type').is_in(['SO']))\n",
    ")\n",
    "\n",
    "# Adding week number and month number - \n",
    "temp_calls = temp_calls.with_columns(\n",
    "    pl.col('CallDate')\n",
    "    .map_elements(\n",
    "        lambda x : (intck('DAY',x,curr_date) // 7) + 1,return_dtype=pl.Int64\n",
    "    )\n",
    "    .alias('call_week')\n",
    ")\n",
    "\n",
    "temp_calls = temp_calls.with_columns(\n",
    "    pl.col('CallDate')\n",
    "    .map_elements(\n",
    "        lambda x : intck('MONTH',x,curr_date) + 1,return_dtype=pl.Int64\n",
    "    )\n",
    "    .alias('call_month')\n",
    ")\n",
    "\n",
    "temp_calls = temp_calls.filter(pl.col('call_month')<=13) # only have 13 months of data\n",
    "temp_calls.to_pandas().to_parquet(dflib+'temp_calls.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### temp_samples\n",
    "- this should contain all rows with CallType = 'Group Detail with Sample' or 'Detail with Sample'\n",
    "- source should still be non FRX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_CallProductDescriptions = [\"72 MCG\",\"145 MCG\",\"145 MCG SAMPLE 30CT\",\"290 MCG\",\"290 MCG SAMPLE 30CT\",\"CANASA\",\"DELZICOL\"]\n",
    "sample_CallTypes = ['Group Detail with Sample','Detail with Sample']\n",
    "in_active_samples = active_calls.filter(\n",
    "    (pl.col('CallProductDescription').str.to_uppercase().is_in(sample_CallProductDescriptions)) & (pl.col('CallType').is_in(sample_CallTypes))\n",
    ")\n",
    "temp_samples = in_active_samples.filter(\n",
    "    pl.col('source').is_in(['EGM', 'IRN', 'REG', 'RFT', 'ROT', 'RTE', 'RWE', 'RZO'])\n",
    ")\n",
    "# Adding week number and month number - \n",
    "temp_samples = temp_samples.with_columns(\n",
    "    pl.col('CallDate')\n",
    "    .map_elements(\n",
    "        lambda x : (intck('DAY',x,curr_date) // 7) + 1,return_dtype=pl.Int64\n",
    "    )\n",
    "    .alias('sample_week')\n",
    ")\n",
    "\n",
    "temp_samples = temp_samples.with_columns(\n",
    "    pl.col('CallDate')\n",
    "    .map_elements(\n",
    "        lambda x : intck('MONTH',x,curr_date) + 1,return_dtype=pl.Int64\n",
    "    )\n",
    "    .alias('sample_month')\n",
    ")\n",
    "\n",
    "temp_samples = temp_samples.filter(pl.col('sample_month')<=13) # only have 13 months of data\n",
    "temp_samples.to_pandas().to_parquet(dflib+'temp_samples.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### temp_abbv\n",
    "- this should contain all FRX and AEM rows with non Sample Only Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_abbv = active_calls.filter(\n",
    "    (pl.col('source').is_in(['FRX','AEM'])) & \n",
    "    (pl.col('type') != 'SO')\n",
    ")\n",
    "# Adding week number and month number - \n",
    "temp_abbv = temp_abbv.with_columns(\n",
    "    pl.col('CallDate')\n",
    "    .map_elements(\n",
    "        lambda x : (intck('DAY',x,curr_date) // 7) + 1,return_dtype=pl.Int64\n",
    "    )\n",
    "    .alias('call_week')\n",
    ")\n",
    "\n",
    "temp_abbv = temp_abbv.with_columns(\n",
    "    pl.col('CallDate')\n",
    "    .map_elements(\n",
    "        lambda x : intck('MONTH',x,curr_date) + 1,return_dtype=pl.Int64\n",
    "    )\n",
    "    .alias('call_month')\n",
    ")\n",
    "\n",
    "temp_abbv = temp_abbv.filter(pl.col('call_month')<=13) # only have 13 months of data\n",
    "temp_abbv.to_pandas().to_parquet(dflib+'temp_abbv.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working Day File\n",
    "- Used to get number of working days (aggregated at rep_id level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing working day file \n",
    "wd_raw = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/working_day/Working Day Data for KMK_{working_day_file}.xlsx'))\n",
    "wd_raw.columns = ['rep_name','rep_id','day','wd']\n",
    "wd_raw = (\n",
    "    wd_raw\n",
    "    .with_columns(pl.col('day').cast(pl.Date))\n",
    "    .filter((pl.col('day') >= quarter_start) & (pl.col('day') <= curr_date))\n",
    "    .group_by(['rep_name','rep_id'])\n",
    "    .agg(days_in_field=pl.col('wd').sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>rep_name</th><th>rep_id</th><th>days_in_field</th></tr><tr><td>str</td><td>i64</td><td>f64</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 3)\n",
       "┌──────────┬────────┬───────────────┐\n",
       "│ rep_name ┆ rep_id ┆ days_in_field │\n",
       "│ ---      ┆ ---    ┆ ---           │\n",
       "│ str      ┆ i64    ┆ f64           │\n",
       "╞══════════╪════════╪═══════════════╡\n",
       "└──────────┴────────┴───────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# roster\n",
    "roster = pl.read_parquet(\n",
    "    f's3://{bucket}/BIT/roster/MasterRoster_{roster_file}.parquet',columns = ['EmpCode','SalesRepIID']\n",
    ")\n",
    "roster = roster.with_columns(pl.col('EmpCode').cast(pl.Int64))\n",
    "roster = roster.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "\n",
    "#\n",
    "wd_raw = wd_raw.join(roster,left_on= 'rep_id',right_on = 'EmpCode' ,how = 'left')\n",
    "wd_raw.to_pandas().to_parquet(dflib+'wd_raw.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### call plan file import \n",
    "- importing and exporting ironwood call plan (we use the call_freq column here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lirwd_call_plan = pl.read_parquet(lincall + 'IRWD_CALL_PLAN.parquet',\n",
    "                                  columns=['IID','CALL_FREQ'])\n",
    "lirwd_call_plan.columns = ['IID','call_freq_quarter']\n",
    "lirwd_call_plan.to_pandas().to_parquet(dflib+'lirwd_call_plan.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Cleaning up to make space for RX import\n",
    "del wd_raw\n",
    "del lirwd_call_plan\n",
    "del temp_abbv\n",
    "del temp_calls\n",
    "del temp_samples\n",
    "del active_calls\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing mp so that we can pull in geo_id and roll up rx data\n",
    "mp_spec_seg_dec = pl.read_parquet(dflib+'mp_spec_seg_dec.parquet')\n",
    "\n",
    "#load xpn-\n",
    "laxdn = pl.read_parquet(xpn+'LAX_DN.parquet',columns=['IID'] + [f'LINFTUF{i}' for i in range(1,num_weeks_rx+1)]\n",
    ").with_columns(wk_qtd = pl.sum_horizontal([f'LINFTUF{i}' for i in range(1,num_weeks_rx+1)])\n",
    ").join(mp_spec_seg_dec[['IID','geography_id']],on = 'IID', how = 'left'\n",
    ").group_by('geography_id').agg(wk_qtd = pl.col('wk_qtd').sum())\n",
    "\n",
    "# load voucher-\n",
    "vch = pl.read_parquet(xpn+'LIN_VOUCHER.parquet',columns=['IID'] + [f'LINVTUF{i}' for i in range(1,num_weeks_rx+1)]\n",
    ").with_columns(vwk_qtd = pl.sum_horizontal([f'LINVTUF{i}' for i in range(1,num_weeks_rx+1)])\n",
    ").join(mp_spec_seg_dec[['IID','geography_id']],on = 'IID', how = 'left'\n",
    ").group_by('geography_id').agg(vwk_qtd = pl.col('vwk_qtd').sum())\n",
    "\n",
    "laxdn = laxdn.join(vch,on='geography_id',how='left').with_columns(\n",
    "    wk_qtd = pl.col('wk_qtd') - pl.col('vwk_qtd')\n",
    ").drop('vwk_qtd')\n",
    "\n",
    "laxdn.to_pandas().to_parquet(dflib+'laxdn_geoid_sum.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
