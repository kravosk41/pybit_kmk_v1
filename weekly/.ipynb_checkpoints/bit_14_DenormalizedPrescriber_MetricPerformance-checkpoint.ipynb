{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenormalizedPrescriber MetricPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "data_date = js['data_date']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "load('mp_spec_seg_dec')\n",
    "load('MASTER_UNI')\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal - \n",
    "def get_lin_voucher():\n",
    "    vch = pl.read_parquet(f'{xpn}LIN_VOUCHER.parquet') # n_rows=500\n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod in ['LIN1','LIN2','LIN3']: # LINV\n",
    "        vch_prod = (\n",
    "            vch.select(\n",
    "                pl.col('IID'),\n",
    "                pl.col(f'{prod}TUF1').alias(f'vTUF_1c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,5)]).alias(f'vTUF_4c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,14)]).alias(f'vTUF_13c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,27)]).alias(f'vTUF_26c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,num_weeks_rx+1)]).alias(f'vTUF_qtdc'),\n",
    "                pl.col(f'{prod}TUF2').alias(f'vTUF_1p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(5,9)]).alias(f'vTUF_4p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(14,27)]).alias(f'vTUF_13p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(27,53)]).alias(f'vTUF_26p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(14,14+num_weeks_rx)]).alias(f'vTUF_qtdp'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,106)]).alias(f'vTUF_all')\n",
    "            )\n",
    "            .with_columns(pl.lit(f'LI{prod[-1]}').alias('PROD_CD'))\n",
    "        )\n",
    "        if prod[-1] == '1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "\n",
    "    # voucher_mapping = {'LI1': 4, 'LI2': 5, 'LI3': 3, 'LIV': 2}\n",
    "    # vch1 = vch1.with_columns(pl.col('PROD_CD').replace(voucher_mapping,return_dtype=pl.Int64).alias('product_id')).fill_null(0)#.drop('PROD_CD')\n",
    "    vch1 = vch1.fill_null(0)\n",
    "\n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_period_iid(prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + ['TUF'+str(i) for i in range(1,27)] + ['NUF'+str(i) for i in range(1,27)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 4,13 for current and prior period for TUF and NUF\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.sum_horizontal(['TUF'+str(i) for i in range(1,5)]).alias('TUF'+'_4c'),\n",
    "        pl.sum_horizontal(['TUF'+str(i) for i in range(1,14)]).alias('TUF'+'_13c'),\n",
    "        pl.sum_horizontal(['TUF'+str(i) for i in range(5,9)]).alias('TUF'+'_4p'),\n",
    "        pl.sum_horizontal(['TUF'+str(i) for i in range(14,27)]).alias('TUF'+'_13p'),\n",
    "\n",
    "        pl.sum_horizontal(['NUF'+str(i) for i in range(1,5)]).alias('NUF'+'_4c'),\n",
    "        pl.sum_horizontal(['NUF'+str(i) for i in range(1,14)]).alias('NUF'+'_13c'),\n",
    "        pl.sum_horizontal(['NUF'+str(i) for i in range(5,9)]).alias('NUF'+'_4p'),\n",
    "        pl.sum_horizontal(['NUF'+str(i) for i in range(14,27)]).alias('NUF'+'_13p'),\n",
    "    )\n",
    "\n",
    "    # For removing Voucher -\n",
    "    dfv = get_lin_voucher()\n",
    "    df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "    cols_to_remove = dfv.columns[1:-1]\n",
    "    df = df.with_columns(\n",
    "        pl.col(f'TUF_4c') -  pl.col(f'vTUF_4c').alias(f'TUF_4c'),\n",
    "        pl.col(f'TUF_13c') -  pl.col(f'vTUF_13c').alias(f'TUF_13c'),\n",
    "        pl.col(f'TUF_4p') -  pl.col(f'vTUF_4p').alias(f'TUF_4p'),\n",
    "        pl.col(f'TUF_13p') -  pl.col(f'vTUF_13p').alias(f'TUF_13p')\n",
    "    ).drop(cols_to_remove)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    return(df.drop(['specialty_group','segment','decile','geography_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data Prep -\n",
    "all_products_tuf_nuf = get_summed_period_iid(fetch_products).join(\n",
    "    prod_mapping[['code','product_id']],left_on = 'PROD_CD',right_on='code',how='left'\n",
    ").drop('PROD_CD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lax_tuf_nuf = all_products_tuf_nuf.group_by('IID').agg(\n",
    "    TUF_4c_LAX = pl.col('TUF_4c').sum(),TUF_13c_LAX = pl.col('TUF_13c').sum(),\n",
    "    TUF_4p_LAX = pl.col('TUF_4p').sum(),TUF_13p_LAX = pl.col('TUF_13p').sum(),\n",
    "    NUF_4c_LAX = pl.col('NUF_4c').sum(),NUF_13c_LAX = pl.col('NUF_13c').sum(),\n",
    "    NUF_4p_LAX = pl.col('NUF_4p').sum(),NUF_13p_LAX = pl.col('NUF_13p').sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_linzess_strength_columns(df,prod_id_list,prod):\n",
    "    df = df.join(\n",
    "        all_products_tuf_nuf.filter(pl.col('product_id').is_in(prod_id_list)),\n",
    "        on = 'IID', how = 'left'\n",
    "    )\n",
    "    df = df.group_by('IID').agg(\n",
    "        TUF_4c_prod = pl.col('TUF_4c').sum(),TUF_13c_prod = pl.col('TUF_13c').sum(),\n",
    "        TUF_4p_prod = pl.col('TUF_4p').sum(),TUF_13p_prod = pl.col('TUF_13p').sum(),\n",
    "        NUF_4c_prod = pl.col('NUF_4c').sum(),NUF_13c_prod = pl.col('NUF_13c').sum(),\n",
    "        NUF_4p_prod = pl.col('NUF_4p').sum(),NUF_13p_prod = pl.col('NUF_13p').sum()\n",
    "    )\n",
    "    df = df.join(\n",
    "        lax_tuf_nuf,on = 'IID',how = 'left'\n",
    "    )\n",
    "    expn_dict = {\n",
    "        f'{prod}_13_trx_cur_vol' : pl.col('TUF_13c_prod'),\n",
    "        f'{prod}_13_trx_pri_vol' : pl.col('TUF_13p_prod'),\n",
    "        f'{prod}_13_trx_vol_change' : (pl.col('TUF_13c_prod')-pl.col('TUF_13p_prod')),\n",
    "        f'{prod}_13_trx_share' : (pl.col('TUF_13c_prod')/pl.col('TUF_13c_LAX')),\n",
    "\n",
    "        f'{prod}_13_nrx_cur_vol' : pl.col('NUF_13c_prod'),\n",
    "        f'{prod}_13_nrx_pri_vol' : pl.col('NUF_13p_prod'),\n",
    "        f'{prod}_13_nrx_vol_change' : (pl.col('NUF_13c_prod')-pl.col('NUF_13p_prod')),\n",
    "        f'{prod}_13_nrx_share' : (pl.col('NUF_13c_prod')/pl.col('NUF_13c_LAX')),\n",
    "\n",
    "        f'{prod}_4_trx_cur_vol' : pl.col('TUF_4c_prod'),\n",
    "        f'{prod}_4_trx_pri_vol' : pl.col('TUF_4p_prod'),\n",
    "        f'{prod}_4_trx_vol_change' : (pl.col('TUF_4c_prod')-pl.col('TUF_4p_prod')),\n",
    "        f'{prod}_4_trx_share' : (pl.col('TUF_4c_prod')/pl.col('TUF_4c_LAX')),\n",
    "\n",
    "        f'{prod}_4_nrx_cur_vol' : pl.col('NUF_4c_prod'),\n",
    "        f'{prod}_4_nrx_pri_vol' : pl.col('NUF_4p_prod'),\n",
    "        f'{prod}_4_nrx_vol_change' : (pl.col('NUF_4c_prod')-pl.col('NUF_4p_prod')),\n",
    "        f'{prod}_4_nrx_share' : (pl.col('NUF_4c_prod')/pl.col('NUF_4c_LAX')),\n",
    "    }\n",
    "    # adding columns now :\n",
    "    df2 = df.with_columns(**expn_dict).select(['IID']+list(expn_dict.keys()))\n",
    "\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when((pl.col(f'{prod}_13_trx_vol_change')/pl.col(f'{prod}_13_trx_pri_vol')) > 0.02).then(pl.lit('P'))\n",
    "        .when((pl.col(f'{prod}_13_trx_vol_change')/pl.col(f'{prod}_13_trx_pri_vol')) < -0.02).then(pl.lit('Q'))\n",
    "        .when(pl.col(f'{prod}_13_trx_vol_change')==0).then(pl.lit('//N'))\n",
    "        .otherwise(pl.lit('//N')).alias(f'{prod}_13_vol_change_ind_trx'),\n",
    "        \n",
    "        pl.when((pl.col(f'{prod}_13_nrx_vol_change')/pl.col(f'{prod}_13_nrx_pri_vol')) > 0.02).then(pl.lit('P'))\n",
    "        .when((pl.col(f'{prod}_13_nrx_vol_change')/pl.col(f'{prod}_13_nrx_pri_vol')) < -0.02).then(pl.lit('Q'))\n",
    "        .when(pl.col(f'{prod}_13_nrx_vol_change')==0).then(pl.lit('//N'))\n",
    "        .otherwise(pl.lit('//N')).alias(f'{prod}_13_vol_change_ind_nrx'),\n",
    "\n",
    "        pl.when((pl.col(f'{prod}_4_trx_vol_change')/pl.col(f'{prod}_4_trx_pri_vol')) > 0.02).then(pl.lit('P'))\n",
    "        .when((pl.col(f'{prod}_4_trx_vol_change')/pl.col(f'{prod}_4_trx_pri_vol')) < -0.02).then(pl.lit('Q'))\n",
    "        .when(pl.col(f'{prod}_4_trx_vol_change')==0).then(pl.lit('//N'))\n",
    "        .otherwise(pl.lit('//N')).alias(f'{prod}_4_vol_change_ind_trx'),\n",
    "        \n",
    "        pl.when((pl.col(f'{prod}_4_nrx_vol_change')/pl.col(f'{prod}_4_nrx_pri_vol')) > 0.02).then(pl.lit('P'))\n",
    "        .when((pl.col(f'{prod}_4_nrx_vol_change')/pl.col(f'{prod}_4_nrx_pri_vol')) < -0.02).then(pl.lit('Q'))\n",
    "        .when(pl.col(f'{prod}_4_nrx_vol_change')==0).then(pl.lit('//N'))\n",
    "        .otherwise(pl.lit('//N')).alias(f'{prod}_4_vol_change_ind_nrx')\n",
    "    )\n",
    "\n",
    "    #dropping shr columns if not whole family\n",
    "    if prod != 'LIN':\n",
    "        df2 = df2.drop([col for col in df2.columns if 'share' in col])\n",
    "\n",
    "    return(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Processing\n",
    "temp1 = mp_spec_seg_dec.select(['IID','geography_id'])\n",
    "\n",
    "LIN_cols = get_linzess_strength_columns(temp1,[3,4,5],'LIN')\n",
    "LI1_cols = get_linzess_strength_columns(temp1,[3],'LI1')\n",
    "LI2_cols = get_linzess_strength_columns(temp1,[4],'LI2')\n",
    "LI3_cols = get_linzess_strength_columns(temp1,[5],'LI3')\n",
    "\n",
    "# join all of them back to temp1\n",
    "temp1 = temp1.join(LIN_cols,on='IID',how='left'\n",
    ").join(LI1_cols,on='IID',how='left'\n",
    ").join(LI2_cols,on='IID',how='left'\n",
    ").join(LI3_cols,on='IID',how='left'\n",
    ").with_columns(Product_id = pl.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for filtering extra obs\n",
    "check_cols = []\n",
    "for col in temp1.columns:\n",
    "    if ((col.startswith('LIN')) & ('ind' not in col) & ('share' not in col) & ('change' not in col)):\n",
    "        check_cols.append(col)\n",
    "temp1 = (\n",
    "    temp1\n",
    "    .with_columns(\n",
    "        qc=pl.sum_horizontal(check_cols)\n",
    "    )\n",
    "    .filter(\n",
    "        (pl.col('qc') != 0)\n",
    "    )\n",
    "    .drop(['qc'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PDRP Overrides - \n",
    "temp1 = temp1.join(MASTER_UNI.select(['IID','PDRPOptOutFlag']),on='IID',how='left')\n",
    "override_columns =  temp1.columns[2:-2]\n",
    "expression_list = [\n",
    "    pl.when(pl.col('PDRPOptOutFlag')=='Y').then(pl.lit('\\\\N')).otherwise(pl.col(c)).alias(c)\n",
    "    for c in override_columns\n",
    "]\n",
    "temp1 = temp1.with_columns(expression_list).drop('PDRPOptOutFlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Making Data Feed Ready-\n",
    "\n",
    "col_mapping = {\n",
    "    'IID':'Physician_ID',\n",
    "    'geography_id':'Geography_id',\n",
    "    'LIN_13_trx_cur_vol':'NumberMetric1',\n",
    "    'LIN_13_trx_pri_vol':'NumberMetric2',\n",
    "    'LIN_13_trx_vol_change':'NumberMetric3',\n",
    "    'LIN_13_trx_share':'NumberMetric4',\n",
    "    'LIN_13_nrx_cur_vol':'NumberMetric6',\n",
    "    'LIN_13_nrx_pri_vol':'NumberMetric7',\n",
    "    'LIN_13_nrx_vol_change':'NumberMetric8',\n",
    "    'LIN_13_nrx_share':'NumberMetric9',\n",
    "    'LIN_4_trx_cur_vol':'NumberMetric11',\n",
    "    'LIN_4_trx_pri_vol':'NumberMetric12',\n",
    "    'LIN_4_trx_vol_change':'NumberMetric13',\n",
    "    'LIN_4_trx_share':'NumberMetric14',\n",
    "    'LIN_4_nrx_cur_vol':'NumberMetric16',\n",
    "    'LIN_4_nrx_pri_vol':'NumberMetric17',\n",
    "    'LIN_4_nrx_vol_change':'NumberMetric18',\n",
    "    'LIN_4_nrx_share':'NumberMetric19',\n",
    "    'LI1_13_trx_cur_vol':'NumberMetric21',\n",
    "    'LI1_13_trx_pri_vol':'NumberMetric22',\n",
    "    'LI1_13_trx_vol_change':'NumberMetric23',\n",
    "    'LI1_13_nrx_cur_vol':'NumberMetric24',\n",
    "    'LI1_13_nrx_pri_vol':'NumberMetric25',\n",
    "    'LI1_13_nrx_vol_change':'NumberMetric26',\n",
    "    'LI1_4_trx_cur_vol':'NumberMetric27',\n",
    "    'LI1_4_trx_pri_vol':'NumberMetric28',\n",
    "    'LI1_4_trx_vol_change':'NumberMetric29',\n",
    "    'LI1_4_nrx_cur_vol':'NumberMetric30',\n",
    "    'LI1_4_nrx_pri_vol':'NumberMetric31',\n",
    "    'LI1_4_nrx_vol_change':'NumberMetric32',\n",
    "    'LI2_13_trx_cur_vol':'NumberMetric81',\n",
    "    'LI2_13_trx_pri_vol':'NumberMetric82',\n",
    "    'LI2_13_trx_vol_change':'NumberMetric83',\n",
    "    'LI2_13_nrx_cur_vol':'NumberMetric84',\n",
    "    'LI2_13_nrx_pri_vol':'NumberMetric85',\n",
    "    'LI2_13_nrx_vol_change':'NumberMetric86',\n",
    "    'LI2_4_trx_cur_vol':'NumberMetric87',\n",
    "    'LI2_4_trx_pri_vol':'NumberMetric88',\n",
    "    'LI2_4_trx_vol_change':'NumberMetric89',\n",
    "    'LI2_4_nrx_cur_vol':'NumberMetric90',\n",
    "    'LI2_4_nrx_pri_vol':'NumberMetric91',\n",
    "    'LI2_4_nrx_vol_change':'NumberMetric92',\n",
    "    'LI3_13_trx_cur_vol':'NumberMetric69',\n",
    "    'LI3_13_trx_pri_vol':'NumberMetric70',\n",
    "    'LI3_13_trx_vol_change':'NumberMetric71',\n",
    "    'LI3_13_nrx_cur_vol':'NumberMetric72',\n",
    "    'LI3_13_nrx_pri_vol':'NumberMetric73',\n",
    "    'LI3_13_nrx_vol_change':'NumberMetric74',\n",
    "    'LI3_4_trx_cur_vol':'NumberMetric75',\n",
    "    'LI3_4_trx_pri_vol':'NumberMetric76',\n",
    "    'LI3_4_trx_vol_change':'NumberMetric77',\n",
    "    'LI3_4_nrx_cur_vol':'NumberMetric78',\n",
    "    'LI3_4_nrx_pri_vol':'NumberMetric79',\n",
    "    'LI3_4_nrx_vol_change':'NumberMetric80',\n",
    "    'LIN_13_vol_change_ind_trx':'StringMetric1',\n",
    "    'LIN_13_vol_change_ind_nrx':'StringMetric2',\n",
    "    'LIN_4_vol_change_ind_trx':'StringMetric3',\n",
    "    'LIN_4_vol_change_ind_nrx':'StringMetric4',\n",
    "    'LI1_13_vol_change_ind_trx':'StringMetric5',\n",
    "    'LI1_13_vol_change_ind_nrx':'StringMetric6',\n",
    "    'LI1_4_vol_change_ind_trx':'StringMetric7',\n",
    "    'LI1_4_vol_change_ind_nrx':'StringMetric8',\n",
    "    'LI2_13_vol_change_ind_trx':'StringMetric25',\n",
    "    'LI2_13_vol_change_ind_nrx':'StringMetric26',\n",
    "    'LI2_4_vol_change_ind_trx':'StringMetric27',\n",
    "    'LI2_4_vol_change_ind_nrx':'StringMetric28',\n",
    "    'LI3_13_vol_change_ind_trx':'StringMetric21',\n",
    "    'LI3_13_vol_change_ind_nrx':'StringMetric22',\n",
    "    'LI3_4_vol_change_ind_trx':'StringMetric23',\n",
    "    'LI3_4_vol_change_ind_nrx':'StringMetric24'\n",
    "}\n",
    "final_feed = temp1.rename(col_mapping)\n",
    "#required new columns for feed\n",
    "cols_to_addna = [\n",
    "    'NumberMetric5', 'NumberMetric10', 'NumberMetric15', 'NumberMetric20',\n",
    "    'NumberMetric33', 'NumberMetric34', 'NumberMetric35', 'NumberMetric36',\n",
    "    'NumberMetric37', 'NumberMetric38', 'NumberMetric39', 'NumberMetric40',\n",
    "    'NumberMetric41', 'NumberMetric42', 'NumberMetric43', 'NumberMetric44',\n",
    "    'NumberMetric45', 'NumberMetric46', 'NumberMetric47', 'NumberMetric48',\n",
    "    'NumberMetric49', 'NumberMetric50', 'NumberMetric51', 'NumberMetric52',\n",
    "    'NumberMetric53', 'NumberMetric54', 'NumberMetric55', 'NumberMetric56',\n",
    "    'NumberMetric57', 'NumberMetric58', 'NumberMetric59', 'NumberMetric60',\n",
    "    'NumberMetric61', 'NumberMetric62', 'NumberMetric63', 'NumberMetric64',\n",
    "    'NumberMetric65', 'NumberMetric66', 'NumberMetric67','NumberMetric68',\n",
    "    'StringMetric9', 'StringMetric10','StringMetric11','StringMetric12',\n",
    "    'StringMetric13', 'StringMetric14', 'StringMetric15', 'StringMetric16', \n",
    "    'StringMetric17', 'StringMetric18', 'StringMetric19', 'StringMetric20',\n",
    "    \n",
    "\n",
    "]\n",
    "for my_col in cols_to_addna:\n",
    "        final_feed = final_feed.with_columns(pl.lit('\\\\N').alias(my_col))\n",
    "\n",
    "# rearranging columns accoring to feed.\n",
    "req_cols = variable_names = [\n",
    "    \"Physician_ID\", \"Geography_id\", \"Product_id\"] + ['NumberMetric' + str(i) for i in range(1,93)] + ['StringMetric' + str(i) for i in range(1,29)]\n",
    "final_feed = final_feed.select(req_cols)#final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denorm Presc Metric Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "#Exporting Feeds-\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/DenormalizedPrescriber/Weekly/'\n",
    "final_feed.to_pandas().to_csv(f'{OUT}Weekly_DenormalizedPrescriber_MetricPerformance_Feed.txt',sep='|',lineterminator='\\r\\n',index=False)\n",
    "print('Denorm Presc Metric Feed Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
