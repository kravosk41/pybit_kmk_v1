{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prescriber View - Sales Activity pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta,date\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "num_weeks_calls = js['num_weeks_calls']\n",
    "curr_date = datetime.strptime(js['curr_date'], '%Y-%m-%d').date()\n",
    "num_of_months = js['num_of_months']\n",
    "bucket = js['bucket']\n",
    "quarter_start = datetime.strptime(js['quarter_start'], '%Y-%m-%d').date()\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'\n",
    "call = f's3://{bucket}/PYADM/weekly/archive/{data_date}/calls_samples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('temp_calls')\n",
    "load('mp_spec_seg_dec')\n",
    "load('lirwd_call_plan')\n",
    "load('temp_samples')\n",
    "load('temp_abbv')\n",
    "load('roster')\n",
    "load('MASTER_UNI')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL']\n",
    "\n",
    "#fixes for vortex import -> Probably caused by Polars Upgrades\n",
    "temp_calls = temp_calls.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_samples = temp_samples.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_abbv = temp_abbv.with_columns(pl.col('SalesRepIID').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal - \n",
    "def get_lin_voucher():\n",
    "    vch = pl.read_parquet(f'{xpn}LIN_VOUCHER.parquet') # n_rows=500\n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod in ['LIN1','LIN2','LIN3']: # LINV\n",
    "        vch_prod = (\n",
    "            vch.select(\n",
    "                pl.col('IID'),\n",
    "                pl.col(f'{prod}TUF1').alias(f'vTUF_1c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,5)]).alias(f'vTUF_4c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,14)]).alias(f'vTUF_13c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,27)]).alias(f'vTUF_26c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,num_weeks_rx+1)]).alias(f'vTUF_qtdc'),\n",
    "                pl.col(f'{prod}TUF2').alias(f'vTUF_1p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(5,9)]).alias(f'vTUF_4p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(14,27)]).alias(f'vTUF_13p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(27,53)]).alias(f'vTUF_26p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(14,14+num_weeks_rx)]).alias(f'vTUF_qtdp'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,106)]).alias(f'vTUF_all')\n",
    "            )\n",
    "            .with_columns(pl.lit(f'LI{prod[-1]}').alias('PROD_CD'))\n",
    "        )\n",
    "        if prod[-1] == '1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "\n",
    "    # voucher_mapping = {'LI1': 4, 'LI2': 5, 'LI3': 3, 'LIV': 2}\n",
    "    # vch1 = vch1.with_columns(pl.col('PROD_CD').replace(voucher_mapping,return_dtype=pl.Int64).alias('product_id')).fill_null(0)#.drop('PROD_CD')\n",
    "    vch1 = vch1.fill_null(0)\n",
    "\n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_period_iid_metric(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,106)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,5)]).alias(metric+'_4c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,14)]).alias(metric+'_13c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,27)]).alias(metric+'_26c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,num_weeks_rx+1)]).alias(metric+'_qtdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(5,9)]).alias(metric+'_4p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,27)]).alias(metric+'_13p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(27,53)]).alias(metric+'_26p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,14+num_weeks_rx)]).alias(metric+'_qtdp'),\n",
    "\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,106)]).alias(metric+'_all')\n",
    "    )\n",
    "    # For Voucher Removal - \n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_4c') -  pl.col(f'v{metric}_4c').alias(f'{metric}_4c'),\n",
    "            pl.col(f'{metric}_13c') -  pl.col(f'v{metric}_13c').alias(f'{metric}_13c'),\n",
    "            pl.col(f'{metric}_26c') -  pl.col(f'v{metric}_26c').alias(f'{metric}_26c'),\n",
    "            pl.col(f'{metric}_qtdc') -  pl.col(f'v{metric}_qtdc').alias(f'{metric}_qtdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_4p') -  pl.col(f'v{metric}_4p').alias(f'{metric}_4p'),\n",
    "            pl.col(f'{metric}_13p') -  pl.col(f'v{metric}_13p').alias(f'{metric}_13p'),\n",
    "            pl.col(f'{metric}_26p') -  pl.col(f'v{metric}_26p').alias(f'{metric}_26p'),\n",
    "            pl.col(f'{metric}_qtdp') -  pl.col(f'v{metric}_qtdp').alias(f'{metric}_qtdp'),\n",
    "            pl.col(f'{metric}_all') -  pl.col(f'v{metric}_all').alias(f'{metric}_all')\n",
    "        ).drop(cols_to_remove)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    return(df.drop(['specialty_group','segment','decile','geography_id']))\n",
    "\n",
    "all_products_tuf = get_summed_period_iid_metric('TUF',fetch_products)\n",
    "all_products_nuf = get_summed_period_iid_metric('NUF',fetch_products)\n",
    "\n",
    "def add_parent_product_rows(df):\n",
    "    agg_dict = {}\n",
    "    for col in df.columns[2:]:\n",
    "        agg_dict[col] = pl.col(col).sum()\n",
    "    \n",
    "    #join_cols = ['geography_id','plan_type','PlanID','IID']\n",
    "\n",
    "    df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    \n",
    "    df_1 = df.group_by('IID').agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)\n",
    "\n",
    "all_products_tuf = add_parent_product_rows(all_products_tuf)\n",
    "all_products_nuf = add_parent_product_rows(all_products_nuf)\n",
    "\n",
    "# tuf1 = all_products_tuf.filter(pl.col('TUF_all')!=0).select(['IID','product_id'])\n",
    "# nuf1 = all_products_nuf.filter(pl.col('NUF_all')!=0).select(['IID','product_id'])\n",
    "# xponent = tuf1.join(nuf1,on=['IID','product_id'],how='outer_coalesce')\n",
    "\n",
    "calls = (\n",
    "    temp_calls.filter(pl.col('call_week')<= num_weeks_calls)\n",
    "    .filter(pl.col('CallDate')>= quarter_start)\n",
    "    .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "    .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "    .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    .rename({'AttendeeIID':'IID'})\n",
    "    .select('IID').unique('IID')\n",
    "    .with_columns(product_id = pl.lit(2)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    ")\n",
    "\n",
    "# xponent_calls = xponent.join(calls,on=['IID','product_id'],how='outer_coalesce').filter(~pl.col('product_id').is_in([2,3,4,5]))\n",
    "\n",
    "#delete extra dfs when optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# KPI Indicator\n",
    "def process_kpi_ind(df):\n",
    "\n",
    "    geo_id_full = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "    load('wd_raw')\n",
    "    temp_calls_mp_spec = (\n",
    "        temp_calls\n",
    "        .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "        .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "        .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "        .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    "    )\n",
    "    \n",
    "    source_df = (\n",
    "        temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)).filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    \n",
    "    num_calls_iid_month = (\n",
    "        source_df\n",
    "        .group_by(['AttendeeIID','call_month'])\n",
    "        .agg(num_calls = pl.col('CallID').n_unique())\n",
    "        .filter(pl.col('call_month')<=num_of_months).rename({'AttendeeIID':'IID'})\n",
    "        .group_by('IID').agg(pl.col('call_month').n_unique().alias('num_calls'))\n",
    "    )\n",
    "    \n",
    "    # num_calls_iid_month = temp_calls.group_by(['AttendeeIID','call_month']).agg(\n",
    "    #     num_calls = pl.col('CallID').n_unique()\n",
    "    # ).filter(pl.col('call_month')<=num_of_months).rename({'AttendeeIID':'IID'}\n",
    "    # ).group_by('IID').agg(pl.col('call_month').n_unique().alias('num_calls')) #this was sum before- and suming num_calls\n",
    "    # First Group by calls data by IID and call_month and get num of calls for each month\n",
    "    # then filter that dataset to only have rows for the num of months calls variable\n",
    "    # then group by and sum num_calls just on IID\n",
    "    # end result will be dataset with num_calls for num_of_month worth of rows at IID level\n",
    "\n",
    "    df = df.join(num_calls_iid_month,on = 'IID',how = 'left')\n",
    "\n",
    "    if num_of_months == 0:\n",
    "        return (df.with_columns(kpi_ind = pl.lit('\\\\N')))\n",
    "    elif num_of_months == 1:\n",
    "        result = df.with_columns(\n",
    "            pl.when(pl.col('num_calls')>=1).then(pl.lit('OPTIMAL')).otherwise(pl.lit('BELOW')).alias('kpi_ind')\n",
    "        ).drop('num_calls') #dropping pulled columns\n",
    "    elif num_of_months == 2:\n",
    "        result = df.with_columns(\n",
    "            pl.when(pl.col('num_calls')>=2).then(pl.lit('OPTIMAL')).otherwise(pl.lit('BELOW')).alias('kpi_ind')\n",
    "        ).drop('num_calls') \n",
    "    elif num_of_months == 3:\n",
    "        result = df.with_columns(\n",
    "            pl.when(pl.col('num_calls')>=3).then(pl.lit('OPTIMAL')).otherwise(pl.lit('BELOW')).alias('kpi_ind')\n",
    "        ).drop('num_calls')\n",
    "\n",
    "    result = result.join(lirwd_call_plan,on = 'IID',how = 'left'\n",
    "    ).with_columns(\n",
    "        pl.when(pl.col('call_freq_quarter').is_null()).then(pl.lit('\\\\N')).otherwise(pl.col('kpi_ind')).alias('kpi_ind')\n",
    "    ).drop('call_freq_quarter')\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#num of calls\n",
    "def process_num_calls(df):\n",
    "    source_df = (\n",
    "        temp_calls\n",
    "        .filter(pl.col('call_week')<=num_weeks_calls)\n",
    "        .filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    calls_iid_qtd = source_df.group_by('AttendeeIID').agg(num_calls = pl.col('CallID').n_unique()\n",
    "    ).rename({'AttendeeIID':'IID'})\n",
    "\n",
    "    return(\n",
    "        df.join(calls_iid_qtd,on = 'IID',how = 'left')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#num of samples\n",
    "#- this step causes nobs to go up as left dataset is joined with samples dataframe\n",
    "#- that samples dataframe is at an IID and product_id level , not just IID. hence its not a 1-1 join\n",
    "#- the product_id in that datframe is made manually using a dictionary variable , not some external file\n",
    "def process_num_samples(df):\n",
    "    cpd_pid_mapping  = {'72 mcg' : '3', '145 mcg' : '4', '290 mcg' : '5'}\n",
    "    \n",
    "    source_df = (\n",
    "        temp_samples\n",
    "        .filter(pl.col('sample_week')<=num_weeks_calls)\n",
    "        .filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "\n",
    "    #samples_iid_qtd\n",
    "    sid = (\n",
    "        source_df\n",
    "        .with_columns(pl.col('CallProductDescription').replace(cpd_pid_mapping).alias('product_id'))\n",
    "        .with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "        .group_by(['AttendeeIID','product_id']).agg(num_samples = pl.col('CallProductQuantity').sum())\n",
    "        .rename({'AttendeeIID':'IID'})\n",
    "    )\n",
    "    # summing up products 3 , 4 ,5 \n",
    "    sid2 = (\n",
    "        sid.group_by('IID').agg(num_samples = pl.col('num_samples').sum())\n",
    "        .with_columns(pl.lit(2).alias('product_id')).select(sid.columns)\n",
    "        .with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "    )\n",
    "    # adding back \n",
    "    sid = sid.vstack(sid2)\n",
    "    # Create a DataFrame with all possible combinations of IID and product_id\n",
    "    all_combinations = (\n",
    "        sid.select('IID').unique()\n",
    "        .join(\n",
    "            pl.DataFrame({'product_id': [2, 3, 4, 5]}), \n",
    "            how='cross'\n",
    "        )\n",
    "    )\n",
    "    # Join with the existing sid DataFrame\n",
    "    complete_samples = (\n",
    "        all_combinations.join(sid, on=['IID', 'product_id'], how='left')\n",
    "        .with_columns(pl.col('num_samples').fill_null(0))\n",
    "    )\n",
    "\n",
    "    df_join = df.join(complete_samples,on = 'IID',how = 'left')\n",
    "    \n",
    "    df_join_nulls = df_join.filter(pl.col('product_id').is_null())\n",
    "    djn2 = df_join_nulls.with_columns(pl.col('product_id').fill_null(2))\n",
    "    djn3 = df_join_nulls.with_columns(pl.col('product_id').fill_null(3))\n",
    "    djn4 = df_join_nulls.with_columns(pl.col('product_id').fill_null(4))\n",
    "    djn5 = df_join_nulls.with_columns(pl.col('product_id').fill_null(5))\n",
    "\n",
    "    dfjn = djn2.vstack(djn3).vstack(djn4).vstack(djn5)\n",
    "\n",
    "    df_final = (\n",
    "        df_join.filter(pl.col('product_id').is_not_null())\n",
    "        .vstack(dfjn)\n",
    "    )\n",
    "\n",
    "    return(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Rx per sample\n",
    "#- Rx dataset should be made at prod_cd = 3,4 or 5 level along with IID\n",
    "#- Rx value should be QTD TUF.\n",
    "def process_rx_per_sample(df):\n",
    "\n",
    "    source_df = (\n",
    "        all_products_tuf\n",
    "        .select(['IID','TUF_qtdc','product_id'])\n",
    "        .filter(pl.col('product_id').is_in([2,3,4,5]))\n",
    "        .join(MASTER_UNI.select(['IID','PDRPOptOutFlag']),on='IID',how='left')\n",
    "        .with_columns(pl.when(pl.col('PDRPOptOutFlag')=='Y').then(pl.lit(0)).otherwise(pl.col('TUF_qtdc')).alias('TUF_qtdc')).drop('PDRPOptOutFlag')\n",
    "    )\n",
    "    \n",
    "    result = (\n",
    "        df.join(source_df,on=['IID','product_id'],how='left')\n",
    "        .with_columns(\n",
    "            pl.when((pl.col('num_samples')!=0) & (pl.col('TUF_qtdc').is_not_null()))\n",
    "            .then(pl.col('TUF_qtdc')/pl.col('num_samples'))\n",
    "            .otherwise(pl.lit('\\\\N'))\n",
    "            .alias('rx_per_sample')\n",
    "        )\n",
    "        .drop('TUF_qtdc')\n",
    "    )\n",
    "\n",
    "    # Override Rx_per_sample for all non targets and abbv only -\n",
    "    result = (\n",
    "        result\n",
    "        .join(mp_spec_seg_dec.select(['IID','segment']),on = 'IID', how = 'left')\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('segment')!='Target').then(pl.lit('\\\\N')).otherwise(pl.col('rx_per_sample')).alias('rx_per_sample')\n",
    "        ).drop('segment')\n",
    "    )\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Last Called Date\n",
    "def process_last_called(df):\n",
    "    source_df = (\n",
    "        temp_calls\n",
    "        #.filter(pl.col('call_week')<=num_weeks_calls)\n",
    "        #.filter(pl.col('CallDate')>= quarter_start)\n",
    "        # .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        # .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        # .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    last_called_df = source_df.group_by('AttendeeIID').agg(\n",
    "        last_called_date = pl.col('CallDate').max()\n",
    "    ).rename({'AttendeeIID':'IID'}\n",
    "    ).with_columns(\n",
    "        pl.col('last_called_date').cast(pl.Utf8).str.slice(0,10).alias('last_called_date')\n",
    "    )\n",
    "\n",
    "    return(df.join(last_called_df,on='IID',how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Num of calls 12 months\n",
    "def process_calls_12m(df):\n",
    "    calls_12m_df = temp_calls.filter(pl.col('call_month')<=12\n",
    "    ).group_by('AttendeeIID').agg(num_calls_12m = pl.col('CallID').n_unique()).rename({'AttendeeIID':'IID'})\n",
    "\n",
    "    return(df.join(calls_12m_df,on='IID',how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#num of Called Months in 12 Months\n",
    "def process_called_months_12m(df):\n",
    "    called_months_12m_df  = temp_calls.filter(pl.col('call_month')<=12\n",
    "    ).group_by('AttendeeIID').agg(called_months_12m = pl.col('call_month').n_unique()\n",
    "    ).rename({'AttendeeIID':'IID'})\n",
    "\n",
    "    return(df.join(called_months_12m_df,on='IID',how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Abbv Visits\n",
    "def process_abbv_visits(df):\n",
    "    source_df = temp_abbv.filter(pl.col('call_week')<=num_weeks_calls).filter(pl.col('CallDate')>= quarter_start)\n",
    "    \n",
    "    a_vist_df = source_df.group_by('AttendeeIID'\n",
    "    ).agg(abbv_visits = pl.col('CallID').n_unique()\n",
    "    ).rename({'AttendeeIID':'IID'})\n",
    "\n",
    "    return(df.join(a_vist_df,on='IID',how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Target Reached Status\n",
    "def process_tgt_reach_st(df):\n",
    "    return(\n",
    "    df.join(mp_spec_seg_dec[['IID','segment']],on='IID',how='left').with_columns(\n",
    "        pl.when(pl.col('segment')=='Target').then(\n",
    "            pl.when(pl.col('num_calls')>=1).then(pl.lit('Yes')).otherwise(pl.lit('No'))\n",
    "        ).otherwise(pl.lit('Non-Target')).alias('tgt_rch_status')).drop('segment')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#13 Wk IW Calls\n",
    "def process_num_calls_13wks(df):\n",
    "    calls_iid_13wk = temp_calls.filter(pl.col('call_week')<= 13\n",
    "    ).group_by('AttendeeIID').agg(num_calls_13wks = pl.col('CallID').n_unique()\n",
    "    ).rename({'AttendeeIID':'IID'})\n",
    "\n",
    "    return(\n",
    "        df.join(calls_iid_13wk,on = 'IID',how = 'left')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For converting to Feed Ready data -\n",
    "def get_feed(temp1):\n",
    "    #Renaming columns according to feed\n",
    "    column_mapping = {\n",
    "        \"IID\": \"Physician_ID\",\n",
    "        \"geography_id\": \"Geography_id\",\n",
    "        \"product_id\": \"Product_id\",\n",
    "        \"kpi_ind\": \"KPI_Ind\",\n",
    "        \"num_calls\": \"Num_Of_Calls\",\n",
    "        \"num_samples\": \"Total_Samples\",\n",
    "        \"rx_per_sample\": \"Rx_Per_Sample\",\n",
    "        \"last_called_date\": \"Last_Called_Date\",\n",
    "        \"num_calls_12m\": \"Num_Of_Calls_12Months\",\n",
    "        \"called_months_12m\": \"Num_Of_Called_Months\",\n",
    "        \"abbv_visits\": \"Num_Of_ABBV_Visits\",\n",
    "        \"tgt_rch_status\": \"Target_Reached_Status\",\n",
    "        \"num_calls_13wks\": \"Thirteen_Week_IW_Calls\"\n",
    "    }\n",
    "    final_feed = temp1.rename(column_mapping)\n",
    "    #required new columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addp = ['Period']\n",
    "    col_to_addna = ['Call_Attainment_Prc','Call_Goal','Surveyed_HCP'] + ['Calls' + str(i) for i in range(1,21)]\n",
    "    # func to add columns with desired value\n",
    "    def addcol(df,columns_to_add,wtl):\n",
    "        for my_col in columns_to_add:\n",
    "            df = df.with_columns(pl.lit(wtl).alias(my_col))\n",
    "        return df\n",
    "    final_feed = addcol(final_feed,col_to_addrt,'WEEKLY')\n",
    "    final_feed = addcol(final_feed,col_to_addp,f'{period_num}-WEEK')\n",
    "    final_feed = addcol(final_feed,col_to_addna,'\\\\N')\n",
    "    # rearranging columns accoring to feed.\n",
    "    req_cols = [\"Physician_ID\", \"Geography_id\", \"Product_id\", \"ReportType\", \"Period\", \"KPI_Ind\", \"Call_Attainment_Prc\", \"Call_Goal\", \n",
    "                \"Num_Of_Calls\", \"Total_Samples\", \"Rx_Per_Sample\", \"Surveyed_HCP\", \"Last_Called_Date\", \"Num_Of_Calls_12Months\", \n",
    "                \"Num_Of_Called_Months\", \"Calls1\", \"Calls2\", \"Calls3\", \"Calls4\", \"Calls5\", \"Calls6\", \"Calls7\", \"Calls8\", \"Calls9\", \n",
    "                \"Calls10\", \"Calls11\", \"Calls12\", \"Calls13\", \"Calls14\", \"Calls15\", \"Calls16\", \"Calls17\", \"Calls18\", \"Calls19\", \n",
    "                \"Calls20\", \"Num_Of_ABBV_Visits\", \"Target_Reached_Status\", \"Thirteen_Week_IW_Calls\"]\n",
    "    final_feed = final_feed.select(req_cols)#final dataset\n",
    "    return(final_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period Loop - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 1\n"
     ]
    }
   ],
   "source": [
    "# Calling Functions and Exporting - \n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/Prescriber/Weekly/'\n",
    "for period_num,PN in zip([1,4,13,26,'qtd'],[1,2,3,4,5]):\n",
    "    if PN>1:\n",
    "        break\n",
    "        \n",
    "    period = f'_{period_num}'\n",
    "    temp1 = mp_spec_seg_dec.select(['IID','geography_id'])\n",
    "    temp1 = process_kpi_ind(temp1)\n",
    "    temp1 = process_num_calls(temp1)\n",
    "    temp1 = process_num_samples(temp1)\n",
    "    temp1 = process_rx_per_sample(temp1)\n",
    "    temp1 = process_last_called(temp1)\n",
    "    temp1 = process_calls_12m(temp1)\n",
    "    temp1 = process_called_months_12m(temp1)\n",
    "    temp1 = process_abbv_visits(temp1)\n",
    "    temp1 = process_tgt_reach_st(temp1)\n",
    "    temp1 = process_num_calls_13wks(temp1)\n",
    "    \n",
    "    #prepping xponent -\n",
    "    tuf1 = (\n",
    "        all_products_tuf\n",
    "        .with_columns(psum = pl.col(f'TUF_{period_num}c')+pl.col(f'TUF_{period_num}p'))\n",
    "        .filter(pl.col('psum')!=0).select(['IID','product_id'])\n",
    "    )\n",
    "    nuf1 = (\n",
    "        all_products_nuf\n",
    "        .with_columns(psum = pl.col(f'NUF_{period_num}c')+pl.col(f'NUF_{period_num}p'))\n",
    "        .filter(pl.col('psum')!=0).select(['IID','product_id'])\n",
    "    )\n",
    "    xponent = tuf1.join(nuf1,on=['IID','product_id'],how='outer_coalesce')\n",
    "\n",
    "    # for duping rows on product_id-\n",
    "    # only duping product_id = 2 rows.\n",
    "    temp1_dups = temp1.filter(pl.col('product_id')==2).drop('product_id')\n",
    "    temp1_dups = (\n",
    "        temp1_dups\n",
    "        .join(xponent,on='IID',how= 'left')\n",
    "        .select(temp1.columns)\n",
    "        .filter(pl.col('product_id').is_not_null()) # if data not there in xpn then cant dup\n",
    "        .with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "    )\n",
    "\n",
    "    temp1 = temp1.vstack(temp1_dups).unique(['IID','product_id'])\n",
    "\n",
    "    # for preserving rows where xponent information avaialble but no calls - adding a new flag:\n",
    "    temp1 = temp1.join(xponent.with_columns(fl = 1),on=['IID','product_id'],how='left')\n",
    "\n",
    "    # force keeping linzess = \n",
    "    temp1 = temp1.with_columns(\n",
    "        pl.when(pl.col('product_id').is_in([2,3,4,5])).then(pl.lit(1)).otherwise(pl.lit(None)).alias('fl2')\n",
    "    )\n",
    "\n",
    "    null_check_cond = (\n",
    "        (\n",
    "            (pl.col('num_calls').is_not_null()) |\n",
    "            (pl.col('num_samples').is_not_null()) | \n",
    "            (pl.col('num_calls_12m').is_not_null()) | \n",
    "            (pl.col('called_months_12m').is_not_null()) | \n",
    "            (pl.col('abbv_visits').is_not_null()) | \n",
    "            (pl.col('num_calls_13wks').is_not_null())\n",
    "        )   & ((pl.col('fl').is_not_null()) | (pl.col('fl2').is_not_null()))\n",
    "    )\n",
    "    temp1 = temp1.filter(null_check_cond).drop(['fl','fl2'])\n",
    "\n",
    "    feed_dataset = get_feed(temp1)\n",
    "    #===================================================\n",
    "    feed_dataset = feed_dataset.to_pandas()\n",
    "    # Select columns of type 'object' (string)\n",
    "    string_columns = feed_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "    feed_dataset[string_columns] = feed_dataset[string_columns].fillna('\\\\N')\n",
    "    feed_dataset = feed_dataset.replace('NaN', '\\\\N')\n",
    "\n",
    "    feed_dataset = feed_dataset.replace([np.nan, np.inf, -np.inf], '\\\\N')\n",
    "    feed_dataset.to_csv(f'{OUT}Weekly_Prescriber_SalesActivity_P{PN}_Feed.txt', sep='|',lineterminator='\\r\\n',index=False)\n",
    "    print(f'Exported Feed {PN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
