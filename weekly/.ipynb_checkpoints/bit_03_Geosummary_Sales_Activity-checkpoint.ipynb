{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GS Sales Activity pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import datetime, timedelta,date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "curr_date = datetime.strptime(js['curr_date'], '%Y-%m-%d').date()\n",
    "quarter_start = datetime.strptime(js['quarter_start'], '%Y-%m-%d').date()\n",
    "quarter_end = datetime.strptime(js['quarter_end'], '%Y-%m-%d').date()\n",
    "qtr_data = js['qtr_data']\n",
    "num_weeks_calls = js['num_weeks_calls']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "geo = f's3://{bucket}/PYADM/quaterly/{qtr_data}/geography/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')\n",
    "\n",
    "def intck(interval, start_date, end_date):\n",
    "    if interval == 'DAY':\n",
    "        return (end_date - start_date).days\n",
    "    elif interval == 'MONTH':\n",
    "        rd = relativedelta(end_date, start_date)\n",
    "        return rd.years * 12 + rd.months\n",
    "    elif interval == 'WEEK':\n",
    "        return (end_date - start_date).days // 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "load('temp_calls')\n",
    "load('temp_samples')\n",
    "load('temp_abbv')\n",
    "load('mp_spec_seg_dec')\n",
    "load('hierarchy',geo)\n",
    "load('wd_raw')\n",
    "load('lirwd_call_plan')\n",
    "load('laxdn_geoid_sum')\n",
    "\n",
    "geo_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/GeographyMapping.txt',separator='|')\n",
    "geo_mapping = geo_mapping.with_columns(\n",
    "    Code = pl.when(pl.col('Code')!= 'NATION').then(pl.lit('1111-')+pl.col('Code')).otherwise(pl.col('Code'))\n",
    ")\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "\n",
    "geo_id_full = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "\n",
    "#fixes for vortex import -> Probably caused by Polars Upgrades\n",
    "temp_calls = temp_calls.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_samples = temp_samples.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_abbv = temp_abbv.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "wd_raw = wd_raw.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "laxdn_geoid_sum = laxdn_geoid_sum.with_columns(pl.col('geography_id').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Processing  1. temp calls  2. temp samples 3. temp abbv datasets\n",
    "# - doubt is physiian terr id same as salesrepterrid for every record?\n",
    "temp_calls_mp_spec = (\n",
    "    temp_calls\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")\n",
    "# NOTE -\n",
    "# Combining MP and dropping null geo\n",
    "# Adding Area and Region Code\n",
    "# Adding Working Day\n",
    "# Adding call_freq_quarter\n",
    "\n",
    "# For Supproting Calc ->\n",
    "#geo_code_mapper = temp_calls_mp_spec[['geography_id','region_geography_id','area_geography_id','nation_geography_id']].unique()\n",
    "geo_code_mapper = geo_id_full\n",
    "geo_code_mapper.to_pandas().to_parquet(dflib+'geo_code_mapper.parquet') #exporting for other code use\n",
    "\n",
    "###\n",
    "temp_samples_mp_spec = (\n",
    "    temp_samples\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")\n",
    "###\n",
    "temp_abbv_mp_spec = (\n",
    "    temp_abbv\n",
    "    .join(mp_spec_seg_dec,left_on = 'AttendeeIID',right_on = 'IID', how = 'left').filter(pl.col('geography_id').is_not_null())\n",
    "    .join(geo_id_full,on = 'geography_id',how = 'left')\n",
    "    .join(wd_raw[['SalesRepIID','days_in_field']],on = 'SalesRepIID', how = 'left')\n",
    "    .join(lirwd_call_plan,left_on = 'AttendeeIID', right_on = 'IID', how = 'left')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#num_calls, num_hcps, days_in_field,call_freq_quarter\n",
    "\n",
    "def process_1(df):\n",
    "    source_df = temp_calls_mp_spec.filter(pl.col('call_week')<=num_weeks_calls) #only keeping QTD\n",
    "    for i in range(4): #looping over 4 levels-\n",
    "        g = levels[i]\n",
    "        f = (source_df.group_by([g,p,sg,spc,d])\n",
    "            .agg(\n",
    "                num_calls=pl.col('CallID').n_unique(),  # Count distinct CallIDs\n",
    "                num_hcps = pl.col('AttendeeIID').n_unique(), #Count distinct IIDs\n",
    "                days_in_field = pl.col('days_in_field').mean(), # WD (using mean here because all vals are same)\n",
    "                call_freq_quarter = pl.col('call_freq_quarter').sum(), # doubt\n",
    "                #num_called_months = pl.col('call_month').n_unique()\n",
    "            ))\n",
    "        df[i] = f\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "#total_calls\n",
    "def process_2(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        geo_calls_df = f.group_by(levels[i]).agg(total_calls = pl.sum('num_calls'))\n",
    "        f = f.join(geo_calls_df,on=levels[i],how='left')\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# Abbv Visits -\n",
    "def process_3(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        source_df = (\n",
    "            temp_abbv_mp_spec.filter(pl.col('call_week')<=num_weeks_calls) #only keeping QTD\n",
    "            .group_by([levels[i],p,sg,spc,d])\n",
    "            .agg(num_abbv_calls=pl.col('CallID').n_unique())\n",
    "        )\n",
    "\n",
    "        f = f.join(source_df,on = [levels[i],p,sg,spc,d],how = 'left')\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# number of targets and 13wk tgts\n",
    "def process_4(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        source_df = (\n",
    "            mp_spec_seg_dec\n",
    "            .filter(pl.col('segment')=='Target')\n",
    "            .join(geo_code_mapper,on = levels[0],how = 'left')\n",
    "            .group_by(levels[i])\n",
    "            .agg(target_hcps = pl.col('IID').n_unique())\n",
    "        )\n",
    "\n",
    "        source_df_2 = (\n",
    "            temp_calls_mp_spec\n",
    "            .filter((pl.col('call_week')<=13)&(pl.col('segment')=='Target'))\n",
    "            .group_by(levels[i])\n",
    "            .agg(tgts_13wks = pl.col('AttendeeIID').n_unique())\n",
    "        )\n",
    "\n",
    "        f = (\n",
    "            f\n",
    "            .join(source_df,on=levels[i],how='left')\n",
    "            .join(source_df_2,on=levels[i],how='left')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# wk qtd\n",
    "def process_5(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        source_df = (\n",
    "            laxdn_geoid_sum\n",
    "            .join(geo_code_mapper,on=levels[0],how = 'left')\n",
    "            .group_by(levels[i])\n",
    "            .agg(wk_qtd = pl.col('wk_qtd').sum())\n",
    "        )\n",
    "\n",
    "        f = f.join(source_df,on = levels[i],how='left')\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# num_samples, num_sample_hcps, total_samples\n",
    "def process_6(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        source_df = (\n",
    "            temp_samples_mp_spec\n",
    "            .filter(pl.col('sample_week')<=num_weeks_calls) \n",
    "            .group_by([levels[i],p,sg,spc,d])\n",
    "            .agg(\n",
    "                num_samples=pl.col('CallID').n_unique(),  # Count distinct CallIDs\n",
    "                num_sample_hcps = pl.col('AttendeeIID').n_unique(), #Count distinct IIDs\n",
    "            )\n",
    "        )\n",
    "\n",
    "        source_df_2 = (\n",
    "            source_df\n",
    "            .group_by(levels[i])\n",
    "            .agg(total_samples = pl.sum('num_samples'))\n",
    "        )\n",
    "\n",
    "        f = (\n",
    "            f\n",
    "            .join(source_df,on =[levels[i],p,sg,spc,d],how = 'left')\n",
    "            .join(source_df_2, on = levels[i],how = 'left')\n",
    "        )\n",
    "\n",
    "        df[i] = f\n",
    "\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "#tgts3 - Num_Of_QTD_Tgts_3Plus_Calls\n",
    "def process_7(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        source_df = (\n",
    "            temp_calls_mp_spec.filter((pl.col('call_week')<=num_weeks_calls)&(pl.col('segment')=='Target'))\n",
    "            .group_by([levels[i],p,sg,spc,d])\n",
    "            .agg(\n",
    "                tgts3=pl.col('AttendeeIID').n_unique(),\n",
    "                calls3_tgts = pl.col('CallID').n_unique()\n",
    "            )\n",
    "            .filter(pl.col('calls3_tgts')>3)\n",
    "            .drop('calls3_tgts')\n",
    "        )\n",
    "\n",
    "        f = f.join(source_df,on = [levels[i],p,sg,spc,d],how = 'left')\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# Optimal and Bellow\n",
    "def process_12(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        source_df = temp_calls_mp_spec.filter(pl.col('call_week')<=num_weeks_calls)\n",
    "        months_called_df = source_df.group_by('AttendeeIID').agg(month_call_count = pl.col('call_month').n_unique())\n",
    "        source_df = source_df.join(months_called_df,on = 'AttendeeIID',how = 'left')\n",
    "        source_df_o = (\n",
    "            source_df.filter(pl.col('month_call_count')==3)\n",
    "            .group_by(levels[i],p,sg,spc,d)\n",
    "            .agg(optimal = pl.col('AttendeeIID').n_unique())\n",
    "        )\n",
    "        source_df_b = (\n",
    "            source_df.filter(pl.col('month_call_count')<3)\n",
    "            .group_by(levels[i],p,sg,spc,d)\n",
    "            .agg(below = pl.col('AttendeeIID').n_unique())\n",
    "        )\n",
    "        source_df = source_df_b.join(source_df_o,on=[levels[i],p,sg,spc,d],how='outer_coalesce')\n",
    "        f = f.join(source_df,on = [levels[i],p,sg,spc,d],how = 'left')\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "### adding rollups \n",
    "def add_rollups_calls(all_df):\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('Universe'),pl.lit('0-10'),pl.lit('all_spec')\n",
    "    #Looping over 4 levels (terr,reg,area,nation)\n",
    "    for i in range(4):\n",
    "        df = all_df[i]\n",
    "        g = levels[i]\n",
    "        metric_cols = df.columns[5:] #add more here if adding more columns on top\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        agg_dict['days_in_field'] = pl.col('days_in_field').mean()\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols) #used for vstack later\n",
    "\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        #### Processing Done ####\n",
    "        df = (\n",
    "            df.select(main_seq)\n",
    "            .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "            .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "            .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        # Store Data Back :\n",
    "        all_df[i] = df\n",
    "    return(all_df)\n",
    "\n",
    "\n",
    "\n",
    "#### adding formula backed metrics\n",
    "#call_distribution, calls_per_day\n",
    "def process_8(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = f.with_columns(\n",
    "            call_distribution = pl.col('num_calls') / pl.col('total_calls'),\n",
    "            calls_per_day = pl.col('num_calls')/pl.col('days_in_field')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# Frequency (call_freq)\n",
    "def process_9(df):\n",
    "    for i in range(4):\n",
    "        f= df[i]\n",
    "        \n",
    "        source_df = temp_calls_mp_spec.filter(pl.col('call_week')<=num_weeks_calls)\n",
    "        call_freq_df = (\n",
    "            source_df.group_by(['AttendeeIID',levels[i]])\n",
    "            .agg(calls = pl.col('CallID').n_unique())\n",
    "            .group_by(levels[i]).agg(call_freq = pl.col('calls').mean())\n",
    "        )\n",
    "\n",
    "        f = f.join(call_freq_df,on = levels[i],how = 'left')\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "#qtd reach , 13 week reach, QTD_Tgts_Not_Reached\n",
    "def process_10(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                prc_reach = pl.col('num_hcps')/pl.col('target_hcps'),\n",
    "                tgt_reach_13wk = pl.col('tgts_13wks')/pl.col('target_hcps'),\n",
    "                qtd_tgt_nreach = pl.col('target_hcps')- pl.col('num_hcps')\n",
    "            )\n",
    "            .drop('tgts_13wks')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "#Num_Of_Called_Months_12M\n",
    "def process_11(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        source_df = (\n",
    "            temp_calls_mp_spec\n",
    "            .filter(pl.col('call_month')<=12)\n",
    "            .group_by([levels[i],'AttendeeIID'])\n",
    "            .agg(called_months = pl.col('call_month').n_unique())\n",
    "            .group_by(levels[i])\n",
    "            .agg(called_12m = pl.col('called_months').mean())\n",
    "        )\n",
    "\n",
    "        f = f.join(source_df,on = levels[i],how = 'left')\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# Trx Per Call\n",
    "def process_13(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        f = f.with_columns(\n",
    "            rx_per_call = pl.col('wk_qtd')/pl.col('num_calls')\n",
    "        )\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# sample_distribution, prc_sampled_phy, avg_sample_per_hcp,rx_per_sample\n",
    "\n",
    "def process_14(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                sample_distribution = pl.col('num_samples')/pl.col('total_samples'),\n",
    "                prc_sampled_phy = pl.col('num_sample_hcps')/pl.col('num_hcps'),\n",
    "                avg_sample_per_hcp = pl.col('num_samples')/pl.col('target_hcps'),\n",
    "                rx_per_sample = pl.col('wk_qtd')/pl.col('total_samples')\n",
    "            ).drop('wk_qtd')\n",
    "        )\n",
    "\n",
    "        df[i] = f\n",
    "\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "#call_freq_goal_prc\n",
    "def process_15(df):\n",
    "    formula_helper_1 = intck('DAY',quarter_start,curr_date) / intck('DAY',quarter_start,quarter_end)\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = f.with_columns(\n",
    "            call_freq_goal_prc = pl.when(pl.col('call_freq_quarter').is_null()).then(None\n",
    "            ).otherwise(pl.col('total_calls')/(pl.col('call_freq_quarter') * formula_helper_1))\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Prc_Of_Optimal_And_Above\n",
    "def process_16(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = f.with_columns(perc_opt_above = pl.col('optimal')/(pl.col('optimal')+pl.col('below')))\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# adding upper and lower limits : (use prc_reach)\n",
    "# flow -\n",
    "## for terr level -\n",
    "# three cols , ind1,ind2,ind3\n",
    "# for ind1 ->nation , ind2->area , ind3->region\n",
    "# to create an upper limit and lower limit \n",
    "# upper : meadian + 0.5*stddev\n",
    "# lower : median - 0.5*stddev\n",
    "#bench_ind : if var > upper then A | if var < lower then B | if lower <= var <= upper then E\n",
    "\n",
    "def process_reach_benchmark(df): # WORKING CORRECTLY BUT NOT MODULAR , PLEASE UPDATE STRUCTURE \n",
    "    def add_indicator(df, ind_name, col1, col2, col3):\n",
    "        return df.with_columns(\n",
    "            pl.when(pl.col(col1) > pl.col(col2))\n",
    "            .then(pl.lit('A'))\n",
    "            .when(pl.col(col1) < pl.col(col3))\n",
    "            .then(pl.lit('B'))\n",
    "            .when((pl.col(col3) < pl.col(col1)) & (pl.col(col1) < pl.col(col2)))\n",
    "            .then(pl.lit('E'))\n",
    "            .otherwise(None)  # You can replace 'N/A' with any default value\n",
    "            .alias(ind_name)\n",
    "        )\n",
    "    #Terr\n",
    "    f = df[0]\n",
    "    nf = f.select([levels[0],p,sg,spc,d,'prc_reach'])\n",
    "    nf = nf.join(geo_code_mapper,on = levels[0],how = 'left')\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        nll = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        all = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf_r = nf.group_by([levels[1],p,sg,spc,d]).agg(\n",
    "        rul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        rll = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "        .join(nf_r,on=[levels[1],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[1],levels[2],levels[3])\n",
    "\n",
    "\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind1', 'prc_reach', 'nul', 'nll')\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind2', 'prc_reach', 'aul', 'all')\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind3', 'prc_reach', 'rul', 'rll').drop(['nul','nll','aul','all','rul','rll','prc_reach'])\n",
    "\n",
    "    f = f.join(nf,on=[levels[0],p, sg, spc, d],how = 'left')\n",
    "    df[0] = f\n",
    "    #Region\n",
    "    f = df[1]\n",
    "    nf = f.select([levels[1],p,sg,spc,d,'prc_reach']).join(\n",
    "        geo_code_mapper[['region_geography_id','area_geography_id']].unique(),on = levels[1],how = 'left'\n",
    "    )\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        nll = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        all = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[2],levels[3])\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind1', 'prc_reach', 'nul', 'nll')\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind2', 'prc_reach', 'aul', 'all')\n",
    "    nf = nf.with_columns(pl.lit(None).alias('Reach_Prc_BnchMrk_Ind3')).drop(['nul','nll','aul','all','prc_reach'])\n",
    "    f = f.join(nf,on=[levels[1],p, sg, spc, d],how = 'left')\n",
    "    df[1] = f\n",
    "    #Area\n",
    "    f = df[2]\n",
    "    nf = f.select([levels[2],p,sg,spc,d,'prc_reach'])\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col('prc_reach').median() + (0.5*pl.col('prc_reach').std())),\n",
    "        nll = (pl.col('prc_reach').median() - (0.5*pl.col('prc_reach').std()))\n",
    "    )\n",
    "    nf = (nf.join(nf_n, on=[p, sg, spc, d], how='left'))\n",
    "    nf = add_indicator(nf, 'Reach_Prc_BnchMrk_Ind1', 'prc_reach', 'nul', 'nll')\n",
    "    nf = nf.with_columns(pl.lit(None).alias('Reach_Prc_BnchMrk_Ind2'),pl.lit(None).alias('Reach_Prc_BnchMrk_Ind3')).drop(['nul','nll','prc_reach'])\n",
    "    f = f.join(nf,on=[levels[2],p, sg, spc, d],how = 'left')\n",
    "    df[2] = f\n",
    "    #Nation \n",
    "    f = df[3]\n",
    "    f = f.with_columns(pl.lit(None).alias('Reach_Prc_BnchMrk_Ind1'),pl.lit(None).alias('Reach_Prc_BnchMrk_Ind2'),pl.lit(None).alias('Reach_Prc_BnchMrk_Ind3'))\n",
    "    df[3] = f\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Convert To Feed Ready data\n",
    "def get_feed(temp1):\n",
    "    temp1[0] = temp1[0].rename({'geography_id': 'Geography_id'})\n",
    "    temp1[1] = temp1[1].rename({'region_geography_id': 'Geography_id'})\n",
    "    temp1[2] = temp1[2].rename({'area_geography_id': 'Geography_id'})\n",
    "    temp1[3] = temp1[3].rename({'nation_geography_id': 'Geography_id'})\n",
    "    \n",
    "    final_feed = temp1[0].vstack(temp1[1]).vstack(temp1[2]).vstack(temp1[3])\n",
    "\n",
    "    #fix for product_id\n",
    "    pm = prod_mapping.with_columns(pl.lit('LIN').alias('product')).select(['product','product_id'])\n",
    "    final_feed = final_feed.join(pm,on='product',how='left').drop('product')\n",
    "    \n",
    "    # droping extra columns\n",
    "    final_feed = final_feed.drop(['num_hcps','call_freq_quarter','total_calls','num_samples'])\n",
    "    \n",
    "    #renaming columns \n",
    "    col_mapping = {\n",
    "        'product_id':'Product_id',\n",
    "        'segment':'Segment',\n",
    "        'decile':'Decile',\n",
    "        'specialty_group':'Specialty',\n",
    "        'num_calls':'Num_Of_Calls',\n",
    "        'days_in_field':'Days_In_Field',\n",
    "        'num_abbv_calls':'ABBV_Visits',\n",
    "        'target_hcps':'Num_Of_Targets',\n",
    "        'num_sample_hcps':'Num_Of_Sampled_Physicians',\n",
    "        'total_samples':'Total_Samples',\n",
    "        'tgts3':'Num_Of_QTD_Tgts_3Plus_Calls',\n",
    "        'below':'Below',\n",
    "        'optimal':'Optimal',\n",
    "        'call_distribution':'Call_Distribution',\n",
    "        'calls_per_day':'Calls_Per_Day',\n",
    "        'call_freq':'Frequency',\n",
    "        'prc_reach':'Prc_Reach',\n",
    "        'tgt_reach_13wk':'Thirteen_Week_Tgt_Reach',\n",
    "        'qtd_tgt_nreach':'QTD_Tgts_Not_Reached',\n",
    "        'called_12m':'Num_Of_Called_Months_12M',\n",
    "        'rx_per_call':'Rx_Per_Call',\n",
    "        'sample_distribution':'Sample_Distribution',\n",
    "        'prc_sampled_phy':'Prc_Of_Sampled_Physicians',\n",
    "        'avg_sample_per_hcp':'Avg_Samples_Per_HCP',\n",
    "        'rx_per_sample':'Rx_Per_Sample',\n",
    "        'call_freq_goal_prc':'Call_Freq_Goal_Prc',\n",
    "        'perc_opt_above':'Prc_Of_Optimal_And_Above'\n",
    "    }\n",
    "    final_feed = final_feed.rename(col_mapping)\n",
    "    \n",
    "    # required columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addp = ['Period']\n",
    "    col_to_adds = ['No_Call','Above']\n",
    "    col_to_addna = ['Below','Optimal','Prc_Of_Surveyed_HCPs','Called_1_Time', 'Called_2_Times', 'Called_3_Times', \n",
    "                    'Called_4_Times', 'Called_5_Times', 'Called_6_Times', 'Total_Num_Of_Called_2_Times', \n",
    "                    'Prc_Of_Called_2_Times','Pharmacy_Calls_Per_Day', 'Num_Of_Calls2', 'Calls1', 'Calls2', \n",
    "                    'Calls3', 'Calls4', 'Calls5', 'Calls6', 'Calls7', 'Calls8', 'Calls9', 'Calls10', 'Calls11', \n",
    "                    'Calls12', 'Calls13', 'Calls14', 'Calls15', 'Calls16', 'Calls17', 'Calls18', 'Calls19', 'Calls20']\n",
    "    \t\t\t\t\n",
    "    # func to add columns with desired value\n",
    "    def addcol(df,columns_to_add,wtl):\n",
    "        for my_col in columns_to_add:\n",
    "            df = df.with_columns(pl.lit(wtl).alias(my_col))\n",
    "        return df\n",
    "    \n",
    "    final_feed = addcol(final_feed,col_to_addrt,'WEEKLY')\n",
    "    final_feed = addcol(final_feed,col_to_addp,f'{period_num}-WEEK')\n",
    "    final_feed = addcol(final_feed,col_to_adds,'.')\n",
    "    final_feed = addcol(final_feed,col_to_addna,'\\\\N')\n",
    "    \n",
    "    # arranging columns according to feed\n",
    "    req_col = ['Geography_id', 'Product_id', 'Segment', 'Specialty', 'ReportType', 'Period', 'Decile', \n",
    "               'Call_Distribution', 'Num_Of_Targets', 'Num_Of_Calls', 'Calls_Per_Day', 'Call_Freq_Goal_Prc',\n",
    "                'Prc_Reach', 'Reach_Prc_BnchMrk_Ind1', 'Reach_Prc_BnchMrk_Ind2', 'Reach_Prc_BnchMrk_Ind3', \n",
    "                'Frequency', 'Rx_Per_Call', 'Days_In_Field', 'No_Call', 'Below', 'Optimal', 'Above', \n",
    "                'Prc_Of_Optimal_And_Above', 'Sample_Distribution', 'Prc_Of_Sampled_Physicians', \n",
    "                'Total_Samples', 'Avg_Samples_Per_HCP', 'Rx_Per_Sample', 'Prc_Of_Surveyed_HCPs', \n",
    "                'Called_1_Time', 'Called_2_Times', 'Called_3_Times', 'Called_4_Times', 'Called_5_Times',\n",
    "                'Called_6_Times', 'Total_Num_Of_Called_2_Times', 'Prc_Of_Called_2_Times', \n",
    "                'Num_Of_Sampled_Physicians', 'Pharmacy_Calls_Per_Day', 'Num_Of_Calls2', \n",
    "                'Calls1', 'Calls2', 'Calls3', 'Calls4', 'Calls5', 'Calls6', 'Calls7', 'Calls8', \n",
    "                'Calls9', 'Calls10', 'Calls11', 'Calls12', 'Calls13', 'Calls14', 'Calls15', 'Calls16',\n",
    "                'Calls17', 'Calls18', 'Calls19', 'Calls20', 'ABBV_Visits', 'Num_Of_Called_Months_12M',\n",
    "                'Thirteen_Week_Tgt_Reach', 'QTD_Tgts_Not_Reached', 'Num_Of_QTD_Tgts_3Plus_Calls']\n",
    "    final_feed = final_feed.select(req_col)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    return(final_feed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period :  all periods will have the same data - QTD)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for trvializing formula : \n",
    "p,sg,spc,d = 'product','segment','specialty_group','decile'\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/GeoSummary/Weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calling all functions and Exporting Feeds\n",
    "for period_num,PN in zip([1,4,13,26,'qtd'],[1,2,3,4,5]):\n",
    "    temp1 = [pl.DataFrame() for _ in range(4)] # creating an empty dataframe holder list obj\n",
    "    temp1 = process_1(temp1)\n",
    "    temp1 = process_2(temp1)\n",
    "    temp1 = process_3(temp1)\n",
    "    temp1 = process_4(temp1)\n",
    "    temp1 = process_5(temp1)\n",
    "    temp1 = process_6(temp1)\n",
    "    temp1 = process_7(temp1)\n",
    "    temp1 = process_12(temp1)\n",
    "    temp1 = add_rollups_calls(temp1)\n",
    "    temp1 = process_8(temp1)\n",
    "    temp1 = process_9(temp1)\n",
    "    temp1 = process_10(temp1)\n",
    "    temp1 = process_11(temp1)\n",
    "    temp1 = process_13(temp1)\n",
    "    temp1 = process_14(temp1)\n",
    "    temp1 = process_15(temp1)\n",
    "    temp1 = process_16(temp1)\n",
    "    temp1 = process_reach_benchmark(temp1)\n",
    "    feed_dataset = get_feed(temp1)\n",
    "    feed_dataset.to_pandas().to_csv(f'{OUT}Weekly_GeoSummary_SalesActivity_P{PN}_Feed.txt', sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
