{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GS Sales Performance pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:34.589058Z",
     "iopub.status.busy": "2024-06-25T11:53:34.588791Z",
     "iopub.status.idle": "2024-06-25T11:53:35.339228Z",
     "shell.execute_reply": "2024-06-25T11:53:35.338257Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:35.343178Z",
     "iopub.status.busy": "2024-06-25T11:53:35.342512Z",
     "iopub.status.idle": "2024-06-25T11:53:35.347380Z",
     "shell.execute_reply": "2024-06-25T11:53:35.346316Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:35.350720Z",
     "iopub.status.busy": "2024-06-25T11:53:35.350333Z",
     "iopub.status.idle": "2024-06-25T11:53:35.356097Z",
     "shell.execute_reply": "2024-06-25T11:53:35.355092Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:35.361022Z",
     "iopub.status.busy": "2024-06-25T11:53:35.360346Z",
     "iopub.status.idle": "2024-06-25T11:53:35.364868Z",
     "shell.execute_reply": "2024-06-25T11:53:35.363913Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:35.368924Z",
     "iopub.status.busy": "2024-06-25T11:53:35.368227Z",
     "iopub.status.idle": "2024-06-25T11:53:36.508152Z",
     "shell.execute_reply": "2024-06-25T11:53:36.507298Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL'] # only these products are to be read from lax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:36.512576Z",
     "iopub.status.busy": "2024-06-25T11:53:36.511946Z",
     "iopub.status.idle": "2024-06-25T11:53:36.529752Z",
     "shell.execute_reply": "2024-06-25T11:53:36.528773Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_metric_period(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,53)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,5)]).alias(metric+'_4c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,14)]).alias(metric+'_13c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,27)]).alias(metric+'_26c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,num_weeks_rx+1)]).alias(metric+'_qtdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(5,9)]).alias(metric+'_4p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,27)]).alias(metric+'_13p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(27,53)]).alias(metric+'_26p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,14+num_weeks_rx)]).alias(metric+'_qtdp')\n",
    "    )\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    metrics_to_calc = pl.col(metric+'_1c').sum().alias(metric+'_1c'),pl.col(metric+'_4c').sum().alias(metric+'_4c'),pl.col(metric+'_13c').sum().alias(metric+'_13c'),pl.col(metric+'_26c').sum().alias(metric+'_26c'),\\\n",
    "        pl.col(metric+'_qtdc').sum().alias(metric+'_qtdc'),pl.col(metric+'_1p').sum().alias(metric+'_1p'),pl.col(metric+'_4p').sum().alias(metric+'_4p'),pl.col(metric+'_13p').sum().alias(metric+'_13p'),\\\n",
    "        pl.col(metric+'_26p').sum().alias(metric+'_26p'),pl.col(metric+'_qtdp').sum().alias(metric+'_qtdp')\n",
    "    \n",
    "    df_terr = df.group_by(['geography_id','specialty_group','segment','decile','PROD_CD']).agg(metrics_to_calc)\n",
    "\n",
    "    df_reg = df.join(geo_code_mapper[['geography_id','region_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['region_geography_id','specialty_group','segment','decile','PROD_CD']).agg(metrics_to_calc)\n",
    "\n",
    "    df_area = df.join(geo_code_mapper[['geography_id','area_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['area_geography_id','specialty_group','segment','decile','PROD_CD']).agg(metrics_to_calc)\n",
    "\n",
    "    df_nation = df.join(geo_code_mapper[['geography_id','nation_geography_id']],on='geography_id',how='left'\n",
    "    ).group_by(['nation_geography_id','specialty_group','segment','decile','PROD_CD']).agg(metrics_to_calc)\n",
    "\n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:36.534553Z",
     "iopub.status.busy": "2024-06-25T11:53:36.533956Z",
     "iopub.status.idle": "2024-06-25T11:53:36.553409Z",
     "shell.execute_reply": "2024-06-25T11:53:36.551683Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_period_prec_count_metric(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,53)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,5)]).alias(metric+'_4c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,14)]).alias(metric+'_13c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,27)]).alias(metric+'_26c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,num_weeks_rx+1)]).alias(metric+'_qtdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(5,9)]).alias(metric+'_4p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,27)]).alias(metric+'_13p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(27,53)]).alias(metric+'_26p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,14+num_weeks_rx)]).alias(metric+'_qtdp')\n",
    "    )\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "    # Adding Geo Hier\n",
    "    df = df.join(geo_code_mapper,on='geography_id')\n",
    "\n",
    "    time_periods = [f'{metric}_1c',f'{metric}_4c',f'{metric}_13c',f'{metric}_26c',f'{metric}_qtdc'\n",
    "                    ,f'{metric}_1p',f'{metric}_4p',f'{metric}_13p',f'{metric}_26p',f'{metric}_qtdp'] #time periods\n",
    "    group_levels = ['geography_id','region_geography_id', 'area_geography_id', 'nation_geography_id'] #group levels\n",
    "\n",
    "    df_terr = None\n",
    "    df_reg = None\n",
    "    df_area = None\n",
    "    df_nation = None #initialize the output dfs\n",
    "\n",
    "    for period in time_periods:\n",
    "        df_filtered = df.filter(pl.col(period) >= 1)\n",
    "        for level in group_levels:\n",
    "            df_period = (df_filtered.group_by([level, 'specialty_group', 'segment', 'decile', 'PROD_CD']).agg([pl.col('IID').n_unique().alias(f'cp_{period}')]))\n",
    "\n",
    "            # If the dataframes are not initialized, assign df_period to them\n",
    "            if level == 'geography_id' and df_terr is None:\n",
    "                df_terr = df_period\n",
    "            elif level == 'region_geography_id' and df_reg is None:\n",
    "                df_reg = df_period\n",
    "            elif level == 'area_geography_id' and df_area is None:\n",
    "                df_area = df_period\n",
    "            elif level == 'nation_geography_id' and df_nation is None:\n",
    "                df_nation = df_period\n",
    "            else:\n",
    "                # Else, join df_period with the dataframes\n",
    "                if level == 'geography_id':\n",
    "                    df_terr = df_terr.join(df_period, on=[level, 'specialty_group', 'segment', 'decile', 'PROD_CD'], how='outer_coalesce')\n",
    "                elif level == 'region_geography_id':\n",
    "                    df_reg = df_reg.join(df_period, on=[level, 'specialty_group', 'segment', 'decile', 'PROD_CD'], how='outer_coalesce')\n",
    "                elif level == 'area_geography_id':\n",
    "                    df_area = df_area.join(df_period, on=[level, 'specialty_group', 'segment', 'decile', 'PROD_CD'], how='outer_coalesce')\n",
    "                elif level == 'nation_geography_id':\n",
    "                    df_nation = df_nation.join(df_period, on=[level, 'specialty_group', 'segment', 'decile', 'PROD_CD'], how='outer_coalesce')\n",
    "    \n",
    "    return(\n",
    "        df_terr,df_reg,df_area,df_nation\n",
    "    )\n",
    "#Change log : added outer_coalesce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:36.558195Z",
     "iopub.status.busy": "2024-06-25T11:53:36.557374Z",
     "iopub.status.idle": "2024-06-25T11:53:36.564688Z",
     "shell.execute_reply": "2024-06-25T11:53:36.563822Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows(all_prod_df):\n",
    "    # converting tuple to list , because i cant assign the processed df back to it\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    for i in range(4): \n",
    "        df = all_prod_df[i]\n",
    "        agg_dict = {}\n",
    "        for col in df.columns[5:]:\n",
    "            agg_dict[col] = pl.col(col).sum()\n",
    "        \n",
    "        join_cols = df.columns[0:4]\n",
    "\n",
    "        df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "        df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "        df_2_35 = df_2_35.group_by(join_cols + ['parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "        df_1 = df.group_by(join_cols).agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "        # stack 1, 2_35 with df and return\n",
    "        df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "        vstack_helper = df.columns\n",
    "        df = df.vstack(\n",
    "            df_2_35.select(vstack_helper)\n",
    "        ).vstack(\n",
    "            df_1.select(vstack_helper)\n",
    "        )\n",
    "\n",
    "        all_prod_df[i] = df\n",
    "    return(tuple(all_prod_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:36.568349Z",
     "iopub.status.busy": "2024-06-25T11:53:36.568069Z",
     "iopub.status.idle": "2024-06-25T11:53:36.580075Z",
     "shell.execute_reply": "2024-06-25T11:53:36.578949Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_full_rollups(all_prod_df):\n",
    "    # converting the tuple of dfs into a list for processing\n",
    "    all_prod_df = list(all_prod_df)\n",
    "    # for trivializing formulas - \n",
    "    p,sg,d,spc = 'product_id','segment','decile','specialty_group'\n",
    "    sg_roll_up,d_roll_up,spc_roll_up = pl.lit('UNI'),pl.lit('0-10'),pl.lit('ALL SPEC')\n",
    "    \n",
    "    #Looping over 4 levels (terr,reg,area,nation)\n",
    "    for i in range(4):\n",
    "        df = all_prod_df[i]\n",
    "        g = df.columns[0] #should contain geo level\n",
    "        metric_cols = df.columns[4:-1] #should contain the tuf / nuf columns\n",
    "        main_seq = ([g,p,sg,d,spc] + metric_cols) #used for vstack later\n",
    "        agg_dict = {metric: pl.col(metric).sum() for metric in metric_cols}\n",
    "        # First Round - \n",
    "        sg_df = (df.group_by([g,p,d,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg)).select(main_seq))\n",
    "        d_df = (df.group_by([g,p,sg,spc]).agg(**agg_dict).with_columns(d_roll_up.alias(d)).select(main_seq))\n",
    "        spc_df = (df.group_by([g,p,d,sg]).agg(**agg_dict).with_columns(spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Second Round - \n",
    "        sg_d_df = (df.group_by([g,p,spc]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d)).select(main_seq))\n",
    "        sg_spc_df = (df.group_by([g,p,d]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        d_spc_df = (df.group_by([g,p,sg]).agg(**agg_dict).with_columns(d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        # Third Round\n",
    "        sg_d_spc_df = (df.group_by([g,p]).agg(**agg_dict).with_columns(sg_roll_up.alias(sg),d_roll_up.alias(d),spc_roll_up.alias(spc)).select(main_seq))\n",
    "        #### Processing Done ####\n",
    "        df = (\n",
    "            df.select(main_seq)\n",
    "            .vstack(sg_df).vstack(d_df).vstack(spc_df)\n",
    "            .vstack(sg_d_df).vstack(sg_spc_df).vstack(d_spc_df)\n",
    "            .vstack(sg_d_spc_df)\n",
    "        )\n",
    "        # Store Data Back :\n",
    "        all_prod_df[i] = df\n",
    "    \n",
    "    return(tuple(all_prod_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:36.583879Z",
     "iopub.status.busy": "2024-06-25T11:53:36.582993Z",
     "iopub.status.idle": "2024-06-25T11:53:45.144599Z",
     "shell.execute_reply": "2024-06-25T11:53:45.143566Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# RAW Data Prep - > ETA 10 Seconds\n",
    "# Calling Function to Process Summed Metric Values\n",
    "all_products_tuf = get_summed_metric_period('TUF',fetch_products) # this is a tuple containg all 4 rollups\n",
    "all_products_nuf = get_summed_metric_period('NUF',fetch_products)\n",
    "\n",
    "# calling function to add parent product rows to it.\n",
    "all_products_tuf = add_parent_product_rows(all_products_tuf)\n",
    "all_products_nuf = add_parent_product_rows(all_products_nuf)\n",
    "\n",
    "# calling to add full rollup\n",
    "all_products_tuf = add_full_rollups(all_products_tuf)\n",
    "all_products_nuf = add_full_rollups(all_products_nuf)\n",
    "\n",
    "# Calling Function to process count of Prec ('metric_period' >= 1 only)\n",
    "all_products_tuf_hcp = get_period_prec_count_metric('TUF',fetch_products)\n",
    "all_products_nuf_hcp = get_period_prec_count_metric('NUF',fetch_products)\n",
    "\n",
    "# calling function to add parent product rows to it.\n",
    "all_products_tuf_hcp = add_parent_product_rows(all_products_tuf_hcp)\n",
    "all_products_nuf_hcp = add_parent_product_rows(all_products_nuf_hcp)\n",
    "\n",
    "# calling to add full rollup\n",
    "all_products_tuf_hcp = add_full_rollups(all_products_tuf_hcp)\n",
    "all_products_nuf_hcp = add_full_rollups(all_products_nuf_hcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:45.149661Z",
     "iopub.status.busy": "2024-06-25T11:53:45.149353Z",
     "iopub.status.idle": "2024-06-25T11:53:45.315332Z",
     "shell.execute_reply": "2024-06-25T11:53:45.310348Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_1(df):\n",
    "    hold = [[f'{m}{period}c',f'{m}{period}p'] for m in ['TUF','NUF']]\n",
    "    hold2 = [[f'cp_{m}{period}c',f'cp_{m}{period}p'] for m in ['TUF','NUF']]\n",
    "    for i in range(4):\n",
    "        g = levels[i]\n",
    "        gb_helper = [g,spc,sg,d,p]\n",
    "        f = (\n",
    "            all_products_tuf[i][gb_helper + hold[0]]\n",
    "            .join(all_products_nuf[i][gb_helper + hold[1]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_tuf_hcp[i][gb_helper + hold2[0]],on = gb_helper,how = 'left')\n",
    "            .join(all_products_nuf_hcp[i][gb_helper + hold2[1]],on = gb_helper,how = 'left')\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\t\n",
    "#cur , pri , vol change , prc vol growth, rx_per_hcp\n",
    "def process_2(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .rename(\n",
    "                {\n",
    "                    f'TUF{period}c':'cur_vol_trx',f'NUF{period}c':'cur_vol_nrx',\n",
    "                    f'TUF{period}p':'pri_vol_trx',f'NUF{period}p':'pri_vol_nrx'\n",
    "                }\n",
    "            )\n",
    "            .with_columns(\n",
    "                vol_change_trx = pl.col('cur_vol_trx') - pl.col('pri_vol_trx'),\n",
    "                vol_change_nrx = pl.col('cur_vol_nrx') - pl.col('pri_vol_nrx'),\n",
    "                prc_vol_growth_trx = (pl.col('cur_vol_trx')/pl.col('pri_vol_trx'))-1,\n",
    "                prc_vol_growth_nrx = (pl.col('cur_vol_nrx')/pl.col('pri_vol_nrx'))-1,\n",
    "                rx_per_hcp_trx = pl.col('cur_vol_trx')/pl.col(f'cp_TUF{period}c'),\n",
    "                rx_per_hcp_nrx = pl.col('cur_vol_nrx')/pl.col(f'cp_NUF{period}c')\n",
    "            )\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\t\n",
    "#Sales Distribution -> probably wrong contribution\n",
    "def process_3(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        agg_df = f.group_by([levels[i],p]).agg(geo_cur_vol_trx = pl.col('cur_vol_trx').sum(),geo_cur_vol_nrx = pl.col('cur_vol_nrx').sum())\n",
    "        f = (\n",
    "            f\n",
    "            .join(agg_df,on = [levels[i],p],how='left')\n",
    "            .with_columns(\n",
    "                sales_dist_trx=pl.col('cur_vol_trx') / pl.col('geo_cur_vol_trx'),\n",
    "                sales_dist_nrx=pl.col('cur_vol_nrx') / pl.col('geo_cur_vol_nrx')\n",
    "            ).drop(['geo_cur_vol_trx', 'geo_cur_vol_nrx'])\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\t\n",
    "#perc_hcp_presc_trx # DOUBT: total hcp should be taken from xpn or mp ?\n",
    "def process_4(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        to_df =f.filter(pl.col(sg) == 'Target').select([levels[i],spc,sg,d,p] + [f'cp_TUF{period}c', f'cp_NUF{period}c'])\n",
    "        #since this metric is only to be calculated for Target\n",
    "        mp_df = (\n",
    "            mp_spec_seg_dec\n",
    "            .join(geo_code_mapper,on=levels[0],how='left')\n",
    "            .group_by(levels[i])\n",
    "            .agg(num_targets=pl.col('IID').n_unique())\n",
    "        )\n",
    "\n",
    "        f2 = (\n",
    "            to_df\n",
    "            .join(mp_df,on=levels[i],how='left')\n",
    "            .with_columns(\n",
    "                perc_hcp_presc_trx=pl.col(f'cp_TUF{period}c') / pl.col('num_targets'),\n",
    "                perc_hcp_presc_nrx=pl.col(f'cp_NUF{period}c') / pl.col('num_targets')\n",
    "            ).drop([f'cp_TUF{period}c',f'cp_NUF{period}c'])\n",
    "        )\n",
    "\n",
    "        f = f.join(f2,on=[levels[i],spc,sg,d,p],how='left')\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "#volume change ind\n",
    "def process_5(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "        f = (\n",
    "            f\n",
    "            .with_columns(\n",
    "                pl.when(pl.col('vol_change_trx')/pl.col('pri_vol_trx') > 0.02).then(pl.lit('P'))\n",
    "                .when(pl.col('vol_change_trx')/pl.col('pri_vol_trx') < -0.02).then(pl.lit('Q'))\n",
    "                .when(pl.col('vol_change_trx')==0).then(None)\n",
    "                .otherwise(None).alias('vol_change_ind_trx'),\n",
    "\n",
    "                pl.when(pl.col('vol_change_nrx')/pl.col('pri_vol_nrx') > 0.02).then(pl.lit('P'))\n",
    "                .when(pl.col('vol_change_nrx')/pl.col('pri_vol_nrx') < -0.02).then(pl.lit('Q'))\n",
    "                .when(pl.col('vol_change_nrx')==0).then(None)\n",
    "                .otherwise(None).alias('vol_change_ind_nrx')\n",
    "            )\n",
    "        )\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\n",
    "# current share , mkt share, share change,ind\n",
    "def process_6(df):\n",
    "    for i in range(4):\n",
    "        f = df[i]\n",
    "\n",
    "        f_lax = (\n",
    "            f\n",
    "            .select([levels[i],spc,sg,d,p] + ['cur_vol_trx','cur_vol_nrx','pri_vol_trx','pri_vol_nrx'])\n",
    "            .filter(pl.col(p)==1).drop(p)\n",
    "        )\n",
    "\n",
    "        f = (\n",
    "            f\n",
    "            .join(f_lax,on=[levels[i],spc,sg,d],how='left')\n",
    "            .with_columns(\n",
    "                cur_shr_trx = pl.col('cur_vol_trx')/pl.col('cur_vol_trx_right'),\n",
    "                cur_shr_nrx = pl.col('cur_vol_nrx')/pl.col('cur_vol_nrx_right'),\n",
    "                pri_shr_trx = pl.col('pri_vol_trx')/pl.col('pri_vol_trx_right'),\n",
    "                pri_shr_nrx = pl.col('pri_vol_nrx')/pl.col('pri_vol_nrx_right')\n",
    "            )\n",
    "            #.drop(['cur_vol_trx_right','cur_vol_nrx_right','pri_vol_trx_right','pri_vol_nrx_right'])\n",
    "            .rename({\n",
    "                'cur_vol_trx_right':'cur_mkt_vol_trx','cur_vol_nrx_right':'cur_mkt_vol_nrx',\n",
    "                'pri_vol_trx_right':'pri_mkt_vol_trx','pri_vol_nrx_right':'pri_mkt_vol_nrx'\n",
    "            })\n",
    "            .with_columns(\n",
    "                Mkt_Vol_Change_trx = pl.col('cur_mkt_vol_trx')-pl.col('pri_mkt_vol_trx'),\n",
    "                Mkt_Vol_Change_nrx = pl.col('cur_mkt_vol_nrx')-pl.col('pri_mkt_vol_nrx')\n",
    "            ).drop(['cur_mkt_vol_trx','pri_mkt_vol_trx','cur_mkt_vol_nrx','pri_mkt_vol_nrx'])\n",
    "            .with_columns(\n",
    "                prc_shr_growth_trx = (pl.col('cur_shr_trx')/pl.col('pri_shr_trx'))-1,\n",
    "                prc_shr_growth_nrx = (pl.col('cur_shr_nrx')/pl.col('pri_shr_nrx'))-1\n",
    "            )\n",
    "            .with_columns(\n",
    "                shr_change_trx = pl.col('cur_shr_trx')-pl.col('pri_shr_trx'),\n",
    "                shr_change_nrx = pl.col('cur_shr_nrx')-pl.col('pri_shr_nrx')\n",
    "            ).with_columns(\n",
    "                pl.when(pl.col('shr_change_trx') > 0.005).then(pl.lit('P'))\n",
    "                .when(pl.col('shr_change_trx') < -0.005).then(pl.lit('Q'))\n",
    "                .when(pl.col('shr_change_trx')==0).then(None)\n",
    "                .otherwise(None).alias('shr_change_ind_trx'),\n",
    "\n",
    "                pl.when(pl.col('shr_change_nrx') > 0.005).then(pl.lit('P'))\n",
    "                .when(pl.col('shr_change_nrx') < -0.005).then(pl.lit('Q'))\n",
    "                .when(pl.col('shr_change_nrx')==0).then(None)\n",
    "                .otherwise(None).alias('shr_change_ind_nrx')\n",
    "            ).with_columns(\n",
    "                pl.when(pl.col('Mkt_Vol_Change_trx') > 0.005).then(pl.lit('P'))\n",
    "                .when(pl.col('Mkt_Vol_Change_trx') < -0.005).then(pl.lit('Q'))\n",
    "                .when(pl.col('Mkt_Vol_Change_trx')==0).then(None)\n",
    "                .otherwise(None).alias('Mkt_Vol_Change_Ind_trx'),\n",
    "\n",
    "                pl.when(pl.col('Mkt_Vol_Change_nrx') > 0.005).then(pl.lit('P'))\n",
    "                .when(pl.col('Mkt_Vol_Change_nrx') < -0.005).then(pl.lit('Q'))\n",
    "                .when(pl.col('Mkt_Vol_Change_nrx')==0).then(None)\n",
    "                .otherwise(None).alias('Mkt_Vol_Change_Ind_nrx')\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "        df[i] = f\n",
    "    return(df)\n",
    "\t\n",
    "def process_vol_change_benchmark(df,metric,ind_col_name):# WORKING CORRECTLY BUT NOT MODULAR \n",
    "    def add_indicator(df, ind_name, col1, col2, col3):\n",
    "        return df.with_columns(\n",
    "            pl.when(pl.col(col1) > pl.col(col2))\n",
    "            .then(pl.lit('A'))\n",
    "            .when(pl.col(col1) < pl.col(col3))\n",
    "            .then(pl.lit('B'))\n",
    "            .when((pl.col(col3) < pl.col(col1)) & (pl.col(col1) < pl.col(col2)))\n",
    "            .then(pl.lit('E'))\n",
    "            .otherwise(None)  # You can replace 'N/A' with any default value\n",
    "            .alias(ind_name)\n",
    "        )\n",
    "    #Terr\n",
    "    f = df[0]\n",
    "    nf = f.select([levels[0],p,sg,spc,d,metric])\n",
    "    nf = nf.join(geo_code_mapper,on = levels[0],how = 'left')\n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        all = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_r = nf.group_by([levels[1],p,sg,spc,d]).agg(\n",
    "        rul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        rll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "        .join(nf_r,on=[levels[1],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[1],levels[2],levels[3])\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1_{metric[-3:]}', metric, 'nul', 'nll')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind2_{metric[-3:]}', metric, 'aul', 'all')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind3_{metric[-3:]}', metric, 'rul', 'rll').drop(['nul','nll','aul','all','rul','rll',metric])\n",
    "    f = f.join(nf,on=[levels[0],p, sg, spc, d],how = 'left')\n",
    "    df[0] = f\n",
    "    #Region\n",
    "    f = df[1]\n",
    "    nf = f.select([levels[1],p,sg,spc,d,metric]).join(\n",
    "        geo_code_mapper[['region_geography_id','area_geography_id']].unique(),on = levels[1],how = 'left'\n",
    "    )\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf_a = nf.group_by([levels[2],p,sg,spc,d]).agg(\n",
    "        aul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        all = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (\n",
    "        nf\n",
    "        .join(nf_n, on=[p, sg, spc, d], how='left')\n",
    "        .join(nf_a,on=[levels[2],p, sg, spc, d],how='left')\n",
    "    ).drop(levels[2],levels[3])\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1_{metric[-3:]}', metric, 'nul', 'nll')\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind2_{metric[-3:]}', metric, 'aul', 'all')\n",
    "    nf = nf.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3_{metric[-3:]}')).drop(['nul','nll','aul','all',metric])\n",
    "    f = f.join(nf,on=[levels[1],p, sg, spc, d],how = 'left')\n",
    "    df[1] = f\n",
    "    #Area\n",
    "    f = df[2]\n",
    "    nf = f.select([levels[2],p,sg,spc,d,metric])\n",
    "    # create upper and lowers : \n",
    "    nf_n = nf.group_by([p,sg,spc,d]).agg(\n",
    "        nul = (pl.col(metric).median() + (0.5*pl.col(metric).std())),\n",
    "        nll = (pl.col(metric).median() - (0.5*pl.col(metric).std()))\n",
    "    )\n",
    "    nf = (nf.join(nf_n, on=[p, sg, spc, d], how='left'))\n",
    "    nf = add_indicator(nf, f'{ind_col_name}_BnchMrk_Ind1_{metric[-3:]}', metric, 'nul', 'nll')\n",
    "    nf = nf.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind2_{metric[-3:]}'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3_{metric[-3:]}')).drop(['nul','nll',metric])\n",
    "    f = f.join(nf,on=[levels[2],p, sg, spc, d],how = 'left')\n",
    "    df[2] = f\n",
    "    #Nation \n",
    "    f = df[3]\n",
    "    f = f.with_columns(pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind1_{metric[-3:]}'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind2_{metric[-3:]}'),pl.lit(None).alias(f'{ind_col_name}_BnchMrk_Ind3_{metric[-3:]}'))\n",
    "    df[3] = f\n",
    "    return(df)\n",
    "\t\n",
    "#Prc_Benchmark_Vol_Growth ,Vol_Growth_Ind\n",
    "def process_7(df,metric):\n",
    "    for i in range(3):\n",
    "        f = df[i]\n",
    "        geomap  =geo_code_mapper[[levels[i],levels[i+1]]].unique()\n",
    "        fp = df[i+1].select([levels[i+1],spc,sg,d,p,metric]).join(geomap,on=levels[i+1],how='left')\n",
    "        ft = (\n",
    "            f.select([levels[i],spc,sg,d,p])\n",
    "            .join(fp,on=[levels[i],spc,sg,d,p],how='left').drop(levels[i+1])\n",
    "            .rename({f'{metric}':f'Prc_Benchmark_Vol_Growth_{metric[-3:]}'})\n",
    "        )\n",
    "        f = f.join(ft,on=[levels[i],spc,sg,d,p],how='left').with_columns(\n",
    "            pl.when(pl.col(f'prc_vol_growth_{metric[-3:]}')>pl.col(f'Prc_Benchmark_Vol_Growth_{metric[-3:]}'))\n",
    "            .then(pl.lit('L'))\n",
    "            .otherwise(pl.lit(None))\n",
    "            .alias(f'Vol_Growth_Ind_{metric[-3:]}')\n",
    "        )\n",
    "        df[i] = f\n",
    "    \n",
    "    \n",
    "    df[3] = df[3].with_columns(\n",
    "        pl.lit(None).alias(f'Prc_Benchmark_Vol_Growth_{metric[-3:]}'),\n",
    "        pl.lit(None).alias(f'Vol_Growth_Ind_{metric[-3:]}')\n",
    "    )\n",
    "\n",
    "    return(df)\n",
    "\t\n",
    "#Prc_Benchmark_Shr_Growth, Shr_Growth_Ind\n",
    "def process_8(df,metric):\n",
    "    for i in range(3):\n",
    "        f = df[i]\n",
    "        geomap  =geo_code_mapper[[levels[i],levels[i+1]]].unique()\n",
    "        fp = df[i+1].select([levels[i+1],spc,sg,d,p,metric]).join(geomap,on=levels[i+1],how='left')\n",
    "        ft = (\n",
    "            f.select([levels[i],spc,sg,d,p])\n",
    "            .join(fp,on=[levels[i],spc,sg,d,p],how='left').drop(levels[i+1])\n",
    "            .rename({f'{metric}':f'Prc_Benchmark_Shr_Growth_{metric[-3:]}'})\n",
    "        )\n",
    "        f = f.join(ft,on=[levels[i],spc,sg,d,p],how='left').with_columns(\n",
    "            pl.when(pl.col(f'prc_vol_growth_{metric[-3:]}')>pl.col(f'Prc_Benchmark_Shr_Growth_{metric[-3:]}'))\n",
    "            .then(pl.lit('L'))\n",
    "            .otherwise(pl.lit(None))\n",
    "            .alias(f'Shr_Growth_Ind_{metric[-3:]}')\n",
    "        )\n",
    "        df[i] = f\n",
    "    \n",
    "    \n",
    "    df[3] = df[3].with_columns(\n",
    "        pl.lit(None).alias(f'Prc_Benchmark_Shr_Growth_{metric[-3:]}'),\n",
    "        pl.lit(None).alias(f'Shr_Growth_Ind_{metric[-3:]}')\n",
    "    )\n",
    "\n",
    "    return(df)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:45.322458Z",
     "iopub.status.busy": "2024-06-25T11:53:45.322007Z",
     "iopub.status.idle": "2024-06-25T11:53:45.342489Z",
     "shell.execute_reply": "2024-06-25T11:53:45.338554Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Convert to feed ready data \n",
    "def get_feed(temp1):\n",
    "    temp1[0] = temp1[0].rename({'geography_id': 'Geography_id'})\n",
    "    temp1[1] = temp1[1].rename({'region_geography_id': 'Geography_id'})\n",
    "    temp1[2] = temp1[2].rename({'area_geography_id': 'Geography_id'})\n",
    "    temp1[3] = temp1[3].rename({'nation_geography_id': 'Geography_id'})\n",
    "    final_feed = temp1[0].vstack(temp1[1]).vstack(temp1[2]).vstack(temp1[3])\n",
    "    not_req_cols = ['cp_TUF_1c','cp_TUF_1p','cp_NUF_1c','cp_NUF_1p','num_targets']\n",
    "    final_feed = final_feed.drop(not_req_cols)\n",
    "    #function to diving dataframe in two levels('Trx','Nrx')\n",
    "    def select_columns_by_condition(df,metric):\n",
    "        # Get the column names to be excluded based on the condition\n",
    "        excluded_columns = [col for col in df.columns if not col.endswith(metric)]\n",
    "        \n",
    "        # Select all columns except the excluded ones\n",
    "        selected_df = df.select(excluded_columns)\n",
    "        return selected_df\n",
    "    final_feed_nrx = select_columns_by_condition(final_feed,'trx')# dataframe not including trx columns\n",
    "    final_feed_nrx = final_feed_nrx.with_columns(pl.lit('NRX').alias(\"Metric\"))\n",
    "    final_feed_trx = select_columns_by_condition(final_feed,'nrx')\n",
    "    final_feed_trx = final_feed_trx.with_columns(pl.lit('TRX').alias(\"Metric\"))\n",
    "    #function to remove _trx or _nrx from final_feed_nrx and final_feed_trx\n",
    "    def rename_columns_by_condition(df,metric):\n",
    "        renamed_columns = {col: col[:-4] if col.endswith(metric) else col for col in df.columns}\n",
    "        renamed_df = df.rename(renamed_columns)\n",
    "        return renamed_df\n",
    "    # making trx feed columns and nrx feed columns similar so that we can vstack them\n",
    "    final_feed_nrx = rename_columns_by_condition(final_feed_nrx,'nrx')\n",
    "    final_feed_trx = rename_columns_by_condition(final_feed_trx,'trx')\n",
    "    final_feed = final_feed_trx.vstack(final_feed_nrx)\n",
    "    # renming final feed columns according to feeds\n",
    "    rename_cols = {\n",
    "        'segment':'Segment',\n",
    "        'product_id':'Product_id',\n",
    "        'specialty_group':'Specialty',\n",
    "        'decile':'Decile',\n",
    "        'sales_dist': 'Sales_Distribution',\n",
    "        'rx_per_hcp': 'Rx_Per_HCP',\n",
    "        'prc_shr_growth':'Prc_Shr_Growth',\n",
    "        'perc_hcp_presc': 'Prc_Of_HCPs_Prescribing',\n",
    "        'cur_vol': 'Current_Vol',\n",
    "        'pri_vol': 'Prior_Vol',\n",
    "        'vol_change': 'Vol_Change',\n",
    "        'vol_change_ind': 'Vol_Change_Ind',\n",
    "        'prc_vol_growth': 'Prc_Vol_Growth',\n",
    "        'Prc_Benchmark_Vol_Growth': 'Prc_Benchmark_Vol_Growth',\n",
    "        'Vol_Growth_Ind': 'Vol_Growth_Ind',\n",
    "        'cur_shr': 'Current_Shr',\n",
    "        'pri_shr': 'Prior_Shr',\n",
    "        'shr_change': 'Shr_Change',\n",
    "        'shr_change_ind': 'Shr_Change_Ind',\n",
    "    }\n",
    "    final_feed = final_feed.rename(rename_cols)\n",
    "    #required new columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addp = ['Period']\n",
    "    col_to_addna = [ \"Num_Of_HCPs_With_Redemptions\",\n",
    "        \"Total_Num_Of_Redemptions\",\n",
    "        \"Num_Of_HCPs_With_Red_Change\",\n",
    "        \"Num_Of_Redemptions_Change\",\n",
    "        \"Frozen_Competitor_Vol\",\n",
    "        \"DS1_Current_Vol\",\n",
    "        \"DS1_Prior_Vol\",\n",
    "        \"DS2_Current_Vol\",\n",
    "        \"DS2_Prior_Vol\"]\n",
    "    # func to add columns with desired value\n",
    "    def addcol(df,columns_to_add,wtl):\n",
    "        for my_col in columns_to_add:\n",
    "            df = df.with_columns(pl.lit(wtl).alias(my_col))\n",
    "        return df\n",
    "    final_feed = addcol(final_feed,col_to_addrt,'WEEKLY')\n",
    "    final_feed = addcol(final_feed,col_to_addp,f'{period_num}-WEEK')\n",
    "    final_feed = addcol(final_feed,col_to_addna,'\\\\N')\n",
    "    # rearranging columns accoring to feed.\n",
    "    req_cols = [\"Geography_id\", \"Product_id\", \"Segment\", \"Specialty\", \"Metric\", \"ReportType\", \"Period\", \"Decile\", \"Sales_Distribution\", \"Rx_Per_HCP\",\n",
    "        \"Prc_Of_HCPs_Prescribing\", \"Current_Vol\", \"Prior_Vol\", \"Vol_Change\", \"Vol_Change_Ind\", \"Vol_Change_BnchMrk_Ind1\", \"Vol_Change_BnchMrk_Ind2\",\n",
    "        \"Vol_Change_BnchMrk_Ind3\", \"Prc_Vol_Growth\", \"Prc_Benchmark_Vol_Growth\", \"Vol_Growth_Ind\", \"Current_Shr\", \"Prior_Shr\", \"Shr_Change\",\n",
    "        \"Shr_Change_Ind\", \"Shr_Change_BnchMrk_Ind1\", \"Shr_Change_BnchMrk_Ind2\", \"Shr_Change_BnchMrk_Ind3\", \"Prc_Shr_Growth\", \"Prc_Benchmark_Shr_Growth\",\n",
    "        \"Shr_Growth_Ind\", \"Num_Of_HCPs_With_Redemptions\", \"Total_Num_Of_Redemptions\", \"Num_Of_HCPs_With_Red_Change\", \"Num_Of_Redemptions_Change\",\n",
    "        \"Frozen_Competitor_Vol\", \"DS1_Current_Vol\", \"DS1_Prior_Vol\", \"DS2_Current_Vol\", \"DS2_Prior_Vol\", \"Mkt_Vol_Change\", \"Mkt_Vol_Change_Ind\",\n",
    "        \"Mkt_Vol_Change_BnchMrk_Ind1\", \"Mkt_Vol_Change_BnchMrk_Ind2\", \"Mkt_Vol_Change_BnchMrk_Ind3\"]\n",
    "    final_feed = final_feed.select(req_cols)#Final Dataset\n",
    "    \n",
    "    return (final_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period Loop -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:45.351209Z",
     "iopub.status.busy": "2024-06-25T11:53:45.347850Z",
     "iopub.status.idle": "2024-06-25T11:53:45.359369Z",
     "shell.execute_reply": "2024-06-25T11:53:45.357783Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for trvializing formula : \n",
    "p,sg,spc,d = 'product_id','segment','specialty_group','decile'\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/GeoSummary/Weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:53:45.366814Z",
     "iopub.status.busy": "2024-06-25T11:53:45.366473Z",
     "iopub.status.idle": "2024-06-25T11:54:28.464323Z",
     "shell.execute_reply": "2024-06-25T11:54:28.463365Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 1!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 2!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 3!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 4!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 5!\n"
     ]
    }
   ],
   "source": [
    "# Calling Functions and Exporting Feeds-\n",
    "#for period_num,PN in zip(['qtd'],[5]):\n",
    "for period_num,PN in zip([1,4,13,26,'qtd'],[1,2,3,4,5]):\n",
    "    period = f'_{period_num}'\n",
    "    temp1 = [pl.DataFrame() for _ in range(4)] # creating an empty dataframe holder list obj\n",
    "\n",
    "    temp1 = process_1(temp1)\n",
    "    temp1 = process_2(temp1)\n",
    "    temp1 = process_3(temp1)\n",
    "    temp1 = process_4(temp1)\n",
    "    temp1 = process_5(temp1)\n",
    "    temp1 = process_6(temp1)\n",
    "    \n",
    "    # helper ###\n",
    "    terr_growths = (\n",
    "        temp1[0]\n",
    "        .select([levels[0],p,spc,sg,d,'prc_vol_growth_trx','prc_vol_growth_nrx','prc_shr_growth_trx','prc_shr_growth_nrx'])\n",
    "        .filter(pl.col(d)=='0-10').drop(d)\n",
    "    )\n",
    "    terr_growths.to_pandas().to_parquet(dflib+f'terr_growths_{PN}.parquet')\n",
    "    #### helper ####\n",
    "    \n",
    "    temp1 = process_vol_change_benchmark(temp1,'vol_change_trx','Vol_Change')\n",
    "    temp1 = process_vol_change_benchmark(temp1,'vol_change_nrx','Vol_Change')\n",
    "    temp1 = process_7(temp1,'prc_vol_growth_trx')\n",
    "    temp1 = process_7(temp1,'prc_vol_growth_nrx')\n",
    "    #Shr_Change_BnchMrk_Ind1\n",
    "    temp1 = process_vol_change_benchmark(temp1,'shr_change_trx','Shr_Change') #re-using vol change function becasue same logic\n",
    "    temp1 = process_vol_change_benchmark(temp1,'shr_change_nrx','Shr_Change')\n",
    "    \n",
    "    temp1 = process_8(temp1,'prc_shr_growth_trx')\n",
    "    temp1 = process_8(temp1,'prc_shr_growth_nrx')\n",
    "    #Mkt_Vol_Change_BnchMrk_Ind\n",
    "    temp1 = process_vol_change_benchmark(temp1,'Mkt_Vol_Change_trx','Mkt_Vol_Change') #re-using vol change function becasue same logic\n",
    "    temp1 = process_vol_change_benchmark(temp1,'Mkt_Vol_Change_nrx','Mkt_Vol_Change')\n",
    "\n",
    "    feed_dataset = get_feed(temp1)\n",
    "    feed_dataset.to_pandas().to_csv(f'{OUT}Weekly_GeoSummary_SalesPerformance_P{PN}_Feed.txt', sep='|')\n",
    "    print(f'Exported Feed {PN}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
