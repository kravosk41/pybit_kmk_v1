{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presc Trend Feed v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime, timedelta,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "load('MASTER_UNI')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal -\n",
    "def get_lin_voucher_13vols():    \n",
    "    vch = pl.read_parquet(f'{xpn}LIN_VOUCHER.parquet') \n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod,prod2 in zip(['LI1','LI2','LI3'],['LIN1','LIN2','LIN3']):\n",
    "        rename_dict = dict(zip([f'{prod2}TUF{i}' for i in range(1,14)],[f'vVol{i}_TUF' for i in range(1,14)]))\n",
    "        vch_prod = (\n",
    "            vch.select(['IID'] + [f'{prod2}TUF{i}' for i in range(1,14)])\n",
    "            .rename(rename_dict)\n",
    "            .with_columns(pl.lit(prod).alias('PROD_CD'))\n",
    "        )\n",
    "        if prod == 'LI1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "    vch1 = vch1.fill_null(0)\n",
    "    \n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_volumes(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,14)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "    rename_dict = dict(zip(columns[2:],['Vol'+str(i)+'_'+metric for i in range(1,14)]))\n",
    "    df = df.rename(rename_dict)\n",
    "\n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher_13vols()\n",
    "        df = (\n",
    "            df\n",
    "            .join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "            .with_columns([(pl.col(f'Vol{i}_TUF') - pl.col(f'vVol{i}_TUF')).alias(f'Vol{i}_TUF') for i in range(1,14)])\n",
    "            .drop(dfv.columns[1:-1])\n",
    "        )\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null()\n",
    "    ).drop(['specialty_group','segment','decile','geography_id'])\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_lin_voucher_52wkvol():\n",
    "    vch = pl.read_parquet(f'{xpn}LIN_VOUCHER.parquet') \n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod,prod2 in zip(['LI1','LI2','LI3'],['LIN1','LIN2','LIN3']):\n",
    "        vch_prod = (\n",
    "            vch\n",
    "            .select(['IID'] + [f'{prod2}TUF{i}' for i in range(1,52)])\n",
    "            .with_columns(pl.sum_horizontal([f'{prod2}TUF{i}' for i in range(1,52)]).alias(f'vTUF_52c'))\n",
    "            .with_columns(pl.lit(prod).alias('PROD_CD'))\n",
    "            .select(['IID','PROD_CD','vTUF_52c'])\n",
    "        )\n",
    "    \n",
    "        if prod == 'LI1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "    vch1 = vch1.fill_null(0)\n",
    "    return (vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_52_iid_metric(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,53)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 52 wk\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,53)]).alias(metric+'_52c')\n",
    "    )\n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher_52wkvol()\n",
    "        df = (\n",
    "            df\n",
    "            .join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "            .with_columns((pl.col('TUF_52c')-pl.col('vTUF_52c')).alias('TUF_52c'))\n",
    "            .drop('vTUF_52c')\n",
    "        )\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    return(df.drop(['specialty_group','segment','decile','geography_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##### adding parent product rows\n",
    "def add_parent_product_rows(df): #pass dataframe with all products and 13 week volumes for both metrics here\n",
    "\n",
    "    agg_dict  = {}\n",
    "\n",
    "    for i in range(1,14):\n",
    "        for metric in ['TUF','NUF']:\n",
    "            col_name = f'Vol{i}_{metric}'\n",
    "            agg_dict[col_name] = pl.col(col_name).sum()\n",
    "\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    df_1 = df.group_by(['IID']).agg(**agg_dict).with_columns(product_id = pl.lit(1)\n",
    "    ).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data Prep \n",
    "all_products_volume_tuf = get_volumes('TUF',fetch_products)\n",
    "all_products_volume_nuf = get_volumes('NUF',fetch_products)\n",
    "all_products_volume = all_products_volume_tuf.join(all_products_volume_nuf,on = ['IID','PROD_CD'],how='left')\n",
    "\n",
    "#for sub level groups -\n",
    "prod_mapping1 = prod_mapping[['product_id','parent_product_id','code']]#.filter(pl.col('parent_product_id')!=1)\n",
    "\n",
    "all_products_volume = all_products_volume.join(prod_mapping1,left_on='PROD_CD',right_on='code',how='left')\n",
    "all_products_volume = add_parent_product_rows(all_products_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Filtering Rows -\n",
    "lin_52_TUF = get_summed_52_iid_metric('TUF',['LI1','LI2','LI3']).join(prod_mapping1,left_on='PROD_CD',right_on='code',how='left').drop('parent_product_id')\n",
    "lin52tp = lin_52_TUF.group_by('IID').agg(TUF_52c = pl.col('TUF_52c').sum()).with_columns(product_id = pl.lit(2)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "lin_52_TUF = lin_52_TUF.select(['IID','TUF_52c','product_id']).vstack(lin52tp).filter(pl.col('TUF_52c')>0).drop('TUF_52c')\n",
    "\n",
    "lin_52_NUF = get_summed_52_iid_metric('NUF',['LI1','LI2','LI3']).join(prod_mapping1,left_on='PROD_CD',right_on='code',how='left').drop('parent_product_id')\n",
    "lin52np = lin_52_NUF.group_by('IID').agg(NUF_52c = pl.col('NUF_52c').sum()).with_columns(product_id = pl.lit(2)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "lin_52_NUF = lin_52_NUF.select(['IID','NUF_52c','product_id']).vstack(lin52np).filter(pl.col('NUF_52c')>0).drop('NUF_52c')\n",
    "\n",
    "lin_52 = lin_52_TUF.join(lin_52_NUF,on=['IID','product_id'],how = 'outer_coalesce')\n",
    "#adding row for product_id=1\n",
    "lin_52p1 = lin_52.select('IID').unique('IID').with_columns(pl.lit(1).alias('product_id')).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "lin_52 = lin_52.vstack(lin_52p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Volume\n",
    "#this will increase nobs on temp1 as its one IID to many prod rows\n",
    "def add_volume_cols(df): \n",
    "    return(df.join(all_products_volume,on='IID',how='left').filter(pl.col('product_id').is_not_null()))\n",
    "\n",
    "# Share\n",
    "def process_share(df):\n",
    "    rename_dict = {}\n",
    "    expn_dict = {}\n",
    "    for i in range(1,14):\n",
    "        for metric in ['TUF','NUF']:\n",
    "            rename_dict[f'Vol{i}_{metric}'] = f'lax_Vol{i}_{metric}'\n",
    "            expn_dict[f'Shr{i}_{metric}'] = pl.col(f'Vol{i}_{metric}')/pl.col(f'lax_Vol{i}_{metric}')\n",
    "\n",
    "    df_1 = df.filter(pl.col('product_id')==1).rename(rename_dict).drop(['product_id','geography_id']) # this will contain LAX volumes for each IID\n",
    "    df = df.join(df_1,on='IID',how='left'\n",
    "    ).with_columns(**expn_dict).drop(list(rename_dict.values()))\n",
    "\n",
    "    return(df)\n",
    "\n",
    "# Trend\n",
    "def process_trend(df,metric):\n",
    "    THRE_13 = 1/26\n",
    "    THRE_4 = 1/10\n",
    "    #THRE = 1/4 # not used\n",
    "\n",
    "    df2 = df.select(['IID','geography_id','product_id']+[f'Vol{i}_{metric}' for i in range(1,14)])\n",
    "    #AVG_TUF\n",
    "    df2 = df2.with_columns(\n",
    "        AVG_TUF = pl.mean_horizontal([f'Vol{i}_{metric}' for i in range(1,14)])\n",
    "    )\n",
    "\n",
    "    #SLOPE_13\n",
    "    AVG_TUF = pl.col('AVG_TUF') #just to make formatting easier (polars.expr.expr.Expr obj)\n",
    "    df2 = df2.with_columns(\n",
    "        SLOPE_13 = (\n",
    "        -5.5 * (pl.col(f'Vol13_{metric}') - AVG_TUF) \n",
    "        -4.5 * (pl.col(f'Vol12_{metric}') - AVG_TUF) \n",
    "        -3.5 * (pl.col(f'Vol11_{metric}') - AVG_TUF) \n",
    "        -2.5 * (pl.col(f'Vol10_{metric}') - AVG_TUF) \n",
    "        -1.5 * (pl.col(f'Vol9_{metric}') - AVG_TUF) \n",
    "        -0.5 * (pl.col(f'Vol8_{metric}') - AVG_TUF) \n",
    "        +0.5 * (pl.col(f'Vol6_{metric}') - AVG_TUF) \n",
    "        +1.5 * (pl.col(f'Vol5_{metric}') - AVG_TUF) \n",
    "        +2.5 * (pl.col(f'Vol4_{metric}') - AVG_TUF) \n",
    "        +3.5 * (pl.col(f'Vol3_{metric}') - AVG_TUF) \n",
    "        +4.5 * (pl.col(f'Vol2_{metric}') - AVG_TUF) \n",
    "        +5.5 * (pl.col(f'Vol1_{metric}') - AVG_TUF)\n",
    "        ) / 143\n",
    "    )\n",
    "\n",
    "    #AVG_TUF_4\n",
    "    df2 = df2.with_columns(\n",
    "        AVG_TUF_4 = pl.mean_horizontal([f'Vol{i}_{metric}' for i in range(5,9)])\n",
    "    )\n",
    "\n",
    "    #SLOPE_4\n",
    "    AVG_TUF_4 = pl.col('AVG_TUF_4') # just for formatting\n",
    "    df2 = df2.with_columns(\n",
    "        SLOPE_4 = (\n",
    "        -1.5 * (pl.col(f'Vol4_{metric}') - AVG_TUF_4) \n",
    "        -0.5 * (pl.col(f'Vol3_{metric}') - AVG_TUF_4) \n",
    "        +0.5 * (pl.col(f'Vol2_{metric}') - AVG_TUF_4) \n",
    "        +1.5 * (pl.col(f'Vol1_{metric}') - AVG_TUF_4)\n",
    "        ) / 15\n",
    "    )\n",
    "\n",
    "    #INDICATOR_SLOPE13\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('SLOPE_13')>THRE_13).then(pl.lit(1))\n",
    "        .when(pl.col('SLOPE_13')<-1*THRE_13).then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_SLOPE13')\n",
    "    )\n",
    "\n",
    "    #INDICATOR_SLOPE4\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('SLOPE_4')>THRE_4).then(pl.lit(1))\n",
    "        .when(pl.col('SLOPE_4')<-1*THRE_4).then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_SLOPE4')\n",
    "    )\n",
    "\n",
    "    #PEAK_DETECTOR\n",
    "    cols_1_13 = [f'Vol{i}_{metric}' for i in range(1,14)]\n",
    "    df2 = df2.with_columns(\n",
    "        PEAK_DETECTOR = pl.max_horizontal(cols_1_13)/pl.sum_horizontal(cols_1_13)\n",
    "    )\n",
    "\n",
    "    #INDICATOR_PEAK\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('PEAK_DETECTOR')>= 0.5).then(pl.lit(1))\n",
    "        .otherwise(pl.lit(0)).alias('INDICATOR_PEAK')\n",
    "    )\n",
    "\n",
    "    #FINAL_SLOPE\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when((pl.col('INDICATOR_SLOPE13') == 1) & (pl.col('INDICATOR_SLOPE4') >= 0))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 0) & (pl.col('INDICATOR_SLOPE4') == 1))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == -1) & (pl.col('INDICATOR_SLOPE4') <= 0))\n",
    "            .then(pl.lit(-1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 0) & (pl.col('INDICATOR_SLOPE4') == -1))\n",
    "            .then(pl.lit(-1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == -1) & (pl.col('INDICATOR_SLOPE4') == 1) & (pl.col('SLOPE_4') >= 1))\n",
    "            .then(pl.lit(1))\n",
    "        .when((pl.col('INDICATOR_SLOPE13') == 1) & (pl.col('INDICATOR_SLOPE4') == -1) & (pl.col('SLOPE_4') <= -1))\n",
    "            .then(pl.lit(-1))\n",
    "        .otherwise(pl.lit(0))\n",
    "        .alias('FINAL_SLOPE')\n",
    "    )\n",
    "\n",
    "    #GROWDECL\n",
    "    df2 = df2.with_columns(\n",
    "        pl.when(pl.col('FINAL_SLOPE') == 1)\n",
    "            .then(pl.lit(\"P\"))\n",
    "        .when(pl.col('FINAL_SLOPE') == -1)\n",
    "            .then(pl.lit(\"Q\"))\n",
    "        .otherwise(pl.lit(\"S\"))\n",
    "        .alias(f'Trend_{metric}')\n",
    "    )\n",
    "\n",
    "    df2 = df2.select(['IID','product_id',f'Trend_{metric}'])\n",
    "    df = df.join(df2,on = ['IID','product_id'],how = 'left')\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/Prescriber/Weekly/'\n",
    "\n",
    "temp1 = mp_spec_seg_dec.select(['IID','geography_id'])\n",
    "temp1 = add_volume_cols(temp1)\n",
    "temp1 = process_share(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For removal of extra rows -\n",
    "\n",
    "#joining with 52wk flag dataset\n",
    "temp1 = (\n",
    "    temp1.join(lin_52.with_columns(fl = 1),on = ['IID','product_id'],how='left')\n",
    "    .with_columns(\n",
    "        pl.when((pl.col(\"product_id\").is_in([2, 3, 4, 5])) & (pl.col(\"fl\").is_null()))\n",
    "        .then(0).otherwise(pl.col(\"fl\")).alias(\"fl\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp1 = temp1.with_columns(\n",
    "    flag_sum_TUF = pl.sum_horizontal([f'Vol{i}_TUF' for i in range(1,14)]),\n",
    "    flag_sum_NUF = pl.sum_horizontal([f'Vol{i}_NUF' for i in range(1,14)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp1= temp1.join(mp_spec_seg_dec[['IID','specialty_group']],on='IID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp1 = temp1.filter(\n",
    "    ((pl.col('fl').is_not_null()) & (pl.col('fl')==1)) | \n",
    "    ((pl.col('fl').is_null()) & ((pl.col('flag_sum_TUF')!=0) | (pl.col('flag_sum_TUF')!=0))) | \n",
    "    (pl.col('specialty_group')=='PED')\n",
    ").drop('fl','flag_sum','specialty_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp1 = process_trend(temp1,'TUF')\n",
    "temp1 = process_trend(temp1,'NUF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Converting to Feed ready data\n",
    "\n",
    "temp1 = temp1.with_columns(ReportType = pl.lit('WEEKLY'))\n",
    "\n",
    "trx_cols = ['IID','geography_id','product_id','ReportType'] + [col for col in temp1.columns if '_TUF' in col ]\n",
    "nrx_cols = ['IID','geography_id','product_id','ReportType'] + [col for col in temp1.columns if '_NUF' in col ]\n",
    "\n",
    "temp1_TUF = temp1.select(trx_cols).with_columns(Metric = pl.lit('TRX'))\n",
    "temp1_NUF = temp1.select(nrx_cols).with_columns(Metric = pl.lit('NRX'))\n",
    "\n",
    "for df_name in ['temp1_TUF','temp1_NUF']: #renaming for vstack\n",
    "    for col in globals()[df_name].columns:\n",
    "        if col.endswith('_TUF'):\n",
    "            globals()[df_name] = globals()[df_name].rename({col: col.replace('_TUF', '')})\n",
    "        elif col.endswith('_NUF'):\n",
    "            globals()[df_name] = globals()[df_name].rename({col: col.replace('_NUF', '')})\n",
    "\n",
    "# Setting up Sequence\n",
    "final_sequence = ['IID','geography_id','product_id','Metric','ReportType','Trend'] + [f'Vol{i}' for i in range(1,14)] + [f'Shr{i}' for i in range(1,14)]\n",
    "\n",
    "temp1_TUF = temp1_TUF.select(final_sequence).rename({'IID':'Physician_ID','geography_id':'Geography_id','product_id':'Product_id'})\n",
    "temp1_NUF = temp1_NUF.select(final_sequence).rename({'IID':'Physician_ID','geography_id':'Geography_id','product_id':'Product_id'})\n",
    "\n",
    "temp2 = temp1_TUF.vstack(temp1_NUF) # final dataframe\n",
    "\n",
    "for new_col in ['DS1_Vol','DS2_Vol']:\n",
    "    for i in range(1,14):\n",
    "        col_name = f'{new_col}{i}'\n",
    "        temp2 = temp2.with_columns(pl.lit('\\\\N').alias(col_name)) #change this to /N later ? # null --> \\N (harsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_columns = temp2.columns[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp2 = temp2.join(\n",
    "    MASTER_UNI[['IID','PDRPOptOutFlag']],left_on= 'Physician_ID',right_on='IID',how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for col in conversion_columns:\n",
    "    temp2 = temp2.with_columns(\n",
    "        pl.when(pl.col(\"PDRPOptOutFlag\") == \"Y\").then(pl.lit('\\\\N')).otherwise(pl.col(col)).alias(col)\n",
    "    )\n",
    "temp2 = temp2.drop('PDRPOptOutFlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presc Trend Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "# Export\n",
    "temp2.to_pandas().to_csv(f'{OUT}Weekly_Prescriber_Trend_Feed.txt', sep='|')\n",
    "print('Presc Trend Feed Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prescriber Trend Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presc X Feed Exported !\n"
     ]
    }
   ],
   "source": [
    "rx_date = datetime.strptime(data_date,'%Y%m%d')\n",
    "list_of_dates = [rx_date]\n",
    "serial_no = [i for i in range(1,14)]\n",
    "for i in range(1,13):\n",
    "    date_val = rx_date - timedelta(days = 7*i)\n",
    "    list_of_dates.append(date_val)\n",
    "\n",
    "\n",
    "date_df = pl.DataFrame(\n",
    "    {\n",
    "        'X':serial_no,\n",
    "        'Name':list_of_dates\n",
    "    }\n",
    ")\n",
    "\n",
    "date_df = date_df.with_columns(\n",
    "   date_df['Name'].dt.strftime('%m/%d/%Y')\n",
    ")\n",
    "\n",
    "date_df.to_pandas().to_csv(f'{OUT}Weekly_Prescriber_X_Feed.txt', sep='|')\n",
    "print('Presc X Feed Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
