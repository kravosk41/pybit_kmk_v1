{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prescriber View - Sales Performance pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:17.746812Z",
     "iopub.status.busy": "2024-06-25T11:55:17.745804Z",
     "iopub.status.idle": "2024-06-25T11:55:18.333237Z",
     "shell.execute_reply": "2024-06-25T11:55:18.332354Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:18.339053Z",
     "iopub.status.busy": "2024-06-25T11:55:18.337923Z",
     "iopub.status.idle": "2024-06-25T11:55:18.345820Z",
     "shell.execute_reply": "2024-06-25T11:55:18.344524Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "data_date = js['data_date']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "bucket = js['bucket']\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:18.372084Z",
     "iopub.status.busy": "2024-06-25T11:55:18.370874Z",
     "iopub.status.idle": "2024-06-25T11:55:18.377196Z",
     "shell.execute_reply": "2024-06-25T11:55:18.376253Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:18.383724Z",
     "iopub.status.busy": "2024-06-25T11:55:18.382834Z",
     "iopub.status.idle": "2024-06-25T11:55:19.963792Z",
     "shell.execute_reply": "2024-06-25T11:55:19.962578Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:19.969073Z",
     "iopub.status.busy": "2024-06-25T11:55:19.968527Z",
     "iopub.status.idle": "2024-06-25T11:55:19.986309Z",
     "shell.execute_reply": "2024-06-25T11:55:19.980017Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_period_iid_metric(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,106)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,5)]).alias(metric+'_4c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,14)]).alias(metric+'_13c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,27)]).alias(metric+'_26c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,num_weeks_rx+1)]).alias(metric+'_qtdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(5,9)]).alias(metric+'_4p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,27)]).alias(metric+'_13p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(27,53)]).alias(metric+'_26p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,14+num_weeks_rx)]).alias(metric+'_qtdp'),\n",
    "\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,106)]).alias(metric+'_all')\n",
    "    )\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    return(df.drop(['specialty_group','segment','decile','geography_id']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:19.990338Z",
     "iopub.status.busy": "2024-06-25T11:55:19.989848Z",
     "iopub.status.idle": "2024-06-25T11:55:19.996795Z",
     "shell.execute_reply": "2024-06-25T11:55:19.995881Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows(df):\n",
    "    agg_dict = {}\n",
    "    for col in df.columns[2:]:\n",
    "        agg_dict[col] = pl.col(col).sum()\n",
    "    \n",
    "    #join_cols = ['geography_id','plan_type','PlanID','IID']\n",
    "\n",
    "    df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    \n",
    "    df_1 = df.group_by('IID').agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:20.008451Z",
     "iopub.status.busy": "2024-06-25T11:55:20.007746Z",
     "iopub.status.idle": "2024-06-25T11:55:38.008310Z",
     "shell.execute_reply": "2024-06-25T11:55:38.007002Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Raw Data Prep ETA - 21 Seconds\n",
    "all_products_tuf = get_summed_period_iid_metric('TUF',fetch_products)\n",
    "all_products_nuf = get_summed_period_iid_metric('NUF',fetch_products)\n",
    "all_products_trx = get_summed_period_iid_metric('TRX',fetch_products)\n",
    "all_products_nrx = get_summed_period_iid_metric('NRX',fetch_products)\n",
    "all_products_tun = get_summed_period_iid_metric('TUN',fetch_products)\n",
    "all_products_nun = get_summed_period_iid_metric('NUN',fetch_products)\n",
    "all_products_tuf = add_parent_product_rows(all_products_tuf)\n",
    "all_products_nuf = add_parent_product_rows(all_products_nuf)\n",
    "all_products_trx = add_parent_product_rows(all_products_trx)\n",
    "all_products_nrx = add_parent_product_rows(all_products_nrx)\n",
    "all_products_tun = add_parent_product_rows(all_products_tun)\n",
    "all_products_nun = add_parent_product_rows(all_products_nun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:38.013498Z",
     "iopub.status.busy": "2024-06-25T11:55:38.013110Z",
     "iopub.status.idle": "2024-06-25T11:55:38.046315Z",
     "shell.execute_reply": "2024-06-25T11:55:38.045088Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_1(df):\n",
    "    cols = ['IID',p]\n",
    "    fetch_df = all_products_tuf[cols+[f'TUF{period}c',f'TUF{period}p']].join(\n",
    "        all_products_nuf[cols+[f'NUF{period}c',f'NUF{period}p']],on = cols,how = 'left'\n",
    "    )\n",
    "    df = df.join(fetch_df,on = 'IID',how = 'left'\n",
    "    ).filter(pl.col(p).is_not_null() #added this to remove people with no rx data.\n",
    "    ).rename({f'TUF{period}c':'cur_vol_trx',f'TUF{period}p' : 'pri_vol_trx',\n",
    "              f'NUF{period}c':'cur_vol_nrx',f'NUF{period}p' : 'pri_vol_nrx'\n",
    "    }).with_columns(\n",
    "        vol_change_trx = pl.col('cur_vol_trx')-pl.col('pri_vol_trx'),\n",
    "        vol_change_nrx = pl.col('cur_vol_nrx')-pl.col('pri_vol_nrx')\n",
    "\n",
    "    ).with_columns(\n",
    "        prc_vol_growth_trx = (pl.col('cur_vol_trx')/pl.col('pri_vol_trx'))-1,\n",
    "        prc_vol_growth_nrx = (pl.col('cur_vol_nrx')/pl.col('pri_vol_nrx'))-1\n",
    "    ).filter(\n",
    "        (pl.col('cur_vol_trx')!=0) | ((pl.col('cur_vol_nrx')!=0))\n",
    "\t)\n",
    "\n",
    "    return(df)\n",
    "\n",
    "# Grower or Decliner (TYPE)\t\n",
    "def process_2(df):\n",
    "    df_t = (\n",
    "        df.select(['IID','geography_id','product_id','cur_vol_trx','pri_vol_trx','vol_change_trx'])\n",
    "        .join(geo_code_mapper[['geography_id','region_geography_id']],on='geography_id',how='left')\n",
    "        .filter((pl.col('pri_vol_trx')!=0) & (pl.col('pri_vol_trx').is_not_null()))\n",
    "        .filter((pl.col('vol_change_trx')!=0))\n",
    "    )\n",
    "    #if prior volume is 0 then that IID does not contribue to grower or decliner.\n",
    "\n",
    "    # next step is to get 90th percentile and 10th percentile for each geo:\n",
    "    df_percentile_t = df_t.group_by('region_geography_id','product_id').agg(\n",
    "        ten_perc = pl.col('vol_change_trx').quantile(0.1,interpolation='linear'),\n",
    "        nin_perc = pl.col('vol_change_trx').quantile(0.9,interpolation='linear')\n",
    "    )\n",
    "\n",
    "    df_temp_t = df_t.join(df_percentile_t,on=['region_geography_id','product_id'],how = 'left')\n",
    "\n",
    "    df_temp_t = df_temp_t.with_columns(\n",
    "        pl.when((pl.col('vol_change_trx')<0) & (pl.col('vol_change_trx') <= pl.col('ten_perc'))).then(pl.lit('GROW'))\n",
    "        .when((pl.col('vol_change_trx')>0) & (pl.col('vol_change_trx') >= pl.col('nin_perc'))).then(pl.lit('DECL'))\n",
    "        .otherwise(pl.lit(None))\n",
    "        .alias('TYPE_trx')\n",
    "    ) \n",
    "    #QC DATASET [Check here for inconsistency if needed]\n",
    "\n",
    "    #HCPs whose prescription volume has fdeclined rom the prior period \n",
    "    #and the volume growth falls in bottom 10 percentile of each region level group -> DECLINERS\n",
    "\n",
    "    #HCPs whose prescription volume has grown from the prior period and the volume growth falls in \n",
    "    #top 10 percentile of each region level group -> GROWERS\n",
    "\n",
    "    df = df.join(df_temp_t[['IID','geography_id','product_id','TYPE_trx']],on = ['IID','geography_id','product_id'],how='left'\n",
    "    ).with_columns(TYPE_nrx = pl.col('TYPE_trx'))\n",
    "\n",
    "    return(df)\n",
    "\n",
    "# New Prescriber | PDRP | NC\n",
    "def process_3(df):\n",
    "    load('MASTER_UNI')\n",
    "    source_df = all_products_tuf.select(['IID',p,'TUF_all']).join(\n",
    "        all_products_nuf.select(['IID',p,'NUF_all']),on=['IID',p],how='left'\n",
    "    )\n",
    "    df = (\n",
    "        df.join(source_df,on=['IID',p],how='left')\n",
    "        .join(MASTER_UNI[['IID','PDRPOptOutFlag']],on='IID',how='left')\n",
    "        .with_columns(\n",
    "            old_volume_trx = pl.col('TUF_all')-pl.col('cur_vol_trx'),\n",
    "            old_volume_nrx = pl.col('NUF_all')-pl.col('cur_vol_nrx')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('old_volume_trx') == 0)\n",
    "            .then(pl.lit('NP'))\n",
    "            .otherwise(pl.col('TYPE_trx'))\n",
    "            .alias('TYPE_trx'),\n",
    "\n",
    "            pl.when(pl.col('old_volume_nrx') == 0)\n",
    "            .then(pl.lit('NP'))\n",
    "            .otherwise(pl.col('TYPE_nrx'))\n",
    "            .alias('TYPE_nrx')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('vol_change_trx') == 0)\n",
    "            .then(pl.lit('NC'))\n",
    "            .otherwise(pl.col('TYPE_trx'))\n",
    "            .alias('TYPE_trx'),\n",
    "\n",
    "            pl.when(pl.col('vol_change_trx') == 0)\n",
    "            .then(pl.lit('NC'))\n",
    "            .otherwise(pl.col('TYPE_nrx'))\n",
    "            .alias('TYPE_nrx')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('PDRPOptOutFlag')=='Y')\n",
    "            .then(pl.lit('PDRP'))\n",
    "            .otherwise(pl.col('TYPE_trx'))\n",
    "            .alias('TYPE_trx'),\n",
    "\n",
    "            pl.when(pl.col('PDRPOptOutFlag')=='Y')\n",
    "            .then(pl.lit('PDRP'))\n",
    "            .otherwise(pl.col('TYPE_nrx'))\n",
    "            .alias('TYPE_nrx')\n",
    "        )\n",
    "        .drop(['old_volume_trx','old_volume_nrx','TUF_all','NUF_all','PDRPOptOutFlag'])\n",
    "    )\n",
    "    return(df)\n",
    "\t\n",
    "#volume change indicator\n",
    "def process_4(df):\n",
    "\n",
    "    expression_for_trx = pl.when(pl.col('vol_change_trx')/pl.col('pri_vol_trx') > 0.02).then(pl.lit('P')\n",
    "    ).when(pl.col('vol_change_trx')/pl.col('pri_vol_trx') < -0.02).then(pl.lit('Q')\n",
    "    ).when(pl.col('vol_change_trx')==0).then(None\n",
    "    ).otherwise(None).alias('vol_change_ind_trx')\n",
    "\n",
    "    expression_for_nrx = pl.when(pl.col('vol_change_nrx')/pl.col('pri_vol_nrx') > 0.02).then(pl.lit('P')\n",
    "    ).when(pl.col('vol_change_nrx')/pl.col('pri_vol_nrx') < -0.02).then(pl.lit('Q')\n",
    "    ).when(pl.col('vol_change_nrx')==0).then(None\n",
    "    ).otherwise(None).alias('vol_change_ind_nrx')\n",
    "\n",
    "    return(df.with_columns(expression_for_trx,expression_for_nrx))\n",
    "\t\n",
    "#current prior and share change\n",
    "def process_5(df):\n",
    "    df1 = df.group_by('IID').agg(\n",
    "        mkt_TUF_c = pl.col('cur_vol_trx').sum(),mkt_TUF_p = pl.col('pri_vol_trx').sum(),\n",
    "        mkt_NUF_c = pl.col('cur_vol_nrx').sum(),mkt_NUF_p = pl.col('pri_vol_nrx').sum()\n",
    "    )\n",
    "\n",
    "    return(\n",
    "    df.join(df1,on='IID',how='left').with_columns(\n",
    "        cur_shr_trx = pl.col('cur_vol_trx')/pl.col('mkt_TUF_c'),cur_shr_nrx = pl.col('cur_vol_nrx')/pl.col('mkt_NUF_c'),\n",
    "        pri_shr_trx = pl.col('pri_vol_trx')/pl.col('mkt_TUF_p'),pri_shr_nrx = pl.col('pri_vol_nrx')/pl.col('mkt_NUF_p')\n",
    "    ).with_columns(\n",
    "        shr_change_trx = pl.col('cur_shr_trx')-pl.col('pri_shr_trx'),shr_change_nrx = pl.col('cur_shr_nrx')-pl.col('pri_shr_nrx')\n",
    "    ).drop(['mkt_TUF_c','mkt_TUF_p','mkt_NUF_c','mkt_NUF_p'] # maybe consider not dropping it.\n",
    "    ) .with_columns(\n",
    "        prc_shr_growth_trx = (pl.col('cur_shr_trx')/pl.col('pri_shr_trx'))-1,\n",
    "        prc_shr_growth_nrx = (pl.col('cur_shr_nrx')/pl.col('pri_shr_nrx'))-1\n",
    "    ))\n",
    "\t\n",
    "#share change indicator\n",
    "def process_6(df):\n",
    "    return(\n",
    "        df.with_columns(\n",
    "            pl.when(pl.col('shr_change_trx') > 0.005).then(pl.lit('P'))\n",
    "            .when(pl.col('shr_change_trx') < -0.005).then(pl.lit('Q'))\n",
    "            .when(pl.col('shr_change_trx')==0).then(None)\n",
    "            .otherwise(None).alias('shr_change_ind_trx'),\n",
    "\n",
    "            pl.when(pl.col('shr_change_nrx') > 0.005).then(pl.lit('P'))\n",
    "            .when(pl.col('shr_change_nrx') < -0.005).then(pl.lit('Q'))\n",
    "            .when(pl.col('shr_change_nrx')==0).then(None)\n",
    "            .otherwise(None).alias('shr_change_ind_nrx')\n",
    "        )\n",
    "    )\n",
    "\t\n",
    "#Trx Size Metrics (copied values for nrx)\n",
    "def process_7(df):\n",
    "    cols = ['IID','product_id']\n",
    "    fetch_df = all_products_tun[cols+[f'TUN{period}c',f'TUN{period}p']].join(\n",
    "        all_products_trx[cols+[f'TRX{period}c',f'TRX{period}p']],on = cols,how = 'left'\n",
    "    )\n",
    "\n",
    "    df2 = df.join(fetch_df,on = ['IID','product_id'],how = 'left'\n",
    "    ).with_columns(\n",
    "        avg_trx_size = pl.col(f'TUN{period}c')/pl.col(f'TRX{period}c'),\n",
    "        pri_avg_trx_size = pl.col(f'TUN{period}p')/pl.col(f'TRX{period}p')\n",
    "    ).with_columns(\n",
    "        avg_trx_size_ch = pl.col('avg_trx_size') - pl.col('pri_avg_trx_size')\n",
    "    ).rename(\n",
    "        {f'TRX{period}c' : 'avg_trx_size_trx',f'TUN{period}c':'avg_trx_size_unit'}\n",
    "    ).with_columns(\n",
    "        avg_nrx_size = pl.col('avg_trx_size'),\n",
    "        avg_nrx_size_ch = pl.lit('\\\\N'), #not copying the raw data columns here nrx metric is not to be calc\n",
    "    ).drop([f'TUN{period}p',f'TRX{period}p','pri_avg_trx_size'])\n",
    "\n",
    "    return(df2)\n",
    "\t\n",
    "#FIX - x SHOULD NOT BE CUR_VOL , it should be CUR_TRX\n",
    "#90 day trx perc (no values for nrx)\n",
    "# trx_90day_pct   =  ((tuf_rx_&ce. - ((tuf_units_&ce. - 90*tuf_rx_&ce.) / -60)) / tuf_rx_&ce.)\n",
    "#simplyfy z = (x-((y-90x)/-60))/x, where x = cur_vol, y = cur_tun\n",
    "# you get z = -(1/2) + (y/60x)\n",
    "def process_8(df):\n",
    "    cols = ['IID','product_id']\n",
    "    fetch_df = all_products_tun[cols+[f'TUN{period}c']].rename({f'TUN{period}c':'tuf_units'})\n",
    "\n",
    "    formula = -(1/2) + (pl.col('tuf_units')/(60*pl.col('cur_vol_trx')))\n",
    "\n",
    "    df2 = df.join(fetch_df,on=cols,how='left').with_columns(\n",
    "        trx_90day_pct = formula,\n",
    "        trx_90day_pct_nrx = None\n",
    "    ).drop('tuf_units')\n",
    "\n",
    "    return(df2)\n",
    "\n",
    "def get_benchmark_cols(df,metric,b_metric,b_name):\n",
    "    df = (\n",
    "        df\n",
    "        .join(\n",
    "            mp_spec_seg_dec.drop('geography_id',d),on='IID',how='left'\n",
    "        )\n",
    "        .join(\n",
    "            terr_growths.select('geography_id',p,spc,sg,b_metric),\n",
    "            on = ['geography_id',spc,sg,p], how = 'left'\n",
    "        )\n",
    "        .rename(\n",
    "            {\n",
    "                f'{b_metric}_right':f'Prc_Benchmark_{b_name}_{metric}'\n",
    "            }\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col(b_metric)>pl.col(f'Prc_Benchmark_{b_name}_{metric}'))\n",
    "            .then(pl.lit('L'))\n",
    "            .otherwise(pl.lit(None))\n",
    "            .alias(f'{b_name}_Ind_{metric}')\n",
    "        )\n",
    "        .drop(spc,sg)\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:38.050510Z",
     "iopub.status.busy": "2024-06-25T11:55:38.050154Z",
     "iopub.status.idle": "2024-06-25T11:55:38.070956Z",
     "shell.execute_reply": "2024-06-25T11:55:38.069514Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For converting to Feed Ready Data -\n",
    "def get_feed(temp1):\n",
    "    final_feed = temp1.with_columns(\n",
    "        pl.col('avg_trx_size_trx').alias('avg_trx_size_trx_nrx')\n",
    "    ).with_columns(\n",
    "        pl.col('avg_trx_size_unit').alias('avg_trx_size_unit_nrx')\n",
    "    )\n",
    "    #function to diving dataframe in two levels('Trx','Nrx')\n",
    "    def select_columns_by_condition(df,metric):\n",
    "        # Get the column names to be excluded based on the condition\n",
    "        excluded_columns = [col for col in df.columns if not col.endswith(metric)]\n",
    "        \n",
    "        # Select all columns except the excluded ones\n",
    "        selected_df = df.select(excluded_columns)\n",
    "        return selected_df\n",
    "    #working on trx level\n",
    "    final_feed_trx = select_columns_by_condition(final_feed,'nrx')\n",
    "    final_feed_trx = final_feed_trx.drop(['avg_nrx_size','avg_nrx_size_ch'])\n",
    "    final_feed_trx = final_feed_trx.with_columns(\n",
    "        pl.lit('TRX').alias('Metric')\n",
    "    )\n",
    "    #working on nrx level\n",
    "    final_feed_nrx = select_columns_by_condition(final_feed,'trx')\n",
    "    final_feed_nrx = final_feed_nrx.drop(['avg_trx_size','avg_trx_size_trx','avg_trx_size_ch','trx_90day_pct','avg_trx_size_unit'])\n",
    "    final_feed_nrx = final_feed_nrx.with_columns(\n",
    "        pl.lit('NRX').alias('Metric')\n",
    "    )\n",
    "    #function to remove _trx or _nrx from final_feed_nrx and final_feed_trx\n",
    "    def rename_columns_by_condition(df,metric):\n",
    "        renamed_columns = {col: col[:-4] if col.endswith(metric) and col != 'avg_trx_size_trx' else col for col in df.columns}\n",
    "        renamed_df = df.rename(renamed_columns)\n",
    "        return renamed_df\n",
    "    # making trx feed columns and nrx feed columns similar so that we can vstack them\n",
    "    final_feed_nrx = rename_columns_by_condition(final_feed_nrx,'nrx')\n",
    "    final_feed_nrx = final_feed_nrx.rename({\n",
    "        'avg_nrx_size':'avg_trx_size',\n",
    "        'avg_nrx_size_ch':'avg_trx_size_ch'\n",
    "    })\n",
    "    final_feed_nrx = final_feed_nrx.with_columns(\n",
    "        pl.lit('\\\\N').alias('trx_90day_pct')\n",
    "    )\n",
    "    \n",
    "    final_feed_nrx = final_feed_nrx.select(['IID',\n",
    "     'geography_id',\n",
    "     'product_id',\n",
    "     'cur_vol',\n",
    "     'pri_vol',\n",
    "     'vol_change',\n",
    "     'prc_vol_growth',\n",
    "     'TYPE',\n",
    "     'vol_change_ind',\n",
    "     'cur_shr',\n",
    "     'pri_shr',\n",
    "     'shr_change',\n",
    "     'prc_shr_growth',\n",
    "     'shr_change_ind',\n",
    "     # 'product_id_right',\n",
    "     'avg_trx_size_unit',\n",
    "     'avg_trx_size_trx',\n",
    "     'avg_trx_size',\n",
    "     'avg_trx_size_ch',\n",
    "     'trx_90day_pct',\n",
    "     'Prc_Benchmark_Vol_Growth',\n",
    "     'Vol_Growth_Ind',\n",
    "     'Prc_Benchmark_Shr_Growth',\n",
    "     'Shr_Growth_Ind',\n",
    "     'Metric'])\n",
    "    #making final_feed_trx ready for vstack with final_feed_nrx \n",
    "    final_feed_trx = rename_columns_by_condition(final_feed_trx,'trx')\n",
    "    final_feed_trx = final_feed_trx.with_columns(\n",
    "        pl.col(\"avg_trx_size_ch\").cast(pl.String)\n",
    "        ).with_columns(\n",
    "            pl.col('trx_90day_pct').cast(pl.String)\n",
    "        )\n",
    "    final_feed = final_feed_trx.vstack(final_feed_nrx)\n",
    "    #removing extra columns a\\c to feed\n",
    "    #final_feed = final_feed.drop(['product_id_right'])\n",
    "    #Renaming existing columns according to feed\n",
    "    rnm_cols = {\n",
    "        'IID':'Physician_ID',\n",
    "        'geography_id':'Geography_id',\n",
    "        'product_id':'Product_id',\n",
    "        'cur_vol':'Current_Vol',\n",
    "        'pri_vol':'Prior_Vol',\n",
    "        'vol_change':'Vol_Change',\n",
    "        'prc_vol_growth':'Prc_Vol_Growth',\n",
    "        'TYPE':'Type',\n",
    "        'vol_change_ind':'Vol_Change_Ind',\n",
    "        'cur_shr':'Current_Shr',\n",
    "        'pri_shr':'Prior_Shr',\n",
    "        'shr_change':'Shr_Change',\n",
    "        'prc_shr_growth':'Prc_Shr_Growth',\n",
    "        'shr_change_ind':'Shr_Change_Ind',\n",
    "        'avg_trx_size_unit':'Avg_TRx_Size_Unit',\n",
    "        'avg_trx_size_trx':'Avg_TRx_Size_TRx',\n",
    "        'avg_trx_size':'Avg_TRx_Size',\n",
    "        'avg_trx_size_ch':'Avg_TRx_Size_Change',\n",
    "        'trx_90day_pct':'Ninty_Day_TRx_Prc',\n",
    "    }\n",
    "    final_feed = final_feed.rename(rnm_cols)\n",
    "    #required new columns for feed\n",
    "    col_to_addrt = ['ReportType']\n",
    "    col_to_addp = ['Period']\n",
    "    col_to_addna = [\"Total_Num_Of_Redemptions\", \"Frozen_Competitor_Vol\", \"DS1_Current_Vol\", \"DS1_Prior_Vol\", \"DS2_Current_Vol\", \"DS2_Prior_Vol\"]\n",
    "    # func to add columns with desired value\n",
    "    def addcol(df,columns_to_add,wtl):\n",
    "        for my_col in columns_to_add:\n",
    "            df = df.with_columns(pl.lit(wtl).alias(my_col))\n",
    "        return df\n",
    "    final_feed = addcol(final_feed,col_to_addrt,'WEEKLY')\n",
    "    final_feed = addcol(final_feed,col_to_addp,f'{period_num}-WEEK')\n",
    "    final_feed = addcol(final_feed,col_to_addna,'\\\\N')\n",
    "    # rearranging columns accoring to feed.\n",
    "    req_cols = [\n",
    "    \"Physician_ID\", \"Geography_id\", \"Product_id\", \"Metric\", \"ReportType\", \"Period\", \"Type\", \"Current_Vol\", \"Prior_Vol\", \"Vol_Change\", \n",
    "    \"Vol_Change_Ind\", \"Prc_Vol_Growth\", \"Prc_Benchmark_Vol_Growth\", \"Vol_Growth_Ind\", \"Current_Shr\", \"Prior_Shr\", \"Shr_Change\", \n",
    "    \"Shr_Change_Ind\", \"Prc_Shr_Growth\", \"Prc_Benchmark_Shr_Growth\", \"Shr_Growth_Ind\", \"Avg_TRx_Size\", \"Avg_TRx_Size_TRx\", \n",
    "    \"Avg_TRx_Size_Unit\", \"Total_Num_Of_Redemptions\", \"Frozen_Competitor_Vol\", \"DS1_Current_Vol\", \"DS1_Prior_Vol\", \"DS2_Current_Vol\", \n",
    "    \"DS2_Prior_Vol\", \"Avg_TRx_Size_Change\", \"Ninty_Day_TRx_Prc\"]\n",
    "    final_feed = final_feed.select(req_cols)# final data set\n",
    "    \n",
    "    return (final_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period Loop -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:38.075839Z",
     "iopub.status.busy": "2024-06-25T11:55:38.075086Z",
     "iopub.status.idle": "2024-06-25T11:55:38.083980Z",
     "shell.execute_reply": "2024-06-25T11:55:38.082575Z"
    }
   },
   "outputs": [],
   "source": [
    "# for trvializing formula : \n",
    "p,sg,spc,d = 'product_id','segment','specialty_group','decile'\n",
    "levels = ['geography_id','region_geography_id','area_geography_id','nation_geography_id']\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/Prescriber/Weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T11:55:38.089012Z",
     "iopub.status.busy": "2024-06-25T11:55:38.088444Z",
     "iopub.status.idle": "2024-06-25T11:58:30.642124Z",
     "shell.execute_reply": "2024-06-25T11:58:30.641287Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 1!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 2!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 3!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 4!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Feed 5!\n"
     ]
    }
   ],
   "source": [
    "#for period_num,PN in zip(['qtd'],[5]):\n",
    "for period_num,PN in zip([1,4,13,26,'qtd'],[1,2,3,4,5]):\n",
    "    period = f'_{period_num}'\n",
    "    temp1 = mp_spec_seg_dec.select(['IID','geography_id'])\n",
    "    temp1 = process_1(temp1)\n",
    "    temp1 = process_2(temp1)\n",
    "    temp1 = process_3(temp1)\n",
    "    temp1 = process_4(temp1)\n",
    "    temp1 = process_5(temp1)\n",
    "    temp1 = process_6(temp1)\n",
    "    temp1 = process_7(temp1)\n",
    "    temp1 = process_8(temp1)\n",
    "    load(f'terr_growths_{PN}')\n",
    "    terr_growths = globals()[f'terr_growths_{PN}']\n",
    "    temp1 = get_benchmark_cols(temp1,'trx','prc_vol_growth_trx','Vol_Growth')\n",
    "    temp1 = get_benchmark_cols(temp1,'nrx','prc_vol_growth_nrx','Vol_Growth')\n",
    "    temp1 = get_benchmark_cols(temp1,'trx','prc_shr_growth_trx','Shr_Growth')\n",
    "    temp1 = get_benchmark_cols(temp1,'nrx','prc_shr_growth_nrx','Shr_Growth')\n",
    "\n",
    "    feed_dataset = get_feed(temp1)\n",
    "    feed_dataset.to_pandas().to_csv(f'{OUT}Weekly_Prescriber_SalesPerformance_P{PN}_Feed.txt', sep='|')\n",
    "    print(f'Exported Feed {PN}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
