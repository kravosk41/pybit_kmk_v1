{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly_LinzessSnapshot_SalesActivity_Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load variables from JSON\n",
    "with open('vars_wk.json', 'r') as json_file:\n",
    "    js = json.load(json_file)\n",
    "\n",
    "bucket = js['bucket']\n",
    "num_weeks_rx = js['num_weeks_rx']\n",
    "num_weeks_calls = js['num_weeks_calls']\n",
    "data_date = js['data_date']\n",
    "quarter_start = datetime.strptime(js['quarter_start'], '%Y-%m-%d').date()\n",
    "\n",
    "dflib = f's3://{bucket}/BIT/dataframes/'\n",
    "xpn = f's3://{bucket}/PYADM/weekly/archive/{data_date}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions -\n",
    "def load(df, lib=dflib):\n",
    "    globals()[df] = pl.read_parquet(f'{lib}{df}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imporing Dependencies\n",
    "prod_mapping = pl.read_csv(f's3://{bucket}/BIT/docs/productmapping_pybit.txt',separator='|')\n",
    "geo_code_mapper = pl.from_pandas(pd.read_excel(f's3://{bucket}/BIT/docs/geo_id_full.xlsx'))\n",
    "load('mp_spec_seg_dec')\n",
    "load('temp_calls')\n",
    "load('temp_samples')\n",
    "load('temp_abbv')\n",
    "load('MASTER_UNI')\n",
    "load('roster')\n",
    "fetch_products = ['LI1','LI2','LI3','TRU','AMT','LAC','MOT','LUB','IRL']\n",
    "\n",
    "#fixes for vortex import -> Probably caused by Polars Upgrades\n",
    "temp_calls = temp_calls.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_samples = temp_samples.with_columns(pl.col('SalesRepIID').cast(pl.Int64))\n",
    "temp_abbv = temp_abbv.with_columns(pl.col('SalesRepIID').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Functions - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Voucher Removal - \n",
    "def get_lin_voucher():\n",
    "    vch = pl.read_parquet(f'{xpn}LIN_VOUCHER.parquet') # n_rows=500\n",
    "    vch1 = pl.DataFrame()\n",
    "    for prod in ['LIN1','LIN2','LIN3']: # LINV\n",
    "        vch_prod = (\n",
    "            vch.select(\n",
    "                pl.col('IID'),\n",
    "                pl.col(f'{prod}TUF1').alias(f'vTUF_1c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,5)]).alias(f'vTUF_4c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,14)]).alias(f'vTUF_13c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,27)]).alias(f'vTUF_26c'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,num_weeks_rx+1)]).alias(f'vTUF_qtdc'),\n",
    "                pl.col(f'{prod}TUF2').alias(f'vTUF_1p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(5,9)]).alias(f'vTUF_4p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(14,27)]).alias(f'vTUF_13p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(27,53)]).alias(f'vTUF_26p'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(14,14+num_weeks_rx)]).alias(f'vTUF_qtdp'),\n",
    "                pl.sum_horizontal([f'{prod}TUF{i}' for i in range(1,106)]).alias(f'vTUF_all')\n",
    "            )\n",
    "            .with_columns(pl.lit(f'LI{prod[-1]}').alias('PROD_CD'))\n",
    "        )\n",
    "        if prod[-1] == '1':\n",
    "            vch1 = vch_prod.clone()\n",
    "        else:\n",
    "            vch1 = pl.concat([vch1, vch_prod])\n",
    "\n",
    "    # voucher_mapping = {'LI1': 4, 'LI2': 5, 'LI3': 3, 'LIV': 2}\n",
    "    # vch1 = vch1.with_columns(pl.col('PROD_CD').replace(voucher_mapping,return_dtype=pl.Int64).alias('product_id')).fill_null(0)#.drop('PROD_CD')\n",
    "    vch1 = vch1.fill_null(0)\n",
    "\n",
    "    return(vch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_summed_period_iid_metric(metric,prod_cd):\n",
    "    columns = ['IID','PROD_CD'] + [metric+str(i) for i in range(1,106)]\n",
    "    df = pl.read_parquet(xpn+'LAX.parquet',columns=columns).filter(pl.col('PROD_CD').is_in(prod_cd))\n",
    "\n",
    "    # 1,4,13,26 for current and prior period for a given Metric\n",
    "    df = df.select(\n",
    "        pl.col('IID'),pl.col('PROD_CD'),\n",
    "        pl.col(metric+'1').alias(metric+'_1c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,5)]).alias(metric+'_4c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,14)]).alias(metric+'_13c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,27)]).alias(metric+'_26c'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,num_weeks_rx+1)]).alias(metric+'_qtdc'),\n",
    "\n",
    "        pl.col(metric+'2').alias(metric+'_1p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(5,9)]).alias(metric+'_4p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,27)]).alias(metric+'_13p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(27,53)]).alias(metric+'_26p'),\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(14,14+num_weeks_rx)]).alias(metric+'_qtdp'),\n",
    "\n",
    "        pl.sum_horizontal([metric+str(i) for i in range(1,106)]).alias(metric+'_all')\n",
    "    )\n",
    "    # For Voucher Removal - \n",
    "    if metric == 'TUF':\n",
    "        dfv = get_lin_voucher()\n",
    "        df = df.join(dfv,on=['IID','PROD_CD'],how='left').fill_null(0)\n",
    "        cols_to_remove = dfv.columns[1:-1]\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'{metric}_1c') -  pl.col(f'v{metric}_1c').alias(f'{metric}_1c'),\n",
    "            pl.col(f'{metric}_4c') -  pl.col(f'v{metric}_4c').alias(f'{metric}_4c'),\n",
    "            pl.col(f'{metric}_13c') -  pl.col(f'v{metric}_13c').alias(f'{metric}_13c'),\n",
    "            pl.col(f'{metric}_26c') -  pl.col(f'v{metric}_26c').alias(f'{metric}_26c'),\n",
    "            pl.col(f'{metric}_qtdc') -  pl.col(f'v{metric}_qtdc').alias(f'{metric}_qtdc'),\n",
    "            pl.col(f'{metric}_1p') -  pl.col(f'v{metric}_1p').alias(f'{metric}_1p'),\n",
    "            pl.col(f'{metric}_4p') -  pl.col(f'v{metric}_4p').alias(f'{metric}_4p'),\n",
    "            pl.col(f'{metric}_13p') -  pl.col(f'v{metric}_13p').alias(f'{metric}_13p'),\n",
    "            pl.col(f'{metric}_26p') -  pl.col(f'v{metric}_26p').alias(f'{metric}_26p'),\n",
    "            pl.col(f'{metric}_qtdp') -  pl.col(f'v{metric}_qtdp').alias(f'{metric}_qtdp'),\n",
    "            pl.col(f'{metric}_all') -  pl.col(f'v{metric}_all').alias(f'{metric}_all')\n",
    "        ).drop(cols_to_remove)\n",
    "\n",
    "    # Adding MP related columns\n",
    "    df = df.join(mp_spec_seg_dec,on='IID',how='left').filter(pl.col('geography_id').is_not_null())\n",
    "\n",
    "    return(df.drop(['specialty_group','segment','decile','geography_id']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_parent_product_rows(df):\n",
    "    agg_dict = {}\n",
    "    for col in df.columns[2:]:\n",
    "        agg_dict[col] = pl.col(col).sum()\n",
    "    \n",
    "    #join_cols = ['geography_id','plan_type','PlanID','IID']\n",
    "\n",
    "    df = df.join(prod_mapping[['code','product_id','parent_product_id']], left_on = 'PROD_CD',right_on = 'code', how = 'left')\n",
    "    df_2_35 = df.filter(pl.col('parent_product_id').is_in([2,35]))\n",
    "    df_2_35 = df_2_35.group_by(['IID','parent_product_id']).agg(**agg_dict).rename({'parent_product_id':'product_id'})\n",
    "    \n",
    "    df_1 = df.group_by('IID').agg(**agg_dict).with_columns(product_id = pl.lit(1)).with_columns(pl.col('product_id').cast(pl.Int64))\n",
    "\n",
    "    # stack 1, 2_35 with df and return\n",
    "    df = df.drop(['PROD_CD','parent_product_id']) #dropping to make same shape\n",
    "    vstack_helper = df.columns\n",
    "    df = df.vstack(\n",
    "        df_2_35.select(vstack_helper)\n",
    "    ).vstack(\n",
    "        df_1.select(vstack_helper)\n",
    "    )\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data prep -\n",
    "all_products_tuf = get_summed_period_iid_metric('TUF',fetch_products)\n",
    "all_products_tuf = add_parent_product_rows(all_products_tuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# iw_calls_13wks, # QTD IW CALLS-\n",
    "def process_1(df,nw,col):\n",
    "\n",
    "    if nw == 13:\n",
    "        source_df = (temp_calls.filter(pl.col('call_week')<=13))\n",
    "    else:\n",
    "        source_df = (\n",
    "            temp_calls\n",
    "            .filter(pl.col('call_week')<=num_weeks_calls)\n",
    "            .filter(pl.col('CallDate')>= quarter_start)\n",
    "            .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "            .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "            .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "        )\n",
    "    source_df = (\n",
    "        source_df\n",
    "        .group_by(['AttendeeIID'])\n",
    "        .agg(pl.col('CallID').n_unique().alias(col))\n",
    "    )\n",
    "\n",
    "    df = df.join(source_df,left_on='IID',right_on = 'AttendeeIID',how='left')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:51:33.608418Z",
     "iopub.status.busy": "2024-08-06T14:51:33.607495Z",
     "iopub.status.idle": "2024-08-06T14:51:33.618049Z",
     "shell.execute_reply": "2024-08-06T14:51:33.616110Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#abbv_visit\n",
    "def process_2(df):\n",
    "    source_df = (\n",
    "        temp_abbv.filter(pl.col('call_week')<=num_weeks_calls)\n",
    "        .filter(pl.col('CallDate')>= quarter_start)\n",
    "        .group_by('AttendeeIID')\n",
    "        .agg(abbv_visit=pl.col('CallID').n_unique())\n",
    "    )\n",
    "\n",
    "    df = df.join(source_df,left_on='IID',right_on='AttendeeIID',how='left')\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:51:33.622366Z",
     "iopub.status.busy": "2024-08-06T14:51:33.622059Z",
     "iopub.status.idle": "2024-08-06T14:51:33.637840Z",
     "shell.execute_reply": "2024-08-06T14:51:33.636751Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Total IW Samples , Total Samples IW + ABBV (no samples for ABBV so its the same value)\n",
    "# NOTE : the samples are summed for all 3 dossages here\n",
    "def process_3(df):\n",
    "\n",
    "    source_df = (\n",
    "        temp_samples\n",
    "        .filter(pl.col('sample_week')<=num_weeks_calls)\n",
    "        .filter(pl.col('CallDate')>= quarter_start)\n",
    "        .join(MASTER_UNI.select(['IID','Territory']),left_on = 'AttendeeIID', right_on = 'IID')\n",
    "        .join(roster, on = 'SalesRepIID' , how = 'left')\n",
    "        .filter(pl.col('Territory')==pl.col('GEO'))\n",
    "    )\n",
    "    \n",
    "    source_df = (\n",
    "        source_df\n",
    "        .group_by('AttendeeIID')\n",
    "        .agg(total_iw_samples = pl.col('CallProductQuantity').sum())\n",
    "        .with_columns(total_iw_abbv_samples = pl.col('total_iw_samples'))\n",
    "    )\n",
    "\n",
    "    df = df.join(source_df,left_on='IID',right_on='AttendeeIID',how='left')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:51:33.643391Z",
     "iopub.status.busy": "2024-08-06T14:51:33.642817Z",
     "iopub.status.idle": "2024-08-06T14:51:33.665470Z",
     "shell.execute_reply": "2024-08-06T14:51:33.664112Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Trx Per Sample IW + ABBV -\n",
    "def process_4(df):\n",
    "    # getting Rx Data -\n",
    "    lin_iid = all_products_tuf.filter(pl.col('product_id')==2).select(['IID','TUF_qtdc'])\n",
    "    df = (\n",
    "        df.join(lin_iid,on='IID',how='left')\n",
    "        .with_columns(trx_per_sample = pl.col('TUF_qtdc')/pl.col('total_iw_abbv_samples')).drop('TUF_qtdc')\n",
    "    )\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:51:33.670346Z",
     "iopub.status.busy": "2024-08-06T14:51:33.669346Z",
     "iopub.status.idle": "2024-08-06T14:51:33.679624Z",
     "shell.execute_reply": "2024-08-06T14:51:33.678799Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_feed(temp1):\n",
    "    for col in ['iw_calls_13wks','qtd_iw_calls','total_iw_samples','total_iw_abbv_samples','trx_per_sample']:\n",
    "        globals()['temp1'] = globals()['temp1'].with_columns(pl.col(col).fill_null('\\\\N'))\n",
    "    #renaming columns according to feed\n",
    "    rnm_cols = {\n",
    "        'IID':'Physician_ID',\n",
    "        'geography_id':'Geography_id',\n",
    "        'iw_calls_13wks':'SA_NumberMetric6',\n",
    "        'qtd_iw_calls':'SA_NumberMetric1',\n",
    "        'abbv_visit':'SA_NumberMetric2',\n",
    "        'total_iw_samples':'SA_NumberMetric3',\n",
    "        'total_iw_abbv_samples':'SA_NumberMetric4',\n",
    "        'trx_per_sample':'SA_NumberMetric5'\n",
    "    }\n",
    "    #PDRP override - \n",
    "    pdrp = MASTER_UNI.select(['IID','PDRPOptOutFlag'])\n",
    "    temp1 = (\n",
    "        temp1\n",
    "        .join(pdrp, on='IID',how='left')\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('PDRPOptOutFlag')=='Y').then(pl.lit('\\\\N')).otherwise(pl.col('trx_per_sample')).alias('trx_per_sample')\n",
    "        )\n",
    "    )\n",
    " \n",
    "    final_feed = (\n",
    "        temp1\n",
    "        .rename(rnm_cols)\n",
    "        .select(['Physician_ID','Geography_id'] + [f'SA_NumberMetric{i}' for i in range(1,7)])\n",
    "    )\n",
    "\n",
    "    #Nan Corection -\n",
    "    px = [pl.col(c).replace(None,'\\\\N') for c in [f'SA_NumberMetric{i}' for i in range(1,7)]]\n",
    "    final_feed = final_feed.with_columns(px)\n",
    "    \n",
    "    return(final_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:51:33.683815Z",
     "iopub.status.busy": "2024-08-06T14:51:33.683077Z",
     "iopub.status.idle": "2024-08-06T14:51:33.957837Z",
     "shell.execute_reply": "2024-08-06T14:51:33.955688Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Processing - \n",
    "temp1 = mp_spec_seg_dec.select(['IID','geography_id'])\n",
    "temp1 = process_1(temp1,13,'iw_calls_13wks')\n",
    "temp1 = process_1(temp1,num_weeks_calls,'qtd_iw_calls')\n",
    "temp1 = process_2(temp1)\n",
    "temp1 = process_3(temp1)\n",
    "temp1 = process_4(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:51:33.963409Z",
     "iopub.status.busy": "2024-08-06T14:51:33.962676Z",
     "iopub.status.idle": "2024-08-06T14:51:34.087724Z",
     "shell.execute_reply": "2024-08-06T14:51:34.085170Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Filtering - \n",
    "temp1 = temp1.join(mp_spec_seg_dec.select(['IID','segment']),on='IID',how='left')\n",
    "temp1 = temp1.filter(\n",
    "    (pl.col('iw_calls_13wks').is_not_null()) | (pl.col('segment') == 'Target') \n",
    ")\n",
    "temp1 = temp1.filter(\n",
    "    (pl.col('qtd_iw_calls').is_not_null()) | (pl.col('segment') == 'Target') \n",
    ").drop('segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:51:34.094582Z",
     "iopub.status.busy": "2024-08-06T14:51:34.093814Z",
     "iopub.status.idle": "2024-08-06T14:51:34.407779Z",
     "shell.execute_reply": "2024-08-06T14:51:34.406932Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS Sales Actvity Exported !\n"
     ]
    }
   ],
   "source": [
    "#Exporting Feeds-\n",
    "OUT = 's3://vortex-staging-a65ced90/BIT/output/LinzessSnapshot/Weekly/'\n",
    "feed_dataset = get_feed(temp1)\n",
    "feed_dataset.to_pandas().to_csv(f'{OUT}Weekly_LinzessSnapshot_SalesActivity_Feed.txt', sep='|',lineterminator='\\r\\n',index=False)\n",
    "print(f'LS Sales Actvity Exported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
